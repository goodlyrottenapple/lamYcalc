\documentclass[a4paper, 12pt, twoside]{style/ociamthesis}
% - Customization --------------------------------------------------------------
% - These settings are changed in metadata.yaml
% - You should not touch anything here

\title{A formalization of the \(\lamy\) calculus}            % the title of the thesis

\author{Samuel Balco}          % your name

\college{GTC}        % your college

\supervisor{Faris Abou-Saleh, Luke Ong and Steven Ramsay}  % your supervisor

\degree{MSc in Computer Science}          % the degree
\degreedate{Trinity 2016}  % the degree date

\logofile{style/logobar}

%input macros (i.e. write your own macros file called mymacros.tex
%and uncomment the next line)
% \include{}

% -----------------------------------------------------------------------------
% -- PACKAGES -----------------------------------------------------------------
% -----------------------------------------------------------------------------
\usepackage{framed}
\usepackage{epstopdf}
\usepackage[usenames,dvipsnames]{xcolor}
% Set figure legends and captions to be smaller sized sans serif font
\usepackage[font={footnotesize,sf}]{caption}
\usepackage{float}

\usepackage[titletoc]{appendix}
\usepackage{pdfpages} % incluce pdf files
\usepackage{wallpaper}


% amsthm stuff --------------------------------------------------
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{Theorem}{Theorem}[chapter]
\newtheorem{Lemma}{Lemma}[chapter]
\newtheorem*{Proposition}{Proposition}
\newtheorem*{Corollary}{Corollary}
\theoremstyle{definition}
\newtheorem{Definition}{Definition}[chapter]
\newtheorem{Conjecture}{Conjecture}[chapter]
\newtheorem{Example}{Example}[chapter]
\newtheorem{Postulate}{Postulate}[chapter]
\newtheorem{Problem}{Problem}[chapter]
\theoremstyle{remark}
\newtheorem{Case}{Case}[chapter]
\newtheorem*{Remark}{Remark}
\newtheorem*{Note}{Note}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=2\parskip \thm@postskip=0pt
}
\makeatother

\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
  \vspace{-\topsep}% remove the space after the theorem
  \pushQED{\qed}%
  \normalfont
  \topsep0pt \partopsep0pt % no space before
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{.}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
  \addvspace{6pt plus 6pt} % some space after
}
\makeatother
% -------------------------------------------------------------


% - Font Stuff starts   ---------------------------------------------------------
% \usepackage{xltxtra}

% % \usepackage{lmodern}
% 
% % \usepackage{amssymb,amsmath}
% \usepackage{ifxetex,ifluatex}
% \usepackage{fixltx2e} % provides \textsubscript

% % use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% % use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Fira Sans Light}
    \setsansfont[]{Fira Sans}
    \setmonofont[Mapping=tex-ansi,Scale=0.8]{FreeMono}
    \setmathfont(Digits,Latin,Greek)[]{Fira Sans Light}

% - Geometry  ------------------------------------------------------------------
\usepackage[margin=3cm]{geometry}

% - Layout ---------------------------------------------------------------------
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{titlesec}
\titleformat{\chapter}{\bfseries\huge}{\thechapter.}{20pt}{\huge}

% - Links ----------------------------------------------------------------------
\PassOptionsToPackage{usenames,dvipsnames}{xcolor} % color is loaded by hyperref
\ifxetex
\usepackage[pdfusetitle,setpagesize=false, % page size defined by xetex
  unicode=false, % unicode breaks when used with xetex
xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi

% Make sure url breaks
\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\hypersetup{
            pdftitle={A formalization of the \textbackslash{}lamy calculus},
            pdfauthor={Samuel Balco},
            colorlinks=true,
            linkcolor=cyan,
            citecolor=cyan,
            urlcolor=cyan,
            breaklinks=true}
\urlstyle{same}   % don't use monospace font for urls
\usepackage[all]{hypcap}% improve link placement in floats


% better names when cross-referencing with cleveref
\usepackage[nameinlink]{cleveref}
\makeatother
\crefname{listing}{Figure}{Figures}
\Crefname{listing}{Figure}{Figures}
\crefname{chapter}{Chapter}{Chapters}
\Crefname{chapter}{Chapter}{Chapters}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\crefname{subsection}{Section}{Sections}
\Crefname{subsection}{Section}{Sections}
\crefname{subsubsection}{Section}{Sections}
\Crefname{subsubsection}{Section}{Sections}
\crefname{figure}{Figure}{Figures} % changes default behavior to Figure. 1
\Crefname{figure}{Figure}{Figures} % changes default behavior to Figure. 1
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{subfigure}{Figure}{Figures}
\Crefname{subfigure}{Figure}{Figures}
\crefname{subsubfigure}{Figure}{Figures}
\Crefname{subsubfigure}{Figure}{Figures}
\crefname{appendix}{Appendix}{Appendices}
\Crefname{appendix}{Appendix}{Appendices}
\crefname{Definition}{Definition}{Definitions}
\Crefname{Definition}{Definition}{Definitions}
\crefname{Lemma}{Lemma}{Lemmas}
\Crefname{Lemma}{Lemma}{Lemmas}
\crefname{Example}{Example}{Examples}
\Crefname{Example}{Example}{Examples}

% - Language -------------------------------------------------------------------

% - Bibliography  --------------------------------------------------------------

% - Listings -------------------------------------------------------------------

% - Graphics  ------------------------------------------------------------------
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

% - Other Options --------------------------------------------------------------




% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}


\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}

\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\setcounter{secnumdepth}{5}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


% - Add to Header --------------------------------------------------------------
\usepackage{bussproofs}
\usepackage{amsthm}
\usepackage{minted}
\usepackage{tikz}
\usetikzlibrary{arrows,automata, positioning}
\newcommand{\lamy}{\lambda\text{-}Y}
\newcommand{\concat}{\ensuremath{+\!\!\!\!+\,}}
\newcommand{\wf}{\textsf{Wf-ICtxt}\ }
\newcommand{\red}{\Rightarrow_Y}
\newcommand{\cls}{{}^{\backslash x}}
\newcommand{\tocap}{\leadsto\kern-.5ex\cap}
\newcommand{\conR}{\concat_{\kern-1ex R}}
\newcommand{\conL}{\concat_{\kern-1ex L}}
\newcommand{\poplm}{\textsc{PoplMark}}
\renewcommand{\max}{\textsf{max}\ }
\newcommand{\dip}{\textsf{dp}}
\newcommand{\trm}{\textsf{term}}
\newcommand{\fv}{\textsf{FV}}
\newcommand{\taui}{\bigcap\nolimits_{\underline{n}} \tau_i}
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\usepackage{mdframed}
\mdfdefinestyle{example}{linewidth=2pt,topline=false,bottomline=false,rightline=false}
\let\OldExample\Example
\renewenvironment{Example}{\begin{OldExample}\begin{mdframed}[style=example, linecolor=yellow]}{\end{mdframed}\end{OldExample}}
\let\OldRemark\Remark
\renewenvironment{Remark}{\begin{OldRemark}\begin{mdframed}[style=example, linecolor=black]}{\end{mdframed}\end{OldRemark}}
\let\OldDefinition\Definition
\renewenvironment{Definition}{\begin{OldDefinition}\begin{mdframed}[style=example, linecolor=cyan]}{\end{mdframed}\end{OldDefinition}}
\let\OldLemma\Lemma
\renewenvironment{Lemma}{\begin{OldLemma}\begin{mdframed}[style=example, linecolor=cyan]}{\end{mdframed}\end{OldLemma}}
\let\OldTheorem\Theorem
\renewenvironment{Theorem}{\begin{OldTheorem}\begin{mdframed}[style=example, linecolor=cyan]}{\end{mdframed}\end{OldTheorem}}
\BeforeBeginEnvironment{minted}{\vspace{1em}\begin{mdframed}[style=example, linecolor=magenta]}
\AfterEndEnvironment{minted}{\end{mdframed}}

\pagenumbering{roman}

\begin{document}

% - Title ----------------------------------------------------------------------
\maketitle

% - Dedication -----------------------------------------------------------------
\begin{dedication}
This is a dedication
\end{dedication}

% - Acknowledgements -----------------------------------------------------------
\begin{acknowledgements}
Say thanks to whoever listened to your rants for 2 months
\end{acknowledgements}


% - Originality ----------------------------------------------------------------
\begin{originality}
This is the statement of originality
\end{originality}


% - Abstract -------------------------------------------------------------------
\begin{abstract}
This is the abstract. For this and the other front-matter options you
can either include the text directly on the metadata file or you can use
in order to include your text.
\end{abstract}



% - Table of Contents  ---------------------------------------------------------
% \begin{romanpages}
 \hypersetup{linkcolor=black} 
\setcounter{page}{1}
\setcounter{tocdepth}{2}
\tableofcontents 
% \end{romanpages}
% - List of Tables -------------------------------------------------------------
% \end{romanpages}
\newpage

 \hypersetup{linkcolor=cyan} 

% - BODY -----------------------------------------------------------------------
\pagenumbering{arabic}
\chapter{Introduction}\label{introduction}

\section{Motivation}\label{motivation}

Formal verification of software is essential in a lot of safety critical
systems in the industry and has been a field of active research in
computer science. One of the main approaches to verification is model
checking, wherein a system specification is checked against certain
correctness properties, by generating a model of the system, encoding
the desired correctness property as a logical formula and then
exhaustively checking whether the given formula is satisfiable in the
model of the system. Big advances in model checking of
1\textsuperscript{st} order (imperative) programs have been made, with
techniques like abstraction refinement and SAT/SMT-solver use, allowing
scalability.\\
Aspects of functional programming, such as anonymous/\(\lambda\)
functions have gained prominence in mainstream languages, such as C++ or
JavaScript and functional languages like Scala, F\# or Haskell have
garnered wider interest. With growing interest in using functional
programming, interest in verifying higher-order functional programs has
also grown. Current approaches to formal verification of such programs
usually involve the use of (automatic) theorem provers, which usually
require a lot of user interaction and as a result have not managed to
scale as well as model checking in the 1\textsuperscript{st} order
setting.\\
Using type systems is another way to ensure program safety, but using
expressive-enough types often requires explicit type annotations, since
type checking/inference usually becomes undecidable, as is the case for
dependent-type systems. Simpler type systems, where type inference is
decidable, can instead prove too coarse, i.e.~the required properties
are difficult if not impossible to capture in such type systems.\\
In recent years, advances in higher order model checking (HOMC) have
been made (C.-H. L. Ong (\protect\hyperlink{ref-ong06}{2006}), Kobayashi
(\protect\hyperlink{ref-kobayashi13}{2013}), Ramsay, Neatherway, and Ong
(\protect\hyperlink{ref-ramsay14}{2014}), Tsukada and Ong
(\protect\hyperlink{ref-tsukada14}{2014})), but whilst a lot of theory
has been developed for HOMC, there has been little done in
implementing/mechanizing these results in a fully formal setting of a
theorem prover.

\section{Aims}\label{aims}

The aim of this project is to make a start of mechanizing the proofs
underpinning HOMC approaches using type-checking of higher-order
recursion schemes, by formalizing and formally proving certain key
properties about the \(\lamy\) calculus with the intersection-type
system described by \textbf{??which one\ldots{}should I omit??}, which
can be used to study HOMC as an alternative to higher order recursion
schemes (HORS).\\
The first part of this work focuses on the mechanization aspect of the
simply typed \(\lamy\) calculus in a theorem prover, in a fashion
similar to the \(\poplm\) challenge, by exploring different encodings of
binders in a theorem prover and also the use of different theorem
provers. The reason why we chose to do such a comparison was to evaluate
and chose the best mechanization approach and implementation language
for the \(\lamy\) calculus, as there is little infomration available
concerning the merits and disadvantages of different implementation
approaches of \(\lamy\) or indeed just the (simply typed)
\(\lambda\)-calculus. The comparison of different mechanizations focuses
on the engineering choices and formalization overheads which result from
translating the informal definitions into a fully-formal setting of a
theorem prover. The project is roughly split into two main parts, with
the first part exploring and evaluating the different formalizations of
the simply-typed \(\lamy\) calculus together with the proof of the
Church Rosser Theorem. The reason why we chose to formalize the Church
Rosser theorem was to to test the implementation of a non-trivial, but
simple enough proof in a fully formal setting.\\
The second part focuses on implementing the intersection-type system for
the \(\lamy\) calculus and formalizing the proof of subject invariance
for this type system. The formalization and engineering choices made in
the implementation of the intersection-type system reflect the survey
and analysis of the different possible choices of mechanization,
explored in the first part of the project.

\section{Main Achievements}\label{main-achievements}

\textbf{TODO: Expand on the points eventually\ldots{}.leaving for the
end}

\begin{itemize}
\tightlist
\item
  Formalization of the simply typed \(\lamy\) calculus and proofs of
  confluence in Isabelle, using both Nominal sets and locally nameless
  encoding of binders.
\item
  Formalization of the simply typed \(\lamy\) calculus and proofs of
  confluence in Agda, using a locally nameless encoding of binders
\item
  Analysis and comparison of binder encodings
\item
  Comparison of Agda and Isabelle
\item
  Formalization of an intersection-type system for the \(\lamy\)
  calculus and proof of subject invariance for intersection-types
\end{itemize}

\chapter{Background}\label{background}

\section{Binders}\label{binders}

\label{binders}

When describing the (untyped) \(\lambda\)-calculus on paper, the terms
of the \(\lambda\)-calculus are usually inductively defined in the
following way:

\begin{center}
$t::= x\ |\ tt\ |\ \lambda x.t \text{ where }x \in Var$
\end{center}

This definition of terms yields an induction/recursion principle, which
can be used to define functions over the \(\lambda\)-terms by structural
recursion and prove properties about the \(\lambda\)-terms using
structural induction (recursion and induction being two sides of the
same coin).\\
However, whilst the definition above describes valid terms of the
\(\lambda\)-calculus, there are implicit assumptions one makes about the
terms, namely, the \(x\) in the \(\lambda x.t\) case appears bound in
\(t\). This means that while \(x\) and \(y\) might be distinct terms of
the \(\lambda\)-calculus (i.e. \(x \neq y\)), \(\lambda x.x\) and
\(\lambda y.y\) represent the same term, as \(x\) and \(y\) are bound by
the \(\lambda\). Without the notion of \(\alpha\)-equivalence of terms,
one cannot prove any properties of terms involving bound variables, such
as saying that \(\lambda x.x \equiv \lambda y.y\).

In an informal setting, reasoning with \(\alpha\)-equivalence of terms
is often very implicit, however in a formal setting of theorem provers,
having an inductive definition of ``raw'' \(lambda\)-terms, which are
not \(alpha\)-equivalent, yet reasoning about \(\alpha\)-equivalent
\(\lambda\)-terms poses certain challenges.\\
One of the main problems is the fact that the inductive/recursive
definition does not easily lift to \(alpha\)-equivalent terms. Take a
trivial example of a function on raw terms, which checks whether a
variable appears bound in a given \(\lambda\)-term. Clearly, such
function is well formed for ``raw'' terms, but does not work (or even
make sense) for \(\alpha\)-equivalent terms.\\
Conversely, there are informal definitions over \(\alpha\)-equivalent
terms, which are not straight-forward to define over raw terms. Take the
usual definition of substitution, defined over \(\alpha\)-equivalent
terms, which actually relies on this fact in the following case:

\begin{center}
$(\lambda y'. s')[t/x] \equiv \lambda y'.(s'[t/x]) \text{ assuming } y' \not\equiv x\text{ and }y' \not\in FV(t)$
\end{center}

Here in the \(\lambda\) case, it is assumed that a given
\(\lambda\)-term \(\lambda y. s\) can always be swapped out for an alpha
equivalent term \(\lambda y'. s'\), such that \(y'\) satisfies the side
condition. The assumption that a bound variable can be swapped out for a
``fresh'' one to avoid name clashes is often referred to as the
Barendregt Variable Convention.

The direct approach of defining ``raw'' terms and an additional notion
of \(\alpha\)-equivalence introduces a lot of overhead when defining
functions, as one either has to use the recursive principles for ``raw''
terms and then show that the function lifts to the \(\alpha\)-equivalent
terms or define functions on \(alpha\)-equivalence classes and prove
that it is well-founded, without being able to rely on the structurally
inductive principles that one gets ``for free'' with the ``raw''
terms.\\
Because of this, the usual informal representation of the
\(\lambda\)-calculus is rarely used in a fully formal setting.

To mitigate the overheads of a fully formal definition of the
\(\lambda\)-calculus, we want to have an encoding of the
\(\lambda\)-terms, which includes the notion of \(\alpha\)-equivalence
whilst being inductively defined, giving us the inductive/recursive
principles for \(alpha\)-equivalent terms directly. This can be achieved
in several different ways. In general, there are two main approaches
taken in a rigorous formalization of the terms of the lambda calculus,
namely the concrete approaches and the higher-order approaches, both
described in some detail below.

\subsection{Concrete approaches}\label{concrete-approaches}

The concrete or first-order approaches usually encode variables using
names (like strings or natural numbers). Encoding of terms and
capture-avoiding substitution must be encoded explicitly. A survey by B.
Aydemir et al. (\protect\hyperlink{ref-aydemir08}{2008}) details three
main groups of concrete approaches, found in formalizations of the
\(\lambda\)-calculus in the literature:

\subsubsection{Named}\label{named}

This approach generally defines terms in much the same way as the
informal inductive definition given above. Using a functional language,
such as Haskell or ML, such a definition might look like this:

\begin{minted}[]{isabelle}
datatype trm =
  Var name
| App trm trm
| Lam name trm
\end{minted}

As was mentioned before, defining ``raw'' terms and the notion of
\(\alpha\)-equivalence of ``raw'' terms separately carries a lot of
overhead in a theorem prover and is therefore not favored.

To obtain an inductive definition of \(\lambda\)-terms with a built in
notion of \(\alpha\)-equivalence, one can instead use nominal sets
(\textbf{described in the section on nominal sets/Isabelle?}). The
nominal package in Isabelle provides tools to automatically define terms
with binders, which generate inductive definitions of
\(\alpha\)-equivalent terms. Using nominal sets in Isabelle results in a
definition of terms which looks very similar to the informal
presentation of the lambda calculus:

\begin{minted}[]{isabelle}
nominal_datatype trm =
  Var name
| App trm trm
| Lam x::name l::trm  binds x in l
\end{minted}

Most importantly, this definition allows one to define functions over
\(\alpha\)-equivalent terms using structural induction. The nominal
package also provides freshness lemmas and a strengthened induction
principle with name freshness for terms involving binders.

\subsubsection{Nameless/de Bruijn}\label{namelessde-bruijn}

Using a named representation of the lambda calculus in a fully formal
setting can be inconvenient when dealing with bound variables. For
example, substitution, as described in the introduction, with its
side-condition of freshness of \(y\) in \(x\) and \(t\) is not
structurally recursive on ``raw'' terms, but rather requires
well-founded recursion over \(\alpha\)-equivalence classes of terms. To
avoid this problem in the definition of substitution, the terms of the
lambda calculus can be encoded using de Bruijn indices:

\begin{minted}[]{isabelle}
datatype trm =
  Var nat
| App trm trm
| Lam trm
\end{minted}

This representation of terms uses indices instead of named variables.
The indices are natural numbers, which encode an occurrence of a
variable in a \(\lambda\)-term. For bound variables, the index indicates
which \(\lambda\) it refers to, by encoding the number of
\(\lambda\)-binders that are in the scope between the index and the
\(\lambda\)-binder the variable corresponds to.

\begin{Example}

The term \(\lambda x.\lambda y. yx\) will be represented as
\(\lambda\ \lambda\ 0\ 1\). Here, 0 stands for \(y\), as there are no
binders in scope between itself and the \(\lambda\) it corresponds to,
and \(1\) corresponds to \(x\), as there is one \(\lambda\)-binder in
scope. To encode free variables, one simply choses an index greater than
the number of \(\lambda\)'s currently in scope, for example,
\(\lambda\ 4\).

\end{Example}

To see that this representation of \(\lambda\)-terms is isomorphic to
the usual named definition, we can define two function \(f\) and \(g\),
which translate the named representation to de Bruijn notation and vice
versa. More precisely, since we are dealing with \(\alpha\)-equivalence
classes, its is an isomorphism between these that we can formalize.

To make things easier, we consider a representation of named terms,
where we map named variables, \(x, y, z,...\) to indexed variables
\(x_1,x_2,x_3,...\). Then, the mapping from named terms to de Bruijn
term is given by \(f\), which we define in terms of an auxiliary
function \(e\):

\begin{center}
$\begin{aligned}
e_k^m(x_n) &= \begin{cases}
k-m(x_n)-1 & x_n \in \text{dom }m\\
k+n & otherwise
\end{cases}\\
e_k^m(uv) &= e_k^m(u)\ e_k^m(v)\\
e_k^m(\lambda x_n.u) &= \lambda\ e_{k+1}^{m \oplus (x_n,k)}(u)
\end{aligned}$
\end{center}

Then \(f(t) \equiv e_0^\emptyset(t)\)

The function \(e\) takes two additional parameters, \(k\) and \(m\).
\(k\) keeps track of the scope from the root of the term and \(m\) is a
map from bound variables to the levels they were bound at. In the
variable case, if \(x_n\) appears in \(m\), it is a bound variable, and
it's index can be calculated by taking the difference between the
current index and the index \(m(x_k)\), at which the variable was bound.
If \(x_n\) is not in \(m\), then the variable is encoded by adding the
current level \(k\) to \(n\).\\
In the abstraction case, \(x_n\) is added to \(m\) with the current
level \(k\), possibly overshadowing a previous binding of the same
variable at a different level (like in
\(\lambda x_1. (\lambda x_1. x_1)\)) and \(k\) is incremented, going
into the body of the abstraction.

The function \(g\), taking de Bruijn terms to named terms is a little
more tricky. We need to replace indices encoding free variables (those
that have a value greater than or equal to \(k\), where \(k\) is the
number of binders in scope) with named variables, such that for every
index \(n\), we substitute \(x_m\), where \(m = n-k\), without capturing
these free variables.

We need two auxiliary functions to define \(g\):

\begin{center}
$\begin{aligned}
h_k^b(n) &= \begin{cases}
x_{n-k} & n \geq k\\
x_{k+b-n-1} & otherwise
\end{cases}\\
h_k^b(uv) &= h_k^b(u)\ h_k^b(v)\\
h_k^b(\lambda u) &= \lambda x_{k+b}.\ h_{k+1}^b(u)\\[2.5em]
\Diamond_k(n) &= \begin{cases}
n-k & n \geq k\\
0 & otherwise
\end{cases}\\
\Diamond_k(uv) &= \max (\Diamond_k(u),\ \Diamond_k(v))\\
\Diamond_k(\lambda u) &= \Diamond_{k+1}(u)
\end{aligned}$
\end{center}

The function \(g\) is then defined as
\(g(t) \equiv h_0^{\Diamond_0(t)+1}(t)\). As mentioned above, the
complicated definition has to do with avoiding free variable capture. A
term like \(\lambda (\lambda\ 2)\) intuitively represents a named
\(\lambda\)-term with two bound variables and a free variable \(x_0\)
according to the definition above. If we started giving the bound
variables names in a naive way, starting from \(x_0\), we would end up
with a term \(\lambda x_0.(\lambda x_1.x_0)\), which is obviously not
the term we had in mind, as \(x_0\) is no longer a free variable. To
ensure we start naming the bound variables in such a way as to avoid
this situation, we use \(\Diamond\) to compute the maximal value of any
free variable in the given term, and then start naming bound variables
with an index one higher than the value returned by \(\Diamond\).

As one quickly notices, a term like \(\lambda x.x\) and \(\lambda y.y\)
have a single unique representation as a de Bruijn term \(\lambda\ 0\).
Indeed, since there are no named variables in a de Bruijn term, there is
only one way to represent any \(\lambda\)-term, and the notion of
\(\alpha\)-equivalence is no longer relevant. We thus get around our
problem of having an inductive principle and \(\alpha\)-equivalent
terms, by having a representation of \(\lambda\)-terms where every
\(\alpha\)-equivalence class of \(\lambda\)-terms has a single
representative term in the de Bruijn notation.

In their comparison between named vs.~nameless/de Bruijn representations
of \(\lambda\)-terms, Berghofer and Urban
(\protect\hyperlink{ref-berghofer06}{2006}) give details about the
definition of substitution, which no longer needs the variable
convention and can therefore be defined using primitive structural
recursion.\\
The main disadvantage of using de Bruijn indices is the relative
unreadability of both the terms and the formulation of properties about
these terms. For instance, take the substitution lemma, which in the
named setting would be stated as:

\begin{center}
$\text{If }x \neq y\text{ and }x \not\in FV(L)\text{, then }
M[N/x][L/y] \equiv M[L/y][N[L/y]/x].$
\end{center}

In de Bruijn notation, the statement of this lemma becomes:

\begin{center}
$\text{For all indices }i, j\text{ with }i \leq j\text{, }M[N/i][L/j] = M[L/j + 1][N[L/j - i]/i]$
\end{center}

Clearly, the first version of this lemma is much more intuitive.

\subsubsection{Locally Nameless}\label{locally-nameless}

The locally nameless approach to binders is a mix of the two previous
approaches. Whilst a named representation uses variables for both free
and bound variables and the nameless encoding uses de Bruijn indices in
both cases as well, a locally nameless encoding distinguishes between
the two types of variables.\\
Free variables are represented by names, much like in the named version,
and bound variables are encoded using de Bruijn indices. By using de
Bruijn indices for bound variables, we again obtain an inductive
definition of terms which are already \(alpha\)-equivalent.

While closed terms, like \(\lambda x.x\) and \(\lambda y.y\) are
represented as de Bruijn terms, the term \(\lambda x.xz\) and
\(\lambda x.xz\) are encoded as \(\lambda\ 0z\). The following
definition captures the syntax of the locally nameless terms:

\begin{minted}[]{isabelle}
datatype ptrm =
  Fvar name
  BVar nat
| App trm trm
| Lam trm
\end{minted}

Note however, that this definition doesn't quite fit the notion of
\(\lambda\)-terms, since a \texttt{pterm} like \texttt{(BVar 1)} does
not represent a \(\lambda\)-term, since bound variables can only appear
in the context of a lambda, such as in \texttt{(Lam (BVar 1))}.\\
The advantage of using a locally nameless definition of
\(\lambda\)-terms is a better readability of such terms, compared to
equivalent de Bruijn terms. Another advantage is the fact that
definitions of functions and reasoning about properties of these terms
is much closer to the informal setting.

\subsection{Higher-Order approaches}\label{higher-order-approaches}

Unlike concrete approaches to formalizing the lambda calculus, where the
notion of binding and substitution is defined explicitly in the host
language, higher-order formalizations use the function space of the
implementation language, which handles binding. HOAS, or higher-order
abstract syntax (F. Pfenning and Elliott
\protect\hyperlink{ref-pfenning88}{1988}, Harper, Honsell, and Plotkin
(\protect\hyperlink{ref-harper93}{1993})), is a framework for defining
logics based on the simply typed lambda calculus. A form of HOAS,
introduced by Harper, Honsell, and Plotkin
(\protect\hyperlink{ref-harper93}{1993}), called the Logical Framework
(LF) has been implemented as Twelf by Frank Pfenning and Schürmann
(\protect\hyperlink{ref-pfenning99}{1999}), which has been previously
used to encode the \(\lambda\)-calculus.\\
Using HOAS for encoding the \(\lambda\)-calculus comes down to encoding
binders using the meta-language binders. This way, the definitions of
capture avoiding substitution or notion of \(\alpha\)-equivalence are
offloaded onto the meta-language. As an example, take the following
definition of terms of the \(\lambda\)-calculus in Haskell:

\begin{minted}[]{haskell}
data Term where
  Var :: Int -> Term
  App :: Term -> Term -> Term
  Lam :: (Term -> Term) -> Term
\end{minted}

This definition avoids the need for explicitly defining substitution,
because it encodes a \(\lambda\)-term as a Haskell function
\texttt{(Term -> Term)}, relying on Haskell's internal substitution and
notion of \(\alpha\)-equivalence. As with the de Bruijn and locally
nameless representations, this encoding gives us inductively defined
terms with a built in notion of \(\alpha\)-equivalence.\\
However, using HOAS only works if the notion of \(\alpha\)-equivalence
and substitution of the meta-language coincide with these notions in the
object-language.

\newpage

\section{Simple types}\label{simple-types}

The simple types presented throughout this work (except for
\cref{chap:itypes}) are often referred to as simple types \emph{a la
Curry}, where a simply typed \(\lambda\)-term is a triple
\((\Gamma, M, \sigma)\) s.t. \(\Gamma \vdash M : \sigma\), where
\(\Gamma\) is the typing context, \(M\) is a term of the untyped
\(\lambda\)-calculus and \(\sigma\) is a simple type.

\begin{Definition}[Simple-type assignment]

\begin{center}
  \vskip 1.5em
  \AxiomC{$x : A \in \Gamma$}
  \LeftLabel{$(var)$}
  \UnaryInfC{$\Gamma \vdash x : A$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$\Gamma \vdash u : A \to B$}
  \AxiomC{$\Gamma \Vdash v : A$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$\Gamma \vdash uv : B$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{$x : A,\Gamma \vdash m : B$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\Gamma \vdash \lambda.m : A \to B$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$\Gamma \vdash Y_{A} : (A \to A) \to A$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

Such a term is deemed valid, if one can construct a typing tree from the
given type and typing context.

\begin{Example}

Take the following simply typed term
\(\{y:\tau\} \vdash \lambda x.xy : (\tau \to \phi) \to \phi\). To show
that this is a well-typed \(\lambda\)-term, we construct the following
typing tree:

\begin{center}
    \vskip 1.5em
    \AxiomC{}
    \LeftLabel{$(var)$}
    \UnaryInfC{$\{x: \tau \to \phi,\ y:\tau\} \vdash x : \tau \to \phi$}
    \AxiomC{}
    \LeftLabel{$(var)$}
    \UnaryInfC{$\{x: \tau \to \phi,\ y:\tau\} \vdash y : \tau$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\{x: \tau \to \phi,\ y:\tau\} \vdash xy : \phi$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\{y:\tau\} \vdash \lambda x.xy : (\tau \to \phi) \to \phi$}
    \DisplayProof
\end{center}

\end{Example}

In the untyped \(\lambda\)-calculus, simple types and \(\lambda\)-terms
are completely separate, brought together only through the typing
relation \(\vdash\) in the case of simple types \emph{a la Curry}. The
definition of \(\lamy\) terms, however, is dependent on the simple types
in the case of the \(Y\) constants, which are indexed by simple types.
When talking about the \(\lamy\) calculus, we tend to conflate the
``untyped'' \(\lamy\) terms, which are just the terms defined in
\cref{Definition:lamyTrms}, with the ``typed'' \(\lamy\) terms, which
are simply-typed terms \emph{a la Curry} of the form
\(\Gamma \vdash M : \sigma\), where \(M\) is an ``untyped'' \(\lamy\)
term. Thus, results about the \(\lamy\) calculus in this work are in
fact results about the ``typed'' \(\lamy\) calculus.\\
However, the proofs of the Church Rosser theorem, as presented in the
next section, use the untyped definition of \(\beta\)-reduction. Whilst
it is possible to define a typed version of \(\beta\)-reduction, it
turned out to be much easier to first prove the Church Rosser theorem
for the so called ``untyped'' \(\lamy\) calculus and the additionally
restrict this result to only well-types \(\lamy\) terms (see
\cref{utypReason} for more details). Thus, the definition of the Church
Rosser Theorem, formulated for the \(\lamy\) calculus, is the following
one:

\begin{Theorem}[Church Rosser]

\(\Gamma \vdash M : \sigma \land M \red^* M' \land M \red^* M'' \implies \exists M'''.\ \ M' \red^* M''' \land M'' \red^* M''' \land \Gamma \vdash M''' : \sigma\)

\end{Theorem}

In order to prove this typed version of the Church Rosser Theorem, we
need to prove an additional result of subject reduction for \(\lamy\)
calculus, namely:

\begin{Theorem}[Subject reduction for $\red^*$]

\(\Gamma \vdash M : \sigma \land M \red^* M' \implies \Gamma \vdash M' : \sigma\)

\end{Theorem}

\section{\texorpdfstring{\(\lamy\)
calculus}{\textbackslash{}lamy calculus}}\label{lamy-calculus}

Originally, the field of higher order model checking mainly involved
studying higher order recursion schemes (HORS), but more recently,
exploring the \(\lamy\) calculus, which is an extension of the simply
typed \(\lambda\)-calculus, in the context of HOMC has gained traction
(Clairambault and Murawski
(\protect\hyperlink{ref-clairambault13}{2013})). We therefore present
the \(\lamy\) calculus, along with the proofs of the Church Rosser
theorem and the formalization of intersection types for the \(\lamy\)
calculus, as the basis for formalizing the theory of HOMC.

\subsection{Definitions}\label{definitions}

The first part of this project focuses on formalizing the simply typed
\(\lamy\) calculus and the proof of confluence for this calculus (proof
of the Church Rosser Theorem is sometimes also referred to as proof of
confluence). The usual/informal definition of the \(\lamy\) terms and
the simple types are given below:

\begin{Definition}[$\lamy$ types and terms]

The set of simple types \(\sigma\) is built up inductively form the
\(\mathsf{o}\) constant and the arrow type \(\to\).\\
Let \(Var\) be a countably infinite set of atoms in the definition of
the set of \(\lambda\)-terms \(M\): \label{Definition:lamyTrms}

\begin{center}
$\begin{aligned}
\sigma ::=&\ \mathsf{o}\ |\ \sigma \to \sigma \\
M ::=&\ x\ |\ MM\ |\ \lambda x.M\ |\ Y_\sigma \text{ where }x \in Var
\end{aligned}$
\end{center}

\end{Definition}

The \(\lamy\) calculus differs from the simply typed
\(\lambda\)-calculus only in the addition of the \(Y\) constant family,
indexed at every simple type \(\sigma\), where the (simple) type of a
\(Y_A\) constant (indexed with the type \(A\)) is \((A \to A) \to A\).
The usual definition of \(\beta\)-reduction is then augmented with the
\((Y)\) rule (this is the typed version of the rule):

\begin{center}
  \vskip 1.5em
  \AxiomC{$\Gamma \vdash M : \sigma \to \sigma$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$\Gamma \vdash Y_\sigma M \red M (Y_\sigma M) : \sigma$}
  \DisplayProof
  \vskip 1.5em
\end{center}

In essence, the \(Y\) rule allows (some) well-typed recursive
definitions over simply typed \(\lambda\)-terms.

\begin{Example}

Take for example the term \(\lambda x.x\), commonly referred to as the
\emph{identity}. The \emph{identity} term can be given a type
\(\sigma \to \sigma\) for any simple type \(\sigma\). We can therefore
perform the following (well-typed) reduction in the \(\lamy\) calculus:

\begin{center}
$Y_\sigma (\lambda x.x) \red (\lambda x.x)(Y_\sigma (\lambda x.x))$
\end{center}

\end{Example}

The typed version of the rule illustrates the restricted version of
recursion clearly, since a recursive ``\(Y\)-reduction'' will only occur
if the term \(M\) in \(Y_\sigma M\) has the matching type
\(\sigma \to \sigma\) (to \(Y_\sigma\)'s type
\((\sigma \to \sigma) \to \sigma\)), as in the example above.

\subsection{Church-Rosser Theorem}\label{church-rosser-theorem}

\label{cr-def}

The Church-Rosser Theorem states that the \(\beta\)-reduction of the
\(\lambda\)-calculus is confluent, that is, the reflexive-transitive
closure of the \(\beta\)-reduction has the \emph{diamond property}, i.e.
\(\dip(\red^*)\), where:

\begin{Definition}[$\dip(R)$]

A binary relation \(R\) has the \emph{diamond property}, i.e.
\(\dip(R)\), iff

\begin{center}
$\forall a, b, c.\ aRb \land aRc \implies \exists d.\ bRd \land cRd$
\end{center}

\end{Definition}

The proof of confluence of \(\red\), the \(\beta Y\)-reduction defined
as the standard \(\beta\)-reduction with the addition of the
aforementioned \((Y)\) rule, formalized in this project, follows a
variation of the Tait-Martin-Löf Proof originally described in Takahashi
(\protect\hyperlink{ref-takahashi95}{1995}) (specifically using the
notes by R. Pollack (\protect\hyperlink{ref-pollack95}{1995})). To show
why following this proof over the traditional proof is beneficial, we
first give a high level overview of how the usual proof proceeds.

\subsubsection{Overview}\label{overview}

In the traditional proof of the Church Rosser theorem, we define a new
reduction relation, called the \emph{parallel} \(\beta\)-reduction
(\(\gg\)), which, unlike the ``plain'' \(\beta\)-reduction satisfies the
\emph{diamond property} (note that we are talking about the ``single
step'' \(\beta\)-reduction and not the reflexive transitive closure).
Once we prove the \emph{diamond property} for \(\gg\), the proof of
\(\dip(\gg^*)\) follows easily. The reason why we prove \(\dip(\gg)\) in
the first place is because the reflexive-transitive closure of \(\gg\)
coincides with the reflexive transitive closure of \(\red\) and it is
much easier to prove \(\dip(\gg)\) than trying to prove \(\dip(\red^*)\)
directly. The usual proof of the \emph{diamond property} for \(\gg\)
involves a double induction on the shape of the two parallel
\(\beta\)-reductions from \(M\) to \(P\) and \(Q\), where we try to show
that the following diamond always exists, that is, given any reductions
\(M \gg P\) and \(M \gg Q\), there is some \(M'\) s.t. \(P \gg M'\) and
\(Q \gg M'\):

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,semithick]
  \tikzstyle{every state}=[fill=none,draw=none,text=black]

  \node[state] (A)                                     {$M$};
  \node[state] (B) [below left= 1.3cm and 1.3cm of A]  {$P$};
  \node[state] (C) [below right= 1.3cm and 1.3cm of A] {$Q$};
  \node[state] (D) [below= 3cm of A]                   {$M'$};
 
  \path (A) edge [left]          node [pos=0.4] {$\gg$} (B)
            edge                 node [pos=0.59]           {$\gg$} (C)
        (B) edge [left, dashed]  node           {$\gg$} (D)
        (C) edge [right, dashed] node           {$\gg$} (D);
\end{tikzpicture}
\end{center}
\caption{The diamond property of $\gg$, visualized}
\end{figure}

The Takahashi (\protect\hyperlink{ref-takahashi95}{1995}) proof
simplifies this proof by eliminating the need to do simultaneous
induction on the \(M \gg P\) and \(M \gg Q\) reductions. This is done by
introducing another reduction, referred to as the \emph{maximal
parallel} \(\beta\)-reduction (\(\ggg\)). The idea of using \(\ggg\) is
to show that for every term \(M\) there is a reduct term \(M_{max}\)
s.t. \(M \ggg M_{max}\) and that any \(M'\), s.t. \(M \gg M'\), also
reduces to \(M_{max}\). We can then separate the ``diamond'' diagram
above into two instances of the following triangle, where \(M'\) from
the previous diagram is \(M_{max}\):

\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,semithick]
  \tikzstyle{every state}=[fill=none,draw=none,text=black]

  \node[state] (A)                                    {$M$};
  \node[state] (B) [below left= 1.3cm and 1.3cm of A] {$M'$};
  \node[state] (D) [below= 3cm of A]                  {$M_{max}$};
 
  \path (A) edge [left]         node [pos=0.4] {$\gg$}  (B)
            edge                node           {$\ggg$} (D)
        (B) edge [left, dashed] node           {$\gg$}  (D);
\end{tikzpicture}
\end{center}
\caption{The proof of $\dip(\gg)$ is split into two instances of this triangle}
\label{figure:gggTriangle}
\end{figure}

\newpage

\subsubsection{\texorpdfstring{Parallel
\(\beta Y\)-reduction}{Parallel \textbackslash{}beta Y-reduction}}\label{parallel-beta-y-reduction}

Having described the high-level overview of the classical proof and the
reason for following the Takahashi
(\protect\hyperlink{ref-takahashi95}{1995}) proof, we now present some
of the major lemmas in more detail.\\
Firstly, we give the definition of \emph{parallel \(\beta Y\)-reduction}
\(\gg\) formulated for the terms of the \(\lamy\) calculus, which allows
simultaneous reduction of multiple parts of a term:

\begin{Definition}[$\gg$]

\(\ \)

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$x \gg x$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(refl_Y)$}
  \UnaryInfC{$Y_\sigma \gg Y_\sigma$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \gg M'$}
  \AxiomC{$N \gg N'$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$MN \gg M'N'$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$M \gg M'$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\lambda x. M \gg \lambda x. M'$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \gg M'$}
  \AxiomC{$N \gg N'$}
  \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda x. M)N \gg M'[N'/x]$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \gg M'$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$Y_\sigma M \gg M' (Y_\sigma M')$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

The most basic difference between the normal \(\beta\)-reduction and
\emph{parallel} \(\beta Y\)-reduction is the \((refl)/(refl_Y)\) rule,
where \(x \gg x\), for example, is a valid reduction, but we have
\(x \nRightarrow_Y x\) for the normal \(\beta Y\)-reduction
(\(x \red^* x\) is valid, since \(\red^*\) is the reflexive transitive
closure of \(\red\)). In the example below, \((refl^*)\) is a derived
rule \(\forall M.\ M \gg M\) (see \cref{Lemma:reflM}):

\begin{Example}

\label{Example:ggVsGgg} Another example where the two reductions differ
is the simultaneous reduction of multiple sub-terms. \emph{Parallel}
reduction, unlike \(\red\), allows the reduction of the term
\(((\lambda xy.x)z)(\lambda x.x)y\) to \((\lambda y.z)y\), by
simultaneously reducing the two sub-terms \((\lambda xy.x)z\) and
\((\lambda x.x)y\) to \(\lambda y.z\) and \(y\) respectively:

\begin{center}
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(refl^*)$}
  \UnaryInfC{$\lambda xy.x \gg \lambda xy.x$}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$z \gg z$}
  \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda xy.x)z \gg \lambda y.z$}
  \AxiomC{}
  \LeftLabel{$(refl^*)$}
  \UnaryInfC{$\lambda x.x \gg \lambda x.x$}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$y \gg y$}
  \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda x.x)y \gg y$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$((\lambda xy.x)z)(\lambda x.x)y \gg (\lambda y.z)y$}
  \DisplayProof
  \vskip 1.5em
\end{center}

When we try to construct a similar tree for \(\beta\)-reduction, we can
clearly see that the only two rules we can use are \((red_L)\) or
\((red_R)\). We can thus only perform the right-side or the left side
reduction of the two sub-terms, but not both (for the rules of normal
\(\beta\)-reduction see \cref{Definition:betaRedNom}).

\end{Example}

Having described the intuition behind the \emph{parallel}
\(\beta\)-reduction, we proceed to define the \emph{maximum parallel
reduction} \(\ggg\), which contracts all redexes in a given term with a
single step:

\begin{Definition}[$\ggg$]

\(\ \)

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$x \ggg x$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(refl_Y)$}
  \UnaryInfC{$Y_\sigma \ggg Y_\sigma$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \ggg M'$}
  \AxiomC{$N \ggg N'$}
  \LeftLabel{$(app)$}
  \RightLabel{($M$ is not a $\lambda$ or $Y$)}
  \BinaryInfC{$MN \ggg M'N'$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$M \ggg M'$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\lambda x. M \ggg \lambda x. M'$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \ggg M'$}
  \AxiomC{$N \ggg N'$}    \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda x. M)N \ggg M'[N'/x]$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \ggg M'$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$Y_\sigma M \ggg M' (Y_\sigma M')$}
  \DisplayProof
\end{center}

\end{Definition}

This relation only differs from \(\gg\) in the \((app)\) rule, which can
only be applied if \(M\) is not a \(\lambda\) or \(Y\) term.

\begin{Example}

To demonstrate the difference between \(\gg\) and \(\ggg\), we take a
look at the term \((\lambda xy.x)((\lambda x.x)z)\).\\
Whilst \((\lambda xy.x)((\lambda x.x)z) \gg (\lambda xy.x)z\) or
\((\lambda xy.x)((\lambda x.x)z) \gg \lambda y.z\) (amongst others) are
valid reductions, the reduction
\((\lambda xy.x)((\lambda x.x)z) \ggg (\lambda xy.x)z\) is not valid.\\
To see why this is the case, we observe that the last rule applied in
the derivation tree must have been the \((app)\) rule, since we see that
a reduction on the sub-term \((\lambda x.x)z \ggg z\) occurs:

\begin{center}
  \AxiomC{$\vdots$}
  \UnaryInfC{$\lambda xy.x \ggg \lambda xy.x$}

  \AxiomC{$\vdots$}
  \UnaryInfC{$(\lambda x.x)z \ggg z$}
  \LeftLabel{$(app)$}
  \RightLabel{($\lambda xy.x$ is not a $\lambda$ or $Y$)}
  \BinaryInfC{$(\lambda xy.x)(\lambda x.x)z \ggg (\lambda xy.x)z$}
  \DisplayProof
\end{center}

However, this clearly could not happen, because \(\lambda xy.x\) is in
fact a \(\lambda\)-term.

\end{Example}

To prove \(\dip(\gg)\), we first show that there always exists a term
\(M_{max}\) for every term \(M\), where \(M \ggg M_{max}\) is the
maximal parallel reduction which contracts all redexes in \(M\):

\begin{Lemma}

\label{Lemma:maxEx} \(\forall M.\ \exists M_{max}.\ M \ggg M_{max}\)

\begin{proof}

By induction on M.

\end{proof}

\end{Lemma}

Finally, we show that any parallel reduction \(M \gg M'\) can be
``closed'' by reducing to the term \(M_{max}\) where all redexes have
been contracted (as seen in \cref{figure:gggTriangle}):

\begin{Lemma}

\label{Lemma:maxClose}
\(\forall M, M', M_{max}.\ M \ggg M_{max} \land M \gg M' \implies M' \gg M_{max}\)

\begin{proof}

Omitted. Can be found on p.~8 of the R. Pollack
(\protect\hyperlink{ref-pollack95}{1995}) notes.

\end{proof}

\end{Lemma}

\begin{Lemma}

\(\dip(\gg)\)

\begin{proof}

We can now prove \(\dip(\gg)\) by simply applying \cref{Lemma:maxClose}
twice, namely for any term \(M\) there is an \(M_{max}\) s.t.
\(M \ggg M_{max}\) (by \cref{Lemma:maxEx}) and for any \(M', M''\) where
\(M \gg M'\) and \(M \gg M''\), it follows by two applications of
\cref{Lemma:maxClose} that \(M' \gg M_{max}\) and \(M'' \gg M_{max}\).

\end{proof}

\end{Lemma}

\newpage

\section{Intersection types}\label{intersection-types}

\label{itypesIntro}

For the formalization of intersection types, we initially chose a strict
intersection-type system, presented in the Bakel
(\protect\hyperlink{ref-bakel}{2003}) notes. Intersection types, as
classically presented in Barendregt, Dekkers, and Statman
(\protect\hyperlink{ref-barendregt13}{2013}) as \(\lambda_\cap^{BCD}\),
extend simple types by adding a conjunction to the definition of types:

\begin{Definition}[$\lambda_\cap^{BCD}$ types]

\begin{center}
$\mathcal{T} ::= \phi\ |\ \mathcal{T} \leadsto \mathcal{T}\ |\ \mathcal{T} \cap \mathcal{T}$
\end{center}

\end{Definition}

We restrict ourselves to a version of intersection types often called
\emph{strict intersection types}. \emph{Strict intersection types} are a
restriction on \(\lambda_\cap^{BCD}\) types, where an intersection of
types can only appear on the left side of an ``arrow'' type:

\begin{Definition}[Strict intersection types]

In the definition below, \(\phi\) is a constant (analogous to the
constant \(\mathsf{o}\), introduced for the simple types in
\cref{Definition:lamyTrms}).

\begin{center}
$\begin{aligned}
\mathcal{T}_s &::= \phi\ |\ \mathcal{T} \leadsto \mathcal{T}_s \\ 
\mathcal{T} &::= (\mathcal{T}_s \cap\hdots\cap \mathcal{T}_s)
\end{aligned}$
\end{center}

\end{Definition}

The following conventions for intersection types are adopted throughout
this section; \(\omega\) stands for the empty intersection and we write
\(\taui\) for the type \(\tau_1 \cap\hdots\cap \tau_n\). We also define
a subtype relation \(\subseteq\) for intersection types, which
intuitively capture the idea of one intersection of types being a subset
of another, where we think of \(\tau_1 \cap \hdots \cap \tau_i\) as a
finite set \(\{\tau_1, \hdots , \tau_i\}\), wherein \(\subseteq\) for
intersection types corresponds to subset inclusion e.g.
\(\tau \subseteq \tau \cap \psi\) because
\(\{\tau\} \subseteq \{\tau, \psi\}\).

\(\ \)

\begin{Remark}

The reason for defining the subset relation in this way, rather than
taking the usual view of \(\tau \cap \phi \leq \tau\), was due the
implementation of intersection types in Agda. Since intersection types
\(\mathcal{T}\) ended up being defined as lists of strict types
\(\mathcal{T}_s\) (the definition of lists in Agda included the notion
of list inclusion \(\in\) and by extension the \(\subseteq\) relation),
the above convention seemed more natural.

\end{Remark}

The formal definition of this relation is given below:

\begin{Definition}[$\subseteq$]

\label{Definition:subseteqOrig} This relation is the least pre-order on
intersection types s.t.:

\begin{center}
$\begin{aligned}
\forall\ i \in \underline{n}.\ \ \tau_i \subseteq& \taui \\ 
\forall\ i \in \underline{n}.\ \ \tau_i \subseteq \tau \implies& \taui \subseteq \tau \\
\rho \subseteq \psi \land \tau \subseteq \mu \implies& \psi \leadsto \tau \subseteq \rho \leadsto \mu\\
\end{aligned}$
\end{center}

(This relation is equivalent the \(\leq\) relation, defined in R.
Pollack (\protect\hyperlink{ref-pollack95}{1995}) notes, i.e.
\(\tau \leq \psi \equiv \psi \subseteq \tau\).)

\end{Definition}

In this presentation, \(\lamy\) terms are typed with the strict types
\(\mathcal{T}_s\) only. Much like the simple types, presented in the
previous sections, an intersection-typing judgment is a triple
\(\Gamma, M, \tau\), written as \(\Gamma \vDash M : \tau\), where
\(\Gamma\) is the intersection-type context, similar in construction to
the simple typing context, \(M\) is a \(\lamy\) term and \(\tau\) is a
strict intersection type \(\mathcal{T}_s\).\\
The definition of the intersection-typing system, like the \(\subseteq\)
relation, has also been adapted from the typing system found in the R.
Pollack (\protect\hyperlink{ref-pollack95}{1995}) notes, by adding the
typing rule for the \(Y\) constants:

\begin{Definition}[Intersection-type assignment]

\(\ \)

\begin{center}
  \AxiomC{$x: \taui \in \Gamma$}
  \AxiomC{$\tau \subseteq \taui$}
  \LeftLabel{$(var)$}
  \BinaryInfC{$\Gamma \Vdash x : \tau$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$\Gamma \Vdash M : \taui \leadsto \tau$}
  \AxiomC{$\forall\ i \in \underline{n}.\ \ \Gamma \Vdash N : \tau_i$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$\Gamma \Vdash MN : \tau$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{$x: \taui,\Gamma \Vdash M : \tau$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\Gamma \Vdash \lambda x.M : \taui \leadsto \tau$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \RightLabel{$(j \in \underline{n})$}
  \UnaryInfC{$\Gamma \Vdash Y_\sigma : (\taui \leadsto \tau_1 \cap\hdots\cap \taui \leadsto \tau_i) \leadsto \tau_j$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

This is the initial definition, used as a basis for the mechanization,
discussed in \cref{chap:itypes}. Due to different obstacles in the
formalization of the subject invariance proofs, this definition, along
with the definition of intersection types was amended several times. The
reasons for these changes are documented in \cref{chap:itypes}.\\
The definition above also assumes that the context \(\Gamma\) is
\emph{well-formed}:

\begin{Definition}[Well-formed intersection-type context]

Assuming that \(\Gamma\) is a finite list, consisting of pairs of atoms
\(Var\) and intersection types \(\mathcal{T}\), \(\Gamma\) is a
\emph{well-formed} context iff:\(\\\)

\begin{center}
  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\wf [\ ]$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$x \not\in \mathsf{dom}\ \Gamma$}
  \AxiomC{$\wf \Gamma$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\wf (x: \bigcap\tau_i,\Gamma)$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

\chapter{Methodology}\label{methodology}

\section{Comparison of
formalizations}\label{comparison-of-formalizations}

The idea of formalizing a functional language in multiple theorem
provers and objectively assessing the merits and pitfalls of the
different formalizations is definitely not a new idea. The most well
known attempt to do so on a larger scale is the \(\poplm\) challenge,
proposed in the ``Mechanized Metatheory for the Masses: The \(\poplm\)
Challenge'' paper by B. E. Aydemir et al.
(\protect\hyperlink{ref-aydemir05}{2005}).\\
This paper prompted several formalizations of the benchmark typed
\(\lambda\)-calculus, proposed by the authors of the challenge, in
multiple theorem provers, such as Coq, Isabelle, Matita or Twelf.
However, to the best of our knowledge, there has been no published
follow-up work, drawing conclusions about the aptitude of different
mechanizations, which would be useful in deciding the best mechanization
approach to take in formalizing the \(\lamy\) calculus.\\
Whilst this project does not aim to answer the same question as the
original challenge, namely:

\begin{quote}
``How close are we to a world where every paper on programming languages
is accompanied by an electronic appendix with machine- checked proofs?''
(B. E. Aydemir et al. (\protect\hyperlink{ref-aydemir05}{2005}))
\end{quote}

It draws inspiration from the criteria for the ``benchmark
mechanization'', specified by the challenge, to find the best
mechanization approach as well as the right set of tools for our purpose
of effectively mechanizing the theory underpinning HOMC.

Our comparison proceeded in two stages of elimination, where the first
stage was a comparison of the two chosen mechanizations of binders for
the \(\lamy\) calculus (\cref{chap:compIsa}), namely nominal set and
locally nameless representations of binders. The main reason for the
fairly narrow selection of only two binder mechanizations, was the
limited time available for this project. In order to at least partially
achieve the goal of mechanizing the intersection type theory for the
\(\lamy\) calculus, we decided to cut down the number of comparisons to
the two (seemingly) most popular binder mechanizations (chosen by word
of mouth and literature review of the field).\\
After comparing and choosing the optimal mechanization of binders, the
\hyperref[chap:compAgda]{next chapter} then goes on to compare this
mechanization in two different theorem provers, Isabelle and Agda.\\
The ``winning'' theorem prover from this round was then used to
formalize intersection-types and prove subject invariance.

\subsection{Evaluation criteria}\label{evaluation-criteria}

The \(\poplm\) challenge stated three main criteria for evaluating the
submitted mechanizations of the benchmark calculus:

\begin{itemize}
\tightlist
\item
  Mechanization/implementation overheads
\item
  Technology transparency
\item
  Cost of entry
\end{itemize}

To this, we add another criterion:

\begin{itemize}
\tightlist
\item
  Proof automation
\end{itemize}

This project focuses mainly on the three criteria of mechanization
overheads, technology transparency and automation, since the focus of
our comparison is to chose the best mechanization and theorem prover to
use for implementing intersection types for the \(\lamy\) calculus (and
proving subject invariance). These criteria are described in greater
detail below:

\subsubsection{Mechanization/implementation
overheads}\label{mechanizationimplementation-overheads}

When talking about mechanization overheads, we usually mean the
additional theory needed to translate the informal theory we reason
about on paper into the fully formal setting of a theorem prover.
Implementation overheads are usually things deemed too trivial to
consider in a paper proof and a good mechanization will leverage
automation and other language features to hide as much of these
overheads as possible. The best example of a mechanization overhead in
this project is the formalization of binders, discussed in
\cref{binders} of the previous chapter.

\begin{Example}

\label{Example:commNat} Another classic example of mechanization
overheads, imposed by a fully formal setting of a theorem prover is
something as trivial as the proof of commutativity of addition of
natural numbers. In order to prove this property formally in Agda, we
first have to define what is a natural number, using peano numbers where
we have a \texttt{Z} zero constructor and a successor function
\texttt{S}:

\begin{minted}[]{agda}
data ℕ : Set where
  Z : ℕ
  S : ℕ -> ℕ
\end{minted}

Then we can define the addition operation for natural numbers:

\begin{minted}[]{agda}
_+_ : ℕ -> ℕ -> ℕ
Z + x = x
S x + y = S (x + y)
\end{minted}

Finally, the proof of commutativity of addition may end up looking
something like this:

\begin{minted}[]{agda}
S-inj : ∀ {x y} -> x ≡ y -> S x ≡ S y
S-inj refl = refl

+-comm' : ∀ x -> x + Z ≡ x
+-comm' Z = refl
+-comm' (S x) = S-inj (+-comm' x)

+-comm'' : ∀ x y -> x + S y ≡ S (x + y)
+-comm'' Z y = refl
+-comm'' (S x) y = S-inj (+-comm'' x y)

+-comm : ∀ x y -> (x + y) ≡ (y + x)
+-comm Z y rewrite +-comm' y = refl
+-comm (S x) y rewrite +-comm'' y x = S-inj (+-comm x y)
\end{minted}

As we see here, this proof is quite long for something seemingly so
trivial. Proofs of this type are usually unavoidable in a theorem
prover, especially when we want to use such lemmas in a more interesting
result we wan to formalize. Having low implementation overheads thus
usually depends on the automation of the tool, wherein the tool itself
is able to prove these properties automatically. What is more likely,
however, is that a ``good'' tool will include something such a base
theory (of natural numbers) in its library, so that he user does not
have to re-prove these basic properties and instead can focus on the
specific theory she/he wants to prove. This is indeed largely what
happened when we used the nominal library, where the theory was
conveniently hidden away and managed for us by Isabelle's automatic
provers.

\end{Example}

For the scope of this project, binders are discussed and used for
comparison often, since they are the ``weak spot'' where mechanization
overheads are most apparent.\\
In this project, we decided to use nominal sets and locally nameless
representation for binders, due to several reasons. The choice of
nominal sets was tied to the implementation language, namely Isabelle,
which has a well developed
\href{http://www.inf.kcl.ac.uk/staff/urbanc/Nominal/}{nominal sets
library}, maintained by Christian Urban. The appeal of using nominal
sets is of course the touted minimal overheads in comparison to the
informal presentation.\\
The choice of locally nameless encoding, as opposed to using pure de
Bruijn indices, was motivated by the claim that locally nameless
encoding largely mitigates the disadvantages of de Bruijn indices
especially when it comes to technology transparency (i.e.~theorems about
locally nameless presentation are much closer in formulation to the
informal presentation than theorems formulated for de Bruijn indices).\\
Both of these choices were guided in part by the initial choice of
implementation language, Isabelle, which had good support both
mechanizations. Isabelle was also chosen due to previous experience in
mechanizing similar proofs.\\
The comparison between nominal and locally nameless versions of the
\(\lamy\) calculus, presented in \cref{chap:compIsa}, tries to highlight
the differences in the two approaches in contrast to the usual informal
reasoning.

\subsubsection{Technology transparency}\label{technology-transparency}

Technology transparency, as discussed here, is usually concerned with
the presentation of the theory inside a proof assistant, such as
Isabelle or Agda. As the snippets from \cref{Example:commNat}
demonstrate, Agda includes features, such as Unicode support for
mathematical symbols like \(\mathbb{N}\), to make the implementation
look more ``natural''. Other features, like Isabelle's proof
representation language Isar (discussed in \cref{proofAsProg}), are
built into theorem provers to try to make the proofs and definitions
look as close to conventional notation as possible.

\begin{Example}

To demonstrate the Isar proof language and showcase the technology
transparency it affords, we take the proof that a square of an odd
number is itself odd\footnotemark:

\begin{Lemma}[The square of an odd number is also odd]

\begin{proof}

By definition, if \(n\) is an odd integer, it can be expressed as

\begin{center}
$n=2k+1$
\end{center}

for some integer \(k\). Thus

\begin{center}
$\begin{aligned}
n^{2}&=(2k+1)^{2}\\
&=(2k+1)(2k+1)\\
&=4k^{2}+2k+2k+1\\
&=4k^{2}+4k+1\\
&=2(2k^{2}+2k)+1.
\end{aligned}$
\end{center}

Since \(2k^2 + 2k\) is an integer, \(n^2\) is also odd.

\end{proof}

\end{Lemma}

Now, the same (albeit slightly simplified) proof is presented using the
Isar language:

\begin{minted}[]{isabelle}
lemma sq_odd:
  fixes n and odd :: "nat ⇒ bool"
  defines "odd x ≡ ∃k. x = 2 * k + 1"
  assumes "odd n"
  shows "odd (n*n)"
proof -
  from assms obtain k where n_def: "n = 2 * k + 1" 
    unfolding odd_def by auto
  then have "n * n = ( 2 * k + 1) * ( 2 * k + 1)" by simp
  then have "n * n = (4 * k * k) + (4 * k) + 1" by simp
  hence     "n * n = 2 * ((2 * k * k) + (2 * k)) + 1" by simp
  thus "odd (n * n)" unfolding odd_def by blast
qed
\end{minted}

Clearly, this mechanized proof reads much like the rigorous paper proof
that precedes it.

\end{Example}

\footnotetext{The proof was copied from \url{https://en.wikipedia.org/wiki/Direct_proof}}

This criterion of technology transparency is discussed mainly in
\cref{chap:compAgda}, which deals with the comparison of Isabelle and
Agda. The choice of the two theorem provers, but especially of Isabelle,
was largely subjective. Having had previous experience with Isabelle, it
was natural to use it initially, to lower the cost of entry. Initially
only using Isabelle for both formalizations of binders also allowed for
a more uniform comparison of the mechanization overheads.\\
The choice of Agda as the second implementation language was motivated
by Agda having a dependent-type system. As a result, the style of proofs
in Agda seems quite different to Isabelle, since the distinction between
proofs and programs is largely erased. Agda was chosen over Coq, which
is also a dependently-typed language, because it is more ``bare-bones''
and thus seemed more accessible to a novice in dependently-typed
languages. Agda also has a higher ``cool''-factor than Coq, being a
newer language.

\subsubsection{Proof automation}\label{proof-automation}

Proof automation ties into both the mechanization overheads and
transparency aspects of a formalization, since high degree of automation
can often result in a more natural/transparent looking proof where the
``menial'' reasoning steps are taken care of by the theorem prover, and
the user only sees the higher-level reasoning of informal proofs.

\textbf{maybe link to \cref{Lemma:opnSwap}}

Both following chapters discuss the automation features of Isabelle and
Agda and try to draw comparisons by analyzing the same/equivalent lemmas
in different mechanizations and theorem provers, in terms of automation.
Whilst on paper, Isabelle includes a lot more automation, in the form of
several tactics and automated theorem provers, whereas Agda comes with
only very simple proof search tactics, Agda's more sophisticated
type-system takes on and replicates at least some of the automation seen
in Isabelle.

\chapter{Nominal vs.~Locally nameless}\label{comp-isa}

\label{chap:compIsa}

This chapter looks at the two different mechanizations of the \(\lamy\)
calculus, introduced in the previous chapter, namely an implementation
of the calculus using nominal sets and a locally nameless (LN)
mechanization. Having presented the two approaches to formalizing
binders in \cref{binders}, this chapter explores the consequences of
choosing either mechanization, especially in terms of technology
transparency and overheads introduced as a result of the chosen
mechanization.

\section{\texorpdfstring{Capture-avoiding substitution and
\(\beta\)-reduction}{Capture-avoiding substitution and \textbackslash{}beta-reduction}}\label{capture-avoiding-substitution-and-beta-reduction}

We give a brief overview of the basic definitions of well-typed terms
and \(\beta\)-reduction, specific to both mechanizations.
Unsurprisingly, the main differences in these definitions involve
\(\lambda\)-binders.

\subsection{Nominal sets
representation}\label{nominal-sets-representation}

As was shown already, nominal set representation of terms is largely
identical with the informal definitions, which is the main reason why
this representation was chosen. This section will examine the
implementation of \(\lamy\) calculus in Isabelle, using the Nominal
package.\\
We start, by examining the definition of untyped \(\beta\)-reduction,
defined for the \(\lamy\) calculus (referred to as \(\beta Y\)-reduction
due to the addition of the \((Y)\) reduction rule):

\begin{Definition}[$\beta Y$-reduction]

\label{Definition:betaRedNom}

\begin{center}
    \vskip 1.5em
    \AxiomC{$M \red M'$}
    \LeftLabel{$(red_L)$}
    \UnaryInfC{$MN \red M'N$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$N \red N'$}
    \LeftLabel{$(red_R)$}
    \UnaryInfC{$MN \red M'N$}
    \DisplayProof
    \vskip 1.5em
    \AxiomC{$M \red M'$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\lambda x. M \red \lambda x. M'$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{}
    \LeftLabel{$(\beta)$}
    \RightLabel{$(x\ \sharp\ N)$}
    \UnaryInfC{$(\lambda x. M)N \red M[N/x]$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{}
    \LeftLabel{$(Y)$}
    \UnaryInfC{$Y_\sigma M \red M (Y_\sigma M)$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Definition}

This definition, with the exception of the added \((Y)\) rule is the
standard definition of the untyped \(\beta\)-reduction found in
literature (\textbf{link?}). The \(\sharp\) symbol is used to denote the
\emph{freshness} relation in nominal set theory. The side-condition
\(x\ \sharp\ N\) in the \((\beta)\) rule can be read as ``\(x\) is fresh
in \(N\)'', namely, the atom \(x\) does not appear in \(N\). For a
\(\lambda\)-term \(M\), we have \(x\ \sharp\ M\) iff
\(x \not\in \fv(M)\), where we take the usual definition of \fv:

\begin{Definition}

The inductively defined \(\fv\) is the set of \emph{free variables} of a
\(\lambda\)-term \(M\).

\begin{center}
$\begin{aligned}
\fv(x) &= \{ x \}\\
\fv(MN) &= \fv(M) \cup \fv(N)\\
\fv(\lambda x. M) &= \fv(M) \setminus \{ x \}\\
\fv(Y_\sigma) &= \emptyset
\end{aligned}$
\end{center}

\end{Definition}

The definition of substitution, used in the \((\beta)\) rule is also
unchanged with regards to the usual definition (except for the addition
of the \(Y\) case, which is trivial):

\begin{Definition}[Capture-avoiding substitution]

\begin{center}
$\begin{aligned}
x[S/y] &= \begin{cases}
S & \text{if }x \equiv y\\
x & otherwise
\end{cases}\\
(MN)[S/y] &= (M[S/y])(N[S/y])\\
x\ \sharp\ y , S \implies (\lambda x.M)[S/y] &= \lambda x.(M[S/y])\\
(Y_\sigma)[S/y] &= Y_\sigma
\end{aligned}$
\end{center}

\end{Definition}

\subsubsection{Nominal Isabelle
implementation}\label{nominal-isabelle-implementation}

Whilst on paper, all these definitions are unchanged from the usual
presentation, there are a few caveats when it comes to actually
implementing these definitions in Isabelle, using the Nominal package.
The declaration of the terms and types is handled using the reserved
keywords \textbf{\texttt{atom\_decl}} and
\textbf{\texttt{nominal\_datatype}}, which are special versions of the
\textbf{\texttt{typedecl}} and \textbf{\texttt{datatype}} primitives,
used in the usual Isabelle/HOL session:

\begin{minted}[]{isabelle}
atom_decl name

nominal_datatype type = O | Arr type type ("_ → _")

nominal_datatype trm =
  Var name
| App trm trm
| Lam x::name t::trm  binds x in t ("Lam [_]. _" [100, 100] 100)
| Y type
\end{minted}

The special \textbf{\texttt{binds \_ in \_}} syntax in the \texttt{Lam}
constructor declares \texttt{x} to be bound in the body \texttt{t},
telling Nominal Isabelle that \texttt{Lam} terms should be
\textbf{?equated up to \(\alpha\)-equivalence?}, where a term
\(\lambda x. x\) and \(\lambda y. y\) are considered equivalent, because
both \(x\) and \(y\) are bound in the two respective terms, and can both
be \(\alpha\)-converted to the same term, for example \(\lambda z .z\).
In fact, proving such a lemma in Isabelle is trivial:

\begin{minted}[]{isabelle}
lemma "Lam [x]. Var x = Lam [y]. Var y" by simp
\end{minted}

The special \textbf{\texttt{nominal\_datatype}} declaration also
generates definitions of free variables/freshness and other
simplification rules. (Note: These can be inspected in Isabelle, using
the \textbf{\texttt{print\_theorems}} command.)

Next, we define capture avoiding substitution, using a
\textbf{\texttt{nominal\_function}} declaration:

\begin{minted}[]{isabelle}
nominal_function
  subst :: "trm ⇒ name ⇒ trm ⇒ trm"  ("_ [_ ::= _]" [90, 90, 90] 90)
where
  "(Var x)[y ::= s] = (if x = y then s else (Var x))"
| "(App t1 t2)[y ::= s] = App (t1[y ::= s]) (t2[y ::= s])"
| "atom x ♯ (y, s) ⟹ (Lam [x]. t)[y ::= s] = Lam [x].(t[y ::= s])"
| "(Y t)[y ::= s] = Y t"
\end{minted}

Whilst using \textbf{\texttt{nominal\_datatype}} is automatic and
requires no user input, the declaration of a function in Nominal
Isabelle is less straightforward. Unlike using the usual
``\textbf{\texttt{fun}}'' declaration of a recursive function in
Isabelle, where the theorem prover can automatically prove properties
like termination or pattern exhaustiveness, there are several goals (13
in the case of the \texttt{subst} definition) which the user has to
manually prove for any function using nominal data types, such as the
\(\lamy\) terms. This turned out to be a bit problematic, as the goals
involved proving properties like:

\begin{minted}[]{idris}
⋀x t xa ya sa ta.
  eqvt_at subst_sumC (t, ya, sa) ⟹
  eqvt_at subst_sumC (ta, ya, sa) ⟹
  atom x ♯ (ya, sa) ⟹ atom xa ♯ (ya, sa) ⟹ 
  [[atom x]]lst. t = [[atom xa]]lst. ta ⟹ 
  [[atom x]]lst. subst_sumC (t, ya, sa) = 
    [[atom xa]]lst. subst_sumC (ta, ya, sa)
\end{minted}

\textbf{do i need to explain what this property is? or is it ok for
illustrative purposes?}

Whilst most of the goals were trivial, proving cases involving
\(\lambda\)-terms involved a substantial understanding of the internal
workings of Isabelle and the Nominal package early on into the
mechanization and as a novice to using Nominal Isabelle, understanding
and proving these properties proved challenging. The proof script for
the definition of substitution was actually \textbf{lifted/copied?} from
the sample document, found in the Nominal package documentation, which
had a definition of substitution for the untyped \(\lambda\)-calculus
similar enough to be adaptable for the \(\lamy\) calculus.\\
Whilst this formalization required only a handful of other recursive
function definitions, most of which could be copied from the sample
document, in a different theory with significantly more function
definitions, proving such goals from scratch would prove a challenge to
a Nominal Isabelle newcomer as well as a tedious implementation
overhead.

\subsection{Locally nameless
representation}\label{locally-nameless-representation}

As we have seen, on paper at least, the definitions of terms and
capture-avoiding substitution, using nominal sets, are unchanged from
the usual informal definitions. The situation is somewhat different for
the locally nameless mechanization. Since the LN approach combines the
named and de Bruijn representations, there are two different
constructors for free and bound variables:

\subsubsection{Pre-terms}\label{pre-terms}

\begin{Definition}[LN pre-terms]

\label{Definition:pterms}

\begin{center}
$M::= x\ |\ n\ |\ MM\ |\ \lambda M\ |\ Y_\sigma \text{ where }x \in Var \text{ and } n \in \mathbb{N}$
\end{center}

\end{Definition}

Similarly to the de Bruijn presentation of binders, the \(\lambda\)-term
no longer includes a bound variable, so a named representation term
\(\lambda x.x\) becomes \(\lambda 0\) in LN. As was mentioned in
\cref{binders}, the set of pre-terms, defined in
\cref{Definition:pterms}, is a superset of \(\lamy\) terms and includes
terms which are not well formed \(\lamy\) terms.

\begin{Example}

The pre-term \(\lambda 3\) is not a well-formed \(\lamy\) term, since
the bound variable index is out of scope. In other words, there is no
corresponding (named) \(\lamy\) term to \(\lambda 3\).

\end{Example}

Since we don't want to work with terms that do not correspond to
\(\lamy\) terms, we have to introduce the notion of a \emph{well-formed
term}, which restricts the set of pre-terms to only those that
correspond to \(\lamy\) terms (i.e.~this \textbf{?inductive definition?}
ensures that there are no ``out of bounds'' indices in a given
pre-term):

\begin{Definition}[Well-formed terms]

\begin{center}
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (x)$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{}
    \LeftLabel{$(Y)$}
    \UnaryInfC{$\trm (Y_\sigma)$}
    \DisplayProof
    \vskip 1.5em
    \AxiomC{$x \not\in FV(M)$}
    \AxiomC{$\trm (M^x)$}
    \LeftLabel{$(lam)$}
    \BinaryInfC{$\trm (\lambda M)$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (M)$}
    \AxiomC{$\trm (M)$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm (MN)$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Definition}

Already, we see that this formalization introduces some overheads with
respect to the informal/nominal encoding of the \(\lamy\) calculus.\\
The upside of this definition of \(\lamy\) terms becomes apparent when
we start thinking about \(\alpha\)-equivalence and capture-avoiding
substitution. Since the LN terms use de Bruijn levels for bound
variables, there is only one way to write the term \(\lambda x.x\) or
\(\lambda y.y\) as a LN term, namely \(\lambda 0\). As the
\(\alpha\)-equivalence classes of named \(\lamy\) terms collapse into a
singleton \(\alpha\)-equivalence class in a LN representation, the
notion of \(\alpha\)-equivalence becomes trivial.

As a result of using LN representation of binders, the notion of
substitution is split into two distinct operations. One operation is the
substitution of bound variables, called \emph{opening}. The other is
substitution, defined only for free variables.

\begin{Definition}[Opening and substitution]

We will usually assume that \(S\) is a well-formed LN term when proving
properties about substitution and opening. The abbreviation
\(M^N \equiv \{0 \to N\}M\) is used throughout this chapter.

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  Opening:

  \begin{center}
  $\begin{aligned}
  \{k \to S\}x &= x\\
  \{k \to S\}n &= \begin{cases}
  S & \text{if }k \equiv n\\
  n & otherwise
  \end{cases}\\
  \{k \to S\}(MN) &= (\{k \to S\}M)(\{k \to S\}N)\\
  \{k \to S\}(\lambda M) &= \lambda (\{k+1 \to S\}M)\\
  \{k \to S\}Y_\sigma &= Y_\sigma
  \end{aligned}$
  \end{center}
\item
  Substitution:

  \begin{center}
  $\begin{aligned}
  x[S/y] &= \begin{cases}
  S & \text{if }x \equiv y\\
  x & otherwise
  \end{cases}\\
  n[S/y] &= n \\
  (MN)[S/y] &= (M[S/y])(N[S/y])\\
  (\lambda M)[S/y] &= \lambda. (M[S/y])\\
  Y_\sigma[S/y] &= Y_\sigma
  \end{aligned}$
  \end{center}
\end{enumerate}

\end{Definition}

Having defined the \emph{open} operation, we turn back to the definition
of well formed terms, specifically to the \((lam)\) rule, which has the
precondition \(\trm (M^x)\). Intuitively, for the given term
\(\lambda M\), the term \(M^x\) is obtained by replacing all indices
bound to the outermost \(\lambda\) by \(x\). Then, if \(M^x\) is well
formed, so is \(\lambda M\).

\begin{Example}

For example, taking the term \(\lambda\lambda 0(z\ 1)\), we can
construct the following proof-tree, showing that the term is well
formed:

\begin{center}
    \vskip 1.5em
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (y)$}

    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (z)$}
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (x)$}

    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm (z\ x)$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm ((0(z\ x))^y)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm ((\lambda 0(z\ 1))^x)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm (\lambda\lambda 0(z\ 1))$}
    \DisplayProof
    \vskip 1.5em
\end{center}

We assumed that \(x \not\equiv y \not\equiv z\) in the proof tree above
and thus omitted the \(x \not\in \fv \hdots\) branches, as they are not
important for this example.\\
If on the other hand, we try construct a similar tree for a term which
is obviously not well formed, such as \(\lambda \lambda 2(z\ 1)\), we
get a proof tree with a branch which cannot be closed (\(\trm (2)\)):
\newpage
\(\ \)

\begin{center}
    \AxiomC{$\trm (2)$}

    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (z)$}
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (x)$}

    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm (z\ x)$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm ((2(z\ x))^y)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm ((\lambda 2(z\ 1))^x)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm (\lambda\lambda 2(z\ 1))$}
    \DisplayProof
\end{center}

\end{Example}

\subsubsection{\texorpdfstring{\(\beta\)-reduction for LN
terms}{\textbackslash{}beta-reduction for LN terms}}\label{beta-reduction-for-ln-terms}

Finally, we examine the formulation of \(\beta\)-reduction in the LN
presentation of the \(\lamy\) calculus. Since we only want to perform
\(\beta\)-reduction on valid \(\lamy\) terms, the inductive definition
of \(\beta\)-reduction in the LN mechanization now includes the
precondition that the terms appearing in the reduction are well
formed:\(\\\)

\begin{Definition}[$\beta$-reduction (LN)]

\begin{center}
    \AxiomC{$M \red M'$}
    \AxiomC{$\trm (N)$}
    \LeftLabel{$(red_L)$}
    \BinaryInfC{$MN \red M'N$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (M)$}
    \AxiomC{$N \red N'$}
    \LeftLabel{$(red_R)$}
    \BinaryInfC{$MN \red M'N$}
    \DisplayProof
    \vskip 1.5em
    \AxiomC{$ x \not\in \fv(M) \cup \fv(M')$}
    \AxiomC{$M^x \red (M')^x$}
    \LeftLabel{$(abs)$}
    \BinaryInfC{$\lambda M \red \lambda M'$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (\lambda M)$}
    \AxiomC{$\trm (N)$}
    \LeftLabel{$(\beta)$}
    \BinaryInfC{$(\lambda M)N \red M^N$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (M)$}
    \LeftLabel{$(Y)$}
    \UnaryInfC{$Y_\sigma M \red M (Y_\sigma M)$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Definition}

As expected, the \emph{open} operation is now used instead of
substitution in the \((\beta)\) rule.\\
The \((abs)\) rule is also slightly different, also using the
\emph{open} in its precondition. Intuitively, the usual formulation of
the \((abs)\) rule states that in order to prove that \(\lambda x. M\)
reduces to \(\lambda x. M'\), we can simply ``un-bind'' \(x\) in both
\(M\) and \(M'\) and show that \(M\) reduces to \(M'\) (reasoning
bottom-up from the conclusion to the premises). Since in the usual
formulation of the \(\lambda\)-calculus, there is no distinction between
free and bound variables, this change (where \(x\) becomes free) is
implicit. In the LN presentation, however, this operation is made
explicit by opening both \(M\) and \(M'\) with some free variable \(x\)
(not appearing in either \(M\) nor \(M'\)), which replaces the bound
variables/indices (bound to the outermost \(\lambda\)) with \(x\).\\
While this definition is equivalent to \cref{Definition:betaRedNom}, the
induction principle this definition yields may not always be sufficient,
especially in situations where we want to open up a term with a free
variable which is not only fresh in \(M\) and \(M'\), but possibly in a
wider context. We therefore followed the approach of B. Aydemir et al.
(\protect\hyperlink{ref-aydemir08}{2008}) and re-defined the \((abs)\)
rule (and other definitions involving picking fresh/free variables)
using \emph{cofinite quantification}:

\begin{center}
    \vskip 1.5em
    \AxiomC{$\forall x \not\in L.\ M^x \red M'^x$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\lambda M \red \lambda M'$}
    \DisplayProof
    \vskip 1.5em
\end{center}

For an example, where this formulation using \emph{cofinite
quantification} was necessary, see \cref{Lemma:opnClsSubst}).

\subsubsection{Implementation details}\label{implementation-details}

Unlike using the nominal package, the implementation of all the
definitions and functions listed for the LN representation is very
straightforward. To demonstrate this, we present the definition of the
\(\beta\)-reduction in the LN mechanization:

\begin{minted}[]{isabelle}
inductive beta_Y :: "ptrm ⇒ ptrm ⇒ bool" (infix "⇒β" 300)
where
  red_L[intro]: "⟦ trm N ; M ⇒β M' ⟧ ⟹ App M N ⇒β App M' N"
| red_R[intro]: "⟦ trm M ; N ⇒β N' ⟧ ⟹ App M N ⇒β App M N'"
| abs[intro]: "⟦ finite L ; (⋀x. x ∉ L ⟹ M^(FVar x) ⇒β M'^(FVar x)) ⟧ ⟹ 
    Lam M ⇒β Lam M'"
| beta[intro]: "⟦ trm (Lam M) ; trm N ⟧ ⟹ App (Lam M) N ⇒β M^N"
| Y[intro]: "trm M ⟹ App (Y σ) M ⇒β App M (App (Y σ) M)"
\end{minted}

\section{Untyped Church Rosser
Theorem}\label{untyped-church-rosser-theorem}

Having described the implementations of the two binder representations
along with some basic definitions, such as capture-avoiding substitution
or the \emph{open} operation, we come the the main part of the
comparison, namely the proof of the Church Rosser theorem. This section
examines specific instances of some of the major lemmas which are part
of the bigger result. The general outline of the proof has been
described in \cref{cr-def}.

\subsection{Typed vs.~untyped proofs}\label{typ-utyp}

\label{utypReason}

As mentioned previously, when talking about the terms of the \(\lamy\)
calculus, we generally refer to simply typed terms, such as
\(\Gamma \vdash \lambda x. Y_\sigma : \tau \to (\sigma \to \sigma) \to \sigma\).
However, the definitions of reduction seen so far and the consecutive
proofs using these definitions don't use simply typed \(\lamy\) terms,
operating instead on untyped terms. The simplest reason why this is the
case is one of convenience and simplicity.\\
As is the case in most proofs of the Church Rosser Theorem, the result
is usually proved for untyped terms of the \(\lambda\)-calculus and then
extended to simply typed terms by simply restricting the terms we want
to reason about. The CR theorem holds in the restricted setting of
simply typed terms due to subject reduction, which says that if a term
\(M\) can be given a simple type \(\sigma\) and \(\beta\)-reduces to
another term \(M'\), the new term can still be typed with the same type
\(\sigma\). \\
Another reason, besides convention is convenience, specifically
succinctness of code, or the lack thereof, when including simple types
in the definition of \(\beta\)-reduction and all the subsequent lemmas
and theorems. Indeed, the choice of excluding typing information
wherever possible has been an engineering choice to a large degree, as
it is generally not good practice to keep and pass around
variables/objects which are not needed (in classical programming). The
same should also apply to theorem proving, especially since notation can
easily become bloated and difficult to present in a ``natural'' way
(i.e.~using the notation a mathematician would write).\\
Whilst it is true that the implementation of some of the proofs of
Church Rosser might have been shorter, if the typing information was
included directly in the definition of \(\beta\)-reduction, the downside
to this would have been an increased complexity of proofs, resulting in
potentially less understandable and maintainable code. \textbf{This then
also ties into automation? + ex'le??}

\subsection{\texorpdfstring{\cref{Lemma:maxEx}}{}}\label{section}

The first major result in both implementations is \cref{Lemma:maxEx},
which states that for every \(\lamy\) term \(M\), there is a term
\(M'\), s.t. \(M \ggg M'\). This is trivial for \(\gg\), as we can
easily prove the derived rule \((refl^*)\):

\begin{Lemma}[$\gg$ admits $(refl^*)$]

The following rule is admissible in the deduction system \(\gg\):
\label{Lemma:reflM}

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl^*)$}
  \UnaryInfC{$M \gg M$}
  \DisplayProof
 \end{center}

\begin{proof}

By induction on \(M\).

\end{proof}

\end{Lemma}

Since \(\ggg\) restricts the use of the \((app)\) rule to terms which do
not contain a \(\lambda\) or \(Y\) as its left-most sub term,
\cref{Lemma:reflM} does not hold in \(\ggg\) for terms like
\((\lambda x.x)y\), namely, \((\lambda x.x)y \ggg (\lambda x.x)y\) is
not a valid reduction (see \cref{Example:ggVsGgg}). It is, however, not
difficult to see that such terms can simply be \(\beta\)-reduced until
all the redexes have been contracted, so that we have
\((\lambda x.x)y \ggg y\) for the term above.\\
Seen as a weaker version of \cref{Lemma:reflM}, the proof of
\cref{Lemma:maxEx}, at least in theory, should then only differ in the
case of an application, where we have do a case analysis on the left
sub-term of any given \(M\).

This is indeed the case when using the nominal mechanization, where the
proof looks like this:

\begin{minted}[xleftmargin=1em, linenos=true]{isabelle}
lemma pbeta_max_ex:
  fixes M
  shows "∃M'. M >>> M'"
apply (induct M rule:trm.induct)
apply auto
apply (case_tac "not_abst trm1")
apply (case_tac "not_Y trm1")
apply auto[1]
proof goal_cases
case (1 P Q P' Q')
  then obtain σ where 2: "P = Y σ" using not_Y_ex by auto
  have "App (Y σ) Q >>> App Q' (App (Y σ) Q')"
  apply (rule_tac pbeta_max.Y)
  by (rule 1(2))
  thus ?case unfolding 2 by auto
next
case (2 P Q P' Q')
  thus ?case
  apply (nominal_induct P P' avoiding: Q Q' rule:pbeta_max.strong_induct)
  by auto
qed
\end{minted}

After applying induction and calling \texttt{auto}, we can inspect the
remaining goals at line 5, to see that the only goal that remains is the
case of \(M\) being an application:

\begin{minted}[]{idris}
goal (1 subgoal):
 1. ⋀trm1 trm2 M' M'a.
       trm1 >>> M' ⟹ trm2 >>> M'a ⟹ ∃M'. App trm1 trm2 >>> M'
\end{minted}

Lines 6 and 7 correspond to doing a case analysis on \texttt{trm1}
(where \texttt{M = App trm1 trm2}). We end up with 3 goals,
corresponding to \texttt{trm1} either being a \(\lambda\)-term,
\(Y\)-term or neither (shown below in reverse order):

\begin{minted}[]{idris}
 1. ... not_abst trm1 ⟹ not_Y trm1 ⟹ ∃M'. App trm1 trm2 >>> M'
 2. ... not_abst trm1 ⟹ ¬ not_Y trm1 ⟹ ∃M'. App trm1 trm2 >>> M'
 3. ... ¬ not_abst trm1 ⟹ ∃M'. App trm1 trm2 >>> M'
\end{minted}

The first goal is discharged by calling \texttt{auto} again (line 8),
since we can simply apply the \((app)\) rule in this instance. The two
remaining cases are discharged with the additional information that
\texttt{trm1} is either a \(\lambda\)-term or a \(Y\)-term.

So far, we have looked at the version of the proof using nominal
Isabelle and this is especially apparent in line 19, where we use the
stronger \texttt{nominal\_induct} rule, with the extra parameter
\texttt{avoiding: Q Q'}, which ensures that any new bound variables will
be sufficiently fresh with regards to \texttt{Q} and \texttt{Q'}, in
that the fresh variables won't appear in either of the terms.\\
Since bound variables are distinct in the LN representation, the
equivalent proof simply uses the usual induction rule (line 19):

\begin{minted}[xleftmargin=1em, linenos=true]{isabelle}
lemma pbeta_max_ex:
  fixes M assumes "trm M"
  shows "∃M'. M >>> M'"
using assms apply (induct M rule:trm.induct)
apply auto
apply (case_tac "not_abst t1")
apply (case_tac "not_Y t1")
apply auto[1]
proof goal_cases
case (1 P Q P' Q')
  then obtain σ where 2: "P = Y σ" using not_Y_ex by auto
  have "App (Y σ) Q >>> App Q' (App (Y σ) Q')"
  apply (rule_tac pbeta_max.Y)
  by (rule 1(4))
  thus ?case unfolding 2 by auto
next
case (2 P Q P' Q')
  from 2(3,4,5,1,2) show ?case
  apply (induct P P' rule:pbeta_max.induct)
  by auto
next
case (3 L M)
  then obtain x where 4:"x ∉ L ∪ FV M" by (meson FV_finite finite_UnI x_Ex)
  with 3 obtain M' where 5: "M^FVar x >>> M'" by auto

  have 6: "⋀y. y ∉ FV M' ∪ FV M ∪ {x} ⟹ M^FVar y >>> (\\x^M')^FVar y"
  unfolding opn'_def cls'_def 
  apply (subst(3) fv_opn_cls_id2[where x=x])
  using 4 apply simp
  apply (rule_tac pbeta_max_cls)
  using 5 opn'_def by (auto simp add: FV_simp)

  show ?case
  apply rule
  apply (rule_tac L="FV M' ∪ FV M ∪ {x}" in pbeta_max.abs)
  using 6 by (auto simp add: FV_finite)
qed
\end{minted}

As one can immediately see, this proof proceeds exactly in the same
fashion, as the nominal one, up to line 20. However, unlike in the
nominal version of the proof, in the LN proof, the \texttt{auto} call at
line 8 could not automatically prove the case where \(M\) is a
\(\lambda\)-term.\\
This is perhaps not too surprising, since the LN encoding is a lot more
``bare bones'', and thus there is little that would aid Isabelle's
automation. The nominal package, on the other hand, was designed to make
reasoning with binders as painless as possible, which definitely shows
in this example.

When we compare the two goals for the \(\lambda\) case in both versions
of the proof, we clearly see the differences in the treatment of
binders:

\textbf{Nominal:}

\begin{minted}[]{idris}
⋀x M. ∃M'. M >>> M' ⟹ ∃M'. Lam [x]. M >>> M'
\end{minted}

\textbf{Locally nameless:}

\begin{minted}[]{idris}
⋀L M. finite L ⟹
       (⋀x. x ∉ L ⟹ trm M^FVar x) ⟹
       (⋀x. x ∉ L ⟹ ∃M''. M^FVar x >>> M'') ⟹ ∃M'. Lam M >>> M'
\end{minted}

Unlike in the nominal proof, where from \texttt{M >>> M'} we get
\texttt{Lam [x]. M >>> Lam [x]. M'} by \((abs)\) immediately, the proof
of \texttt{∃M'. Lam M >>> M'} in the LN mechanization is not as
trivial.\\
The difficulty arises with the precondition
\(\forall x \not\in L.\ M^x \red M'^x\) in the LN version of the
\((abs)\) rule:

\begin{center}
    \vskip 1.5em
    \AxiomC{$\exists M'.\ \forall x \not\in L.\ M^x \ggg M'^x$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\exists M'.\ \lambda M \ggg \lambda M'\footnotemark$}
    \DisplayProof
    \vskip 1.5em

\footnotetext{ 
While the original goal is \texttt{∃M'. Lam M >>> M'}, since there is only one possible ``shape'' for the right-hands side term, namely \texttt{M'} must be a \(\lambda\)-term, we can easily rewrite this goal as \texttt{∃M'. Lam M >>> Lam M'}.
}
\end{center}

This version of the rule with the existential quantification shows the
subtle difference between the inductive hypothesis
\(\forall x \not\in L.\ \exists M'.\ M^x \ggg M'^x\)\footnote{The
  nominal version was approximately 770 lines of code vs.~1180 for the
  LN version, making it about 50\% longer.} we have and the premise
\(\exists M'.\ \forall x \not\in L.\ M^x \ggg M'^x\) that we want to
show. In order to prove the latter, we assume that there is some \(M'\)
for a specific \(x \not\in L\) s.t. \(M^x \ggg M'^x\).

At this point, we cannot proceed without re-examining the definition of
\emph{opening}, especially in that this operation lacks an inverse.
Whereas in a named representation, where bound variables are bound via
context only, LN terms have specific constructors for free and bound
variables together with an operation for turning bound variables into
free variables, namely the \emph{open} function. In this proof, however,
we need the inverse operation, wherein we turn a free variable into a
bound one. We call this the \emph{close} operation:

\begin{Definition}[Close operation]

This definition was adapted from the B. Aydemir et al.
(\protect\hyperlink{ref-aydemir08}{2008}) paper. We adopt the following
convention, writing \(\cls M \equiv \{0 \leftarrow x\}M\).

\begin{center}
$\begin{aligned}
\{k \leftarrow x\}y &= \begin{cases}
k & \text{if }x \equiv y\\
y & otherwise
\end{cases}\\
\{k \leftarrow S\}n &= n\\
\{k \leftarrow S\}(MN) &= (\{k \leftarrow S\}M)(\{k \leftarrow S\}N)\\
\{k \leftarrow S\}(\lambda M) &= \lambda (\{k+1 \leftarrow S\}M)\\
\{k \leftarrow S\}Y_\sigma &= Y_\sigma
\end{aligned}$
\end{center}

\end{Definition}

\begin{Example}

To demonstrate the close operation, take the term \(\lambda xy\).
Applying the close operation with the free variable \(x\), we get
\(\cls (\lambda xy) = \lambda 1y\). Whilst the original term might have
been well formed, the closed term, as is the case here, may not be.

\end{Example}

Intuitively, it is easy to see that closing a well formed term and then
opening it with the same free variable produces the original term,
namely \((\cls M)^x \equiv M\). This can be made even more general with
the following lemma about the relationship between the open, close and
substitution operations:

\begin{Lemma}

\(\trm(M) \implies \{k \to y\}\{k \leftarrow x\} M = M[y/x]\)
\label{Lemma:opnClsSubst}

\begin{proof}

By induction on the relation \(\trm(M)\). The rough outline of the
\((lam)\) case, which is the only non-trivial case, is shown below:

By \emph{IH}, we have
\(\forall z \not\in L.\ \{k+1 \to y\} \{k+1 \leftarrow x\} M^z = (M^z)[y/x]\).
Then:

\begin{align}
\{k \to y\} \{k \leftarrow x\} (\lambda M) = (\lambda M)[y/x] &\iff\\
\lambda(\{k+1 \to y\} \{k+1 \leftarrow x\} M) = \lambda (M[y/x])&\iff\\
\{k+1 \to y\} \{k+1 \leftarrow x\} M = M[y/x]&\iff\\
\{0 \to z\} \{k+1 \to y\} \{k+1 \leftarrow x\} M = \{0 \to z\} (M[y/x])&\iff\\
\{k+1 \to y\} \{k+1 \leftarrow x\} \{0 \to z\} M = \{0 \to z\} (M[y/x])&\iff\\
\{k+1 \to y\} \{k+1 \leftarrow x\} \{0 \to z\} M = (\{0 \to z\} M)[y/x]&
\end{align}

Starting from the goal (4.1), we expand the definitions of \emph{open},
\emph{close} and substitution for the \(\lambda\) case in (4.2). (4.3)
holds by injectivity of \(\lambda\). THen, by choosing a sufficiently
fresh \(z\) that does not appear in the given context \(L\) as well as
in neither \(\fv(M)\) nor \(\{x, y\}\), we have (4.4). We can reorder
the open and close operations in (4.5) because it can never be the case
that \(k+1 = 0\) and \(z\) is different from both \(x\) and \(y\).
Finally, (4.6) follows from the fact that we have chosen a \(z\) that
does not appear in \(M\) and is different from \(y\).\\
We can now see that
\(\{k+1 \to y\} \{k+1 \leftarrow x\} \{0 \to z\} M = (\{0 \to z\} M)[y/x]\)
is in fact the \emph{IH}
\(\{k+1 \to y\} \{k+1 \leftarrow x\} M^z = (M^z)[y/x]\).

\end{proof}

\end{Lemma}

Having defined the \emph{close} operation and shown that it satisfies
certain properties with respect to the \emph{open} operation and
substitution, we can now ``close'' the term \(M'\), with respect to the
\(x\) we fixed earlier and thus show that
\(\forall y \not\in L.\ M^y \ggg (\cls M')^y\).

\textbf{Should I go into more detail here or just wrap it up by saying
how much more code was necessary over the nominal version??}

\subsection{\texorpdfstring{\cref{Lemma:maxClose}}{}}\label{section-1}

While it may seem that the nominal mechanization was universally more
concise and easier to work in than the locally nameless implementation,
there were a few instances where using the nominal library turned out to
be more difficult to understand and use. One such instance, namely
defining a \textbf{\texttt{nominal\_function}}, was already discussed.
Another example can be found in the implmentation of
\cref{Lemma:maxClose}, which is stated as:

\begin{center}
$\forall M, M', M_{max}.\ M \ggg M_{max} \land M \gg M' \implies M' \gg M_{max}$
\end{center}

The proof of this lemma proceeds by induction on the relation \(\ggg\).
Here we will focus on the \((\beta)\) case, i.e.~when we have
\(M \ggg M_{max}\) by the application of \((\beta)\), first giving an
informal proof and then focusing on the implementation specifics in both
mechanizations:

\subsubsection{\texorpdfstring{\((\beta)\)
case}{(\textbackslash{}beta) case}}\label{beta-case}

We have \(M \equiv (\lambda x. P) Q\) and
\(M_{max} \equiv P_{max}[Q_{max}/x]\), and therefore
\((\lambda x. P) Q \ggg P_{max}[Q_{max}/x]\) and
\((\lambda x. P) Q \gg M'\).\\
By performing case analysis on the reduction
\((\lambda x. P) Q \gg M'\), we know that
\(M' \equiv (\lambda x. P') Q'\) or \(M' \equiv P'[Q'/x]\) for some
\(P', Q'\), since only these two \textbf{??reduction trees??} are valid:

\begin{center}
    \AxiomC{$\vdots$}
    \UnaryInfC{$P \gg P'$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\lambda x. P \gg \lambda x. P'$}
    \AxiomC{$\vdots$}
    \UnaryInfC{$Q \gg Q'$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$(\lambda x. P) Q \gg (\lambda x. P') Q'$}
    \DisplayProof
    \ \ \ \ \ \ or\ \ \ \ \ \ 
    \AxiomC{$\vdots$}
    \UnaryInfC{$P \gg P'$}
    \AxiomC{$\vdots$}
    \UnaryInfC{$Q \gg Q'$}
    \LeftLabel{$(\beta)$}
    \BinaryInfC{$(\lambda x. P) Q \gg P'[Q'/x]$}
    \DisplayProof
    \vskip 1.5em
\end{center}

For the first case, where \(M' \equiv (\lambda x. P') Q'\), by
\emph{IH}, we have \(P' \gg P_{max}\) and \(Q' \gg Q_{max}\). Thus, we
can prove that \(M' \gg P_{max}[Q_{max}/x]\):

\begin{center}
    \vskip 1.5em
    \AxiomC{}
    \LeftLabel{$(IH)$}
    \UnaryInfC{$P' \gg P_{max}$}
    \AxiomC{}
    \LeftLabel{$(IH)$}
    \UnaryInfC{$Q' \gg Q_{max}$}
    \LeftLabel{$(\beta)$}
    \BinaryInfC{$(\lambda x. P') Q' \gg P_{max}[Q_{max}/x]$}
    \DisplayProof
    \vskip 1.5em
\end{center}

In the case where \(M' \equiv P'[Q'/x]\), we also have
\(P' \gg P_{max}\) and \(Q' \gg Q_{max}\) by \emph{IH}. The result
\(M' \gg P_{max}[Q_{max}/x]\) follows from the following auxiliary
lemma:

\begin{Lemma}[Parallel substitution]

\label{Lemma:parRed}The following rule is admissible in \(\gg\):

\begin{center}
    \vskip 1.5em
    \AxiomC{$M \gg M'$}
    \AxiomC{$N \gg N'$}
    \LeftLabel{$(||_{subst})$}
    \BinaryInfC{$M[N/x] \gg M'[N'/x]$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Lemma}

\subsubsection{Nominal implementation}\label{nominal-implementation}

The code below shows the proof of the \((\beta)\) case, described above:

\begin{minted}[xleftmargin=1em, linenos=true]{isabelle}
case (beta x Q Qmax P Pmax)
  from beta(1,7) show ?case
  apply (rule_tac pbeta_cases_2)
  apply (simp, simp)
  proof -
  case (goal2 Q' P')
    with beta have "P' ⇒∥ Pmax" "Q' ⇒∥ Qmax" by simp+
    thus ?case unfolding goal2 apply (rule_tac Lem2_5_1) by simp+
  next
  case (goal1 P' Q')
    with beta have ih: "P' ⇒∥ Pmax" "Q' ⇒∥ Qmax" by simp+
    show ?case unfolding goal1 
    apply (rule_tac pbeta.beta) using goal1 beta ih
    by simp_all
  qed
\end{minted}

There were a few quirks when implementing this proof in the nominal
mechanization, specifically in line 3, where the case analysis on the
shape of \(M'\) needed to be performed. Applying the automatically
generated \texttt{pbeta.cases} rule yielded the following goal for the
case where \(M' \equiv P'[Q'/x]\):

\begin{minted}[]{idris}
 2. ⋀xa Q' R P'.
       [[atom x]]lst. P = [[atom xa]]lst. R ⟹
       M' = P' [xa ::= Q'] ⟹
       atom xa ♯ Q ⟹ atom xa ♯ Q' ⟹ R ⇒∥ P' ⟹ Q ⇒∥ Q' ⟹ 
       M' ⇒∥ Pmax [x ::= Qmax]
\end{minted}

Obviously, this is not the desired shape of the goal, because we
obtained a weaker premise, where we have some \(R\), such that
\(\lambda x. P \equiv_\alpha \lambda xa. R\) (this is essentially what
\texttt{[[atom x]]lst. P = [[atom xa]]lst. R} states) and therefore we
get a \(P'\) where \(M' \equiv P'[Q'/xa]\). What we actually want is a
term \(P'\) s.t. \(M' \equiv P'[Q'/x]\), i.e. \(x = xa\). In order to
``force'' \(x\) and \(xa\) to actually be the same atom, we had to prove
the following ``cases'' lemma:

\begin{minted}[]{isabelle}
lemma pbeta_cases_2:
  shows "atom x ♯ t ⟹ App (Lam [x]. s) t ⇒∥ a2 ⟹ 
    (⋀s' t'. a2 = App (Lam [x]. s') t' ⟹ atom x ♯ t' ⟹ 
        s ⇒∥ s' ⟹ t ⇒∥ t' ⟹ P) ⟹
    (⋀t' s'. a2 = s' [x ::= t'] ⟹ atom x ♯ t ⟹ atom x ♯ t' ⟹ 
        s ⇒∥ s' ⟹ t ⇒∥ t' ⟹ P) ⟹ P"
...
\end{minted}

In the lemma above,
\texttt{(⋀t' s'. a2 = s' [x ::= t'] ⟹ atom x ♯ t ⟹ atom x ♯ t' ⟹ s ⇒∥ s' ⟹ t ⇒∥ t' ⟹ P) ⟹ P}
corresponds to the case with the premises we want to have, instead of
the ones we get from the ``cases'' lemma generated as part of the
definition of \(\gg\).

The proof of this lemma required proving another lemma shown below,
which required descending into nominal set theory that was previously
mostly hidden away from the mechanization (the proofs of the
\texttt{have} lemmas were omitted for brevity):

\begin{minted}[]{isabelle}
lemma "(Lam [x]. s) ⇒∥ s' ⟹ ∃t. s' = Lam [x]. t ∧ s ⇒∥ t"
proof (cases "(Lam [x]. s)" s' rule:pbeta.cases, simp)
  case (goal1 _ _ x')
    then have 1: "s ⇒∥ ((x' ↔ x) ∙ M')" ...
    from goal1 have 2: "(x' ↔ x) ∙ s' = Lam [x]. ((x' ↔ x) ∙ M')" ...
    from goal1 have "atom x ♯ (Lam [x']. M')"  using fresh_in_pbeta ...
    with 2 have "s' = Lam [x]. ((x' ↔ x) ∙ M')" ...
    with 1 show ?case by auto
qed
\end{minted}

Clearly, the custom ``cases'' lemma was necessary from a purely
technical view, as it would be deemed too trivial to bother proving in
an informal setting. The need for such a lemma also demonstrates that
whilst the nominal package package tries to hide away the details of the
theory, every once in a while, the user has to descent into nominal set
theory, to prove certain properties about binders, not handled by the
automation.\\
For us, the nominal package thus proved to be a double edged sword, as
it initially provided a fairly low cost of entry (there was practically
no need to understand any nominal set theory to get started), but proved
to be much more challenging to understand in certain places, such as
when proving \texttt{pbeta\_cases\_2}.\\
Whilst the finial \texttt{pbeta\_cases\_2} proof turned out to be fairly
short thanks to automation of the nominal set theory, it took some time
to work out the proof outline in such a ways as to leverage Isabelle's
automation to a high degree.\\
The LN mechanization, whilst having bigger overheads in terms of extra
definitions and lemmas that had to be proven ``by hand'', was in fact a
lot more transparent as a result, as the degree of difficulty after the
initial cost of entry did not rise significantly with more complicated
lemmas.

\subsubsection{LN implementation}\label{ln-implementation}

The troublesome case analysis in the Nominal version of the proof was
much more straight forward in the LN proof. In fact, there was no need
to prove a separate lemma similar to \texttt{pbeta\_cases\_2}, since the
auto-generated \texttt{pbeta.cases} was sufficient. The only overhead in
this version of the lemma came from the use of \cref{Lemma:parRed}, in
that the lemma was first proved in it's classical formulation using
substitution, but due to the way substitution of bound terms is handled
in the LN mechanization (using the \emph{open function}), a ``helper''
lemma was proved to convert this result to one using \emph{open}:

\begin{Lemma}[Parallel open]

\label{Lemma:parOpn}The following rule is admissible in the LN version
of \(\gg\):

\begin{center}
    \vskip 1.5em
    \AxiomC{$\forall x \not\in L.\ M^x \gg M'^x$}
    \AxiomC{$N \gg N'$}
    \LeftLabel{$(||_{open})$}
    \BinaryInfC{$M^N \gg M'^{N'}$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Lemma}

The reason why \cref{Lemma:parOpn} wasn't proved directly is partially
due to the order of implementation of the two mechanizations of the
\(\lamy\) calculus. Since the nominal version, along with all the proofs
was carried out first, the LN version of the calculus ended up being
more of a port of the nominal theory into a locally nameless setting.\\
The LN mechanization, being a port of the nominal theory, has both
advantages and disadvantages. On the one hand, it ensures a greater
consistency between the two theories and easier direct comparison of
lemmas, but on the other hand, it meant that certain lemmas could have
been made shorter and more ``tailored'' to the LN mechanization.

\section{Subject reduction}\label{subject-reduction}

\textbf{This chapter is already quite long, so this section might end up
being quite brief, as the main differences between the mechanizations
have already been illustrated\ldots{}I think\ldots{}Or?}

\section{Verdict}\label{verdict}

Having given an overview of the main technical points of the \(\lamy\)
calculus mechanization, we concluded that on the whole, neither
mechanization proved to be significantly better than the other.\\
Whilst the LN mechanization proved to have significantly higher
``obvious'' mechanization overheads\footnote{The nominal version was
  approximately 770 lines of code vs.~1180 for the LN version, making it
  about 50\% longer.} in therms of code length, the implementation using
the nominal library proved to be more difficult to use at certain
points, due to the more complex nominal sets theory that implicitly
underpinned the mechanization. The LN mechanization proved to be much
more simple in practice, even without any library support and the
automation which comes with using Nominal Isabelle.\\
Continuing with the next round of comparison between the two theorem
provers, Isabelle and Agda, this point was the main reason to chose LN
over nominal sets, as implementing the LN version of the calculus
requires a lot less ``background'' theory, which was especially
important in Agda, where nominal set support is a lot less mature than
in Isabelle.

\chapter{Isabelle vs.~Agda}\label{comp-agda}

\label{chap:compAgda}

The formalization of the terms and reduction rules of the \(\lambda\)-Y
calculus presented here is a locally nameless presentation due to B.
Aydemir et al. (\protect\hyperlink{ref-aydemir08}{2008}). The basic
definitions of \(\lambda\)-terms and \(\beta\)-reduction were borrowed
from an implementation of the \(\lambda\)-calculus with the associated
Church Rosser proof in Agda, by Mu
(\protect\hyperlink{ref-shing-cheng}{2011}).

One of the most obvious differences between Agda and Isabelle is the
treatment of functions and proofs in both languages. Whilst in Isabelle,
there is always a clear syntactic distinction between programs and
proofs, Agda's richer dependent-type system allows constructing proofs
as programs. This distinction is especially apparent in inductive
proofs, which have a completely distinct syntax in Isabelle. As proofs
are not objects which can be directly manipulated in Isabelle, to modify
the proof goal, user commands such as \texttt{apply rule} or
\texttt{by auto} are used:

\begin{minted}[]{isabelle}
lemma subst_fresh: "x ∉ FV t ⟹ t[x ::= u] = t"
apply (induct t)
by auto
\end{minted}

In the proof above, the command \texttt{apply (induct t)} takes a proof
object with the goal \texttt{x ∉ FV t ⟹ t[x ::= u] = t}, and applies the
induction principle for \texttt{t}, generating 5 new proof obligations:

\begin{minted}[]{idris}
proof (prove)
goal (5 subgoals):
1. ⋀xa. x ∉ FV (FVar xa) ⟹ FVar xa [x ::= u] = FVar xa
2. ⋀xa. x ∉ FV (BVar xa) ⟹ BVar xa [x ::= u] = BVar xa
3. ⋀t1 t2.
    (x ∉ FV t1 ⟹ t1 [x ::= u] = t1) ⟹
    (x ∉ FV t2 ⟹ t2 [x ::= u] = t2) ⟹
    x ∉ FV (App t1 t2) ⟹ App t1 t2 [x ::= u] = App t1 t2
4. ⋀t. (x ∉ FV t ⟹ t [x ::= u] = t) ⟹ x ∉ FV (Lam t) ⟹ 
    Lam t [x ::= u] = Lam t
5. ⋀xa. x ∉ FV (Y xa) ⟹ Y xa [x ::= u] = Y xa
\end{minted}

These can then discharged by the call to \texttt{auto}, which is another
command that invokes the automatic solver, which tries to prove all the
goals in the given context.

In comparison, in an Agda proof the proof objects are available to the
user directly. Instead of using commands modifying the proof state, one
begins with a definition of the lemma:

\begin{minted}[]{agda}
subst-fresh : ∀ x t u -> (x∉FVt : x ∉ (FV t)) -> (t [ x ::= u ]) ≡ t
subst-fresh x t u x∉FVt = ?
\end{minted}

The \texttt{?} acts as a `hole' which the user needs to fill in, to
construct the proof. Using the emacs/atom agda-mode, once can apply a
case split to \texttt{t}, corresponding to the \texttt{apply (induct t)}
call in Isabelle, generating the following definition:

\begin{minted}[]{agda}
subst-fresh : ∀ x t u -> (x∉FVt : x ∉ (FV t)) -> (t [ x ::= u ]) ≡ t
subst-fresh x (bv i) u x∉FVt = {!   0!}
subst-fresh x (fv x₁) u x∉FVt = {!   1!}
subst-fresh x (lam t) u x∉FVt = {!   2!}
subst-fresh x (app t t₁) u x∉FVt = {!   3!}
subst-fresh x (Y t₁) u x∉FVt = {!   4!}
\end{minted}

When the above definition is compiled, Agda generates 5 goals needed to
`fill' each hole:

\begin{minted}[]{agda}
?0  :  (bv i [ x ::= u ]) ≡ bv i
?1  :  (fv x₁ [ x ::= u ]) ≡ fv x₁
?2  :  (lam t [ x ::= u ]) ≡ lam t
?3  :  (app t t₁ [ x ::= u ]) ≡ app t t₁
?4  :  (Y t₁ [ x ::= u ]) ≡ Y t₁
\end{minted}

As one can see, there is a clear correspondence between the 5 generated
goals in Isabelle and the cases of the Agda proof above.

Due to this correspondence, reasoning in both systems is often largely
similar. Whereas in Isabelle, one modifies the proof indirectly by
issuing commands to modify proof goals, in Agda, one generates proofs
directly by writing a program-as-proof, which satisfies the type
constraints given in the definition.

\section{Automation}\label{automation}

As seen in the first example, Isabelle includes several automatic
provers of varying complexity, including \texttt{simp}, \texttt{auto},
\texttt{blast}, \texttt{metis} and others. These are tactics/programs
which automatically apply rewrite-rules until the goal is discharged. If
the tactic fails to discharge a goal within a set number of steps, it
stops and lets the user direct the proof. The use of tactics in Isabelle
is common to prove trivial goals, which usually follow from simple
rewriting of definitions or case analysis of certain variables.

\begin{Example}

For example, the proof goal

\begin{minted}[]{idris}
⋀xa. x ∉ FV (FVar xa) ⟹ FVar xa [x ::= u] = FVar xa
\end{minted}

will be proved by first unfolding the definition of substitution for
\texttt{FVar}

\begin{minted}[]{idris}
(FVar xa)[x ::= u] = (if xa = x then u else FVar xa)
\end{minted}

and then deriving \texttt{x ≠ xa} from the assumption
\texttt{x ∉ FV (FVar xa)}. Applying these steps explicitly, we get:

\begin{minted}[]{isabelle}
lemma subst_fresh: "x ∉ FV t ⟹ t[x ::= u] = t"
apply (induct t)
apply (subst subst.simps(1))
apply (drule subst[OF FV.simps(1)])
apply (drule subst[OF Set.insert_iff])
apply (drule subst[OF Set.empty_iff])
apply (drule subst[OF HOL.simp_thms(31)])
...
\end{minted}

where the goal now has the following shape:

\begin{minted}[]{idris}
1. ⋀xa. x ≠ xa ⟹ (if xa = x then u else FVar xa) = FVar xa
\end{minted}

From this point, the simplifier rewrites \texttt{xa = x} to
\texttt{False} and \texttt{(if False then u else FVar xa)} to
\texttt{FVar xa} in the goal. The use of tactics and automated tools is
heavily ingrained in Isabelle and it is actually impossible
(i.e.~impossible for me) to not use \texttt{simp} at this point in the
proof, partly because one gets so used to discharging such trivial goals
automatically and partly because it becomes nearly impossible to do the
last two steps explicitly without having a detailed knowledge of the
available commands and tactics in Isabelle (i.e.~I don't).\\
Doing these steps explicitly, quickly becomes cumbersome, as one needs
to constantly look up the names of basic lemmas, such as
\texttt{Set.empty\_iff}, which is a simple rewrite rule
\texttt{(?c ∈ \{\}) = False}.

\end{Example}

Unlike Isabelle, Agda does not include nearly as much automation. The
only proof search tool included with Agda is Agsy, which is similar,
albeit often weaker than the \texttt{simp} tactic. It may therefore seem
that Agda will be much more cumbersome to reason in than Isabelle. This,
however, turns out not to be the case in this formalization, in part due
to Agda's type system and the powerful pattern matching as well as
direct access to the proof goals.

However, automation did not play as major a part in this project as it
might have, especially in this round of the comparison, since the LN
mechanization had to be implemented from scratch and thus, the proofs
written in Isabelle were only later modified to leverage some
automation. However, since most proofs required induction, which theorem
provers are generally not very good at performing wihtout user guidance,
the only place where automation was really apparent was in the case of a
few lemmas involving equational reasoning, like the ``open-swap'' lemma:

\begin{Lemma}

\label{Lemma:opnSwap}
\(k \neq n \implies x \neq y \implies \{k \to x\}\{n \to y\}M = \{n \to y\}\{k \to x\}M\)

\end{Lemma}

Whilst in Isabelle, this was a trivial case of applying induction on the
term \(M\) and letting \texttt{auto} prove all the remaining cases. In
Agda, this was a lot more painful, as the cases had to be constructed
and proved more or less manually, yielding this rather longer proof:

\begin{minted}[]{agda}
^-^-swap : ∀ k n x y m -> ¬(k ≡ n) -> ¬(x ≡ y) -> 
  [ k >> fv x ] ([ n >> fv y ] m) ≡ [ n >> fv y ] ([ k >> fv x ] m)
^-^-swap k n x y (bv i) k≠n x≠y with n ≟ i
^-^-swap k n x y (bv .n) k≠n x≠y | yes refl with k ≟ n
^-^-swap n .n x y (bv .n) k≠n x≠y | yes refl | yes refl = ⊥-elim (k≠n refl)
^-^-swap k n x y (bv .n) k≠n x≠y | yes refl | no _ with n ≟ n
^-^-swap k n x y (bv .n) k≠n x≠y | yes refl | no _ | yes refl = refl
^-^-swap k n x y (bv .n) k≠n x≠y | yes refl | no _ | no n≠n = 
  ⊥-elim (n≠n refl)
^-^-swap k n x y (bv i) k≠n x≠y | no n≠i with k ≟ n
^-^-swap n .n x y (bv i) k≠n x≠y | no n≠i | yes refl = ⊥-elim (k≠n refl)
^-^-swap k n x y (bv i) k≠n x≠y | no n≠i | no _ with k ≟ i
^-^-swap k n x y (bv .k) k≠n x≠y | no n≠i | no _ | yes refl = refl
^-^-swap k n x y (bv i) k≠n x≠y | no n≠i | no _ | no k≠i with n ≟ i
^-^-swap k i x y (bv .i) k≠n x≠y | no n≠i | no _ | no k≠i | yes refl = 
  ⊥-elim (n≠i refl)
^-^-swap k n x y (bv i) k≠n x≠y | no n≠i | no _ | no k≠i | no _ = refl
^-^-swap k n x y (fv z) k≠n x≠y = refl
^-^-swap k n x y (lam m) k≠n x≠y = 
  cong lam (^-^-swap (suc k) (suc n) x y m (λ sk≡sn → k≠n (≡-suc sk≡sn)) x≠y)
^-^-swap k n x y (app t1 t2) k≠n x≠y rewrite
  ^-^-swap k n x y t1 k≠n x≠y | ^-^-swap k n x y t2 k≠n x≠y = refl
^-^-swap k n x y (Y _) k≠n x≠y = refl
\end{minted}

\section{Proofs-as-programs}\label{proofs-as-programs}

\label{proofAsProg}

As was already mentioned, Agda treats proofs as programs, and therefore
provides direct access to proof objects. In Isabelle, the proof goal is
of the form:

\begin{minted}[]{idris}
lemma x: "assm-1 ⟹ ... ⟹ assm-n ⟹ concl"
\end{minted}

using the `apply-style' reasoning in Isabelle can become burdensome, if
one needs to modify or reason with the assumptions, as was seen in the
example above. In the example, the \texttt{drule} tactic, which is used
to apply rules to the premises rather than the conclusion, was applied
repeatedly. Other times, we might have to use structural rules for
exchange or weakening, which are necessary purely for
\texttt{organizational} purposes of the proof.\\
In Agda, such rules are not necessary, since the example above looks
like a functional definition:

\begin{minted}[]{idris}
x assm-1 ... assm-n = ?
\end{minted}

Here, \texttt{assm-1} to \texttt{assm-n} are simply arguments to the
function x, which expects something of type \texttt{concl} in the place
of \texttt{?}. This presentation allows one to use the given assumptions
arbitrarily, perhaps passing them to another function/proof or
discarding them if not needed.\\
This way of reasoning is also supported in Isabelle to some extent via
the use of the Isar proof language, where (the previous snippet of) the
proof of \texttt{subst\_fresh} can be expressed in the following way:

\begin{minted}[]{isabelle}
lemma subst_fresh': 
  assumes "x ∉ FV t"
  shows "t[x ::= u] = t"
using assms proof (induct t)
case (FVar y)
  from FVar.prems have "x ∉ {y}" unfolding FV.simps(1) .
  then have "x ≠ y" unfolding Set.insert_iff Set.empty_iff HOL.simp_thms(31) .
  then show ?case unfolding subst.simps(1) by simp
next
...
qed
\end{minted}

This representation is more natural (and readable) to humans, as the
assumptions have been separated and can be referenced and used in a
clearer manner. For example, in the line

\begin{minted}[]{isabelle}
from FVar.prems have "x ∉ {y}"
\end{minted}

the premise \texttt{FVar.prems} is added to the context of the goal
\texttt{x ∉ \{y\}}:

\begin{minted}[]{idris}
proof (prove)
using this:
  x ∉ FV (FVar y)

goal (1 subgoal):
 1. x ∉ {y}
\end{minted}

The individual reasoning steps described in the previous section have
also been separated out into `mini-lemmas' (the command \texttt{have}
creates an new proof goal which has to be proved and then becomes
available as an assumption in the current context) along the lines of
the intuitive reasoning discussed initially. While this proof is more
human readable, it is also more verbose and potentially harder to
automate, as generating valid Isar style proofs is more difficult, due
to `Isar-style' proofs being obviously more complex than `apply-style'
proofs.

Whilst using the Isar proof language gives us a finer control and better
structuring of proofs, one still references proofs only indirectly.
Looking at the same proof in Agda, we have the following definition for
the case of free variables:

\begin{minted}[]{agda}
subst-fresh' x (fv y) u x∉FVt = {!   0!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  fv y [ x ::= u ] ≡ fv y
\end{minted}

The proof of this case is slightly different from the Isabelle proof. In
order to understand why, we need to look at the definition of
substitution for free variables in Agda:

\begin{minted}[]{agda}
fv y [ x ::= u ] with x ≟ y
... | yes _ = u
... | no _ = fv y
\end{minted}

This definition corresponds to the Isabelle definition, however, instead
of using an if-then-else conditional, the Agda definition uses the
\texttt{with} abstraction to pattern match on \texttt{x ≟ y}. The
\texttt{\_≟\_} function takes the arguments \texttt{x} and \texttt{y},
which are natural numbers, and decides syntactic equality, returning a
\texttt{yes p} or \texttt{no p}, where \texttt{p} is the proof object
showing their in/equality.\\
Since the definition of substitution does not require the proof object
of the equality of \texttt{x} and \texttt{y}, it is discarded in both
cases. If \texttt{x} and \texttt{y} are equal, \texttt{u} is returned
(case \texttt{... | yes \_ = u}), otherwise \texttt{fv y} is returned.

In order for Agda to be able to unfold the definition of
\texttt{fv y [ x ::= u ]}, it needs the case analysis on \texttt{x ≟ y}:

\begin{minted}[]{agda}
subst-fresh' x (fv y) u x∉FVt with x ≟ y
... | yes p = {!   0!}
... | no ¬p = {!   1!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  (fv y [ x ::= u ] | yes p) ≡ fv y
?1  :  (fv y [ x ::= u ] | no ¬p) ≡ fv y
\end{minted}

In the second case, when \texttt{x} and \texttt{y} are different, Agda
can automatically fill in the hole with \texttt{refl}. Notice that
unlike in Isabelle, where the definition of substitution had to be
manually unfolded (the command \texttt{unfolding subst.simps(1)}), Agda
performs type reduction automatically and can rewrite the term
\texttt{(fv y [ x ::= u ] | no .¬p)} to \texttt{fv y} when type-checking
the expression. Since all functions in Agda terminate, this operation on
types is safe \textbf{(not sure this is clear enough\ldots{} im not
entirely sure why\ldots{} found here:
http://people.inf.elte.hu/divip/AgdaTutorial/Functions.Equality\_Proofs.html\#automatic-reduction-of-types)}.

For the case where \texttt{x} and \texttt{y} are equal, one can
immediately derive a contradiction from the fact that \texttt{x} cannot
be equal to \texttt{y}, since \texttt{x} is not a free variable in
\texttt{fv y}. The type of false propositions is \texttt{⊥} in Agda.
Given \texttt{⊥}, one can derive any proposition. To derive \texttt{⊥},
we first inspect the type of \texttt{x∉FVt}, which is
\texttt{x ∉ y ∷ []}. Further examining the definition of \texttt{∉}, we
find that \texttt{x ∉ xs = ¬ x ∈ xs}, which further unfolds to
\texttt{x ∉ xs = x ∈ xs → ⊥}. Thus to obtain \texttt{⊥}, we simply have
to show that \texttt{x ∈ xs}, or in this specific instance
\texttt{x ∈ y ∷ []}. The definition of \texttt{∈} is itself just sugar
for \texttt{x ∈ xs = Any (\_≈\_ x) xs}, where \texttt{Any P xs} means
that there is an element of the list \texttt{xs} which satisfies
\texttt{P}. In this instance, \texttt{P = (\_≈\_ x)}, thus an inhabitant
of the type \texttt{Any (\_≈\_ x) (y ∷ [])} can be constructed if one
has a proof that at least one element in \texttt{y ∷ []} is equivalent
to \texttt{x}. As it happens, such a proof was given as an argument in
\texttt{yes p}:

\begin{minted}[]{agda}
False : ⊥
False = x∉FVt (here p)
\end{minted}

The finished case looks like this (note that \texttt{⊥-elim} takes
\texttt{⊥} and produces something of arbitrary type):

\begin{minted}[]{agda}
subst-fresh' x (fv y) u x∉FVt with x ≟ y
... | yes p = ⊥-elim False
  where
  False : ⊥
  False = x∉FVt (here p)
... | no ¬p = refl
\end{minted}

We can even tranform the Isabelle proof to closer match the Agda proof:

\begin{minted}[]{isabelle}
case (FVar y)
  show ?case
  proof (cases "x = y")
  case True
    with FVar have False by simp
    thus ?thesis ..
  next
  case False then show ?thesis unfolding subst.simps(1) by simp
  qed
\end{minted}

We can thus see that using Isar style proofs and Agda reasoning ends up
being rather similar in practice.

\section{Pattern matching}\label{pattern-matching}

Another reason why automation in the form of explicit proof search
tactics needn't play such a significant role in Agda, is the more
sophisticated type system of Agda (compared to Isabelle). Since Agda
uses a dependent type system, there are often instances where the type
system imposes certain constraints on the arguments/assumptions in a
definition/proof and partially acts as a proof search tactic, by guiding
the user through simple reasoning steps. Since Agda proofs are programs,
unlike Isabelle `apply-style' proofs, which are really proof scripts,
one cannot intuitively view and step through the intermediate reasoning
steps done by the user to prove a lemma. The way one proves a lemma in
Agda is to start with a lemma with a `hole', which is the proof goal,
and iteratively refine the goal until this proof object is constructed.
The way Agda's pattern matching makes constructing proofs easier can be
demonstrated with the following example.

\begin{Example}

The following lemma states that the parallel-\(\beta\) maximal reduction
preserves local closure:

\begin{center}
$t \ggg t' \implies \trm(t) \land \trm(t')$
\end{center}

For simplicity, we will prove a slightly simpler version, namely:
\(t \ggg t' \implies \trm(t)\). For comparison, this is a short, highly
automated proof in Isabelle:

\begin{minted}[]{isabelle}
lemma pbeta_max_trm_r : "t >>> t' ⟹ trm t"
apply (induct t t' rule:pbeta_max.induct)
apply (subst trm.simps, simp)+
by (auto simp add: lam trm.Y trm.app)
\end{minted}

In Agda, we start with the following definition:

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l t>>>t' = {!   0!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term .t
\end{minted}

Construction of this proof follows the Isabelle script, in that the
proof proceeds by induction on \(t \ggg t'\), which corresponds to the
command \texttt{apply (induct t t' rule:pbeta\_max.induct)}. As seen
earlier, induction in Agda simply corresponds to a case split. The
agda-mode in Emacs/Atom can perform a case split automatically, if
supplied with the variable which should be used for the case analysis,
in this case \texttt{t>>>t'}.

\vspace{1em}

\begin{Remark}

Note that Agda is very liberal with variable names, allowing almost any
ASCII or Unicode characters, and it is customary to give descriptive
names to the variables, usually denoting their type. In this instance,
\texttt{t>>>t'} is a variable of type \texttt{t >>> t'}.\\
Due to Agda's relative freedom in variable names, whitespace is
important, as \texttt{t>>> t'} is very different from \texttt{t >>> t'}
(the first is parsed a two variables \texttt{t>>>} and \texttt{t'},
whereas the second is parsed as the variable \texttt{t}, the relation
symbol \texttt{>>>} and another variable \texttt{t'}).

\end{Remark}

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = {!   0!}
>>>-Term-l reflY = {!   1!}
>>>-Term-l (app x t>>>t' t>>>t'') = {!   2!}
>>>-Term-l (abs L x) = {!   3!}
>>>-Term-l (beta L cf t>>>t') = {!   4!}
>>>-Term-l (Y t>>>t') = {!   5!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term (fv .x)
?1  :  Term (Y .σ)
?2  :  Term (app .m .n)
?3  :  Term (lam .m)
?4  :  Term (app (lam .m) .n)
?5  :  Term (app (Y .σ) .m)
\end{minted}

The newly expanded proof now contains 5 `holes', corresponding to the 5
constructors for the \(>>>\) reduction. The first two goals are trivial,
since any free variable or Y is a closed term. Here, one can use the
agda-mode again, applying `Refine', which is like a simple proof search,
in that it will try to advance the proof by supplying an object of the
correct type for the specified `hole'. Applying `Refine' to
\texttt{\{!\ \ \ 0!\}} and \texttt{\{!\ \ \ 1!\}} yields:

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x t>>>t' t>>>t'') = {!   0!}
>>>-Term-l (abs L x) = {!   1!}
>>>-Term-l (beta L cf t>>>t') = {!   2!}
>>>-Term-l (Y t>>>t') = {!   3!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term (app .m .n)
?1  :  Term (lam .m)
?2  :  Term (app (lam .m) .n)
?3  :  Term (app (Y .σ) .m)
\end{minted}

Since the constructor for \texttt{var} is
\texttt{var : ∀ {x} -> Term (fv x)}, it is easy to see that the
\texttt{hole} can be closed by supplying \texttt{var} as the proof of
\texttt{Term (fv .x)}.\\
A more interesting case is the \texttt{app} case, where using `Refine'
yields:

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x t>>>t' t>>>t'') = app {!   0!} {!   1!}
>>>-Term-l (abs L x) = {!   2!}
>>>-Term-l (beta L cf t>>>t') = {!   3!}
>>>-Term-l (Y t>>>t') = {!   4!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term .m
?1  :  Term .n
?2  :  Term (lam .m)
?3  :  Term (app (lam .m) .n)
?4  :  Term (app (Y .σ) .m)
\end{minted}

Here, the refine tactic supplied the constructor \texttt{app}, as it's
type \texttt{app : ∀ {e₁ e₂} -> Term e₁ -> Term e₂ -> Term (app e₁ e₂)}
fit the `hole' (\texttt{Term (app .m .n)}), generating two new `holes',
with the goal \texttt{Term .m} and \texttt{Term .n}. However, trying
`Refine' again on either of the `holes' yields no result. This is where
one applies the induction hypothesis, by adding
\texttt{>>>-Term-l t>>>t'} to \texttt{\{!\ \ \ 0!\}} and applying
`Refine' again, which closes the `hole' \texttt{\{!\ \ \ 0!\}}. Perhaps
confusingly, \texttt{>>>-Term-l t>>>t'} produces a proof of
\texttt{Term .m}. To see why this is, one has to inspect the type of
\texttt{t>>>t'} in this context. Helpfully, the agda-mode provides just
this function, which infers the type of \texttt{t>>>t'} to be
\texttt{.m >>> .m'}. Similarly, \texttt{t>>>t''} has the type
\texttt{.n >>> .n'}. Renaming \texttt{t>>>t'} and \texttt{t>>>t''} to
\texttt{m>>>m'} and \texttt{n>>>n'} respectively, now makes the
recursive call obvious:

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = app (>>>-Term-l m>>>m') {!   0!}
>>>-Term-l (abs L x) = {!   1!}
>>>-Term-l (beta L cf t>>>t') = {!   2!}
>>>-Term-l (Y t>>>t') = {!   3!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term .n
?1  :  Term (lam .m)
?2  :  Term (app (lam .m) .n)
?3  :  Term (app (Y .σ) .m)
\end{minted}

The goal \texttt{Term .n} follows in exactly the same fashion. Applying
`Refine' to the next `hole' yields:

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = app (>>>-Term-l m>>>m') (>>>-Term-l n>>>n')
>>>-Term-l (abs L x) = lam {!   0!} {!   1!}
>>>-Term-l (beta L cf t>>>t') = {!   2!}
>>>-Term-l (Y t>>>t') = {!   3!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  FVars
?1  :  {x = x₁ : ℕ} → x₁ ∉ ?0 L x → Term (.m ^' x₁)
?2  :  Term (app (lam .m) .n)
?3  :  Term (app (Y .σ) .m)
\end{minted}

At this stage, the interesting goal is \texttt{?1}, due to the fact that
it is dependent on \texttt{?0}. Indeed, replacing \texttt{?0} with
\texttt{L} (which is the only thing of the type \texttt{FVars} available
in this context) changes goal \texttt{?1} to
\texttt{\{x = x₁ : ℕ\} → x₁ ∉ L → Term (.m \textasciicircum' x₁)}:

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = app (>>>-Term-l m>>>m') (>>>-Term-l n>>>n')
>>>-Term-l (abs L x) = lam L {!   0!}
>>>-Term-l (beta L cf t>>>t') = {!   1!}
>>>-Term-l (Y t>>>t') = {!   2!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  {x = x₁ : ℕ} → x₁ ∉ L → Term (.m ^' x₁)
?1  :  Term (app (lam .m) .n)
?2  :  Term (app (Y .σ) .m)
\end{minted}

Since the goal/type of \texttt{\{!\ \ \ 0!\}} is
\texttt{\{x = x₁ : ℕ\} → x₁ ∉ L → Term (.m \textasciicircum' x₁)},
applying `Refine' will generate a lambda expression
\texttt{(λ x∉L → \{!\ \ \ 0!\})}, as this is obviously the only
`constructor' for a function type. Again, confusingly, we supply the
recursive call \texttt{>>>-Term-l (x x∉L)} to \texttt{\{!\ \ \ 0!\}}. By
examining the type of \texttt{x}, we get that \texttt{x} has the type
\texttt{\{x = x₁ : ℕ\} → x₁ ∉ L → (.m \textasciicircum' x₁) >>> (.m' \textasciicircum' x₁)}.
Then \texttt{(x x∉L)} is clearly of the type
\texttt{(.m \textasciicircum' x₁) >>> (.m' \textasciicircum' x₁)}. Thus
\texttt{>>>-Term-l (x x∉L)} has the desired type
\texttt{Term (.m \textasciicircum' .x)} (note that \texttt{.x} and
\texttt{x} are not the same in this context).

Doing these steps explicitly was not in fact necessary, as the automatic
proof search `Agsy' is capable of automatically constructing proof
objects for all of the cases above. Using `Agsy' in both of the last two
cases, the completed proof is given below:

\begin{minted}[]{agda}
>>>-Term-l : ∀ {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = app (>>>-Term-l m>>>m') (>>>-Term-l n>>>n')
>>>-Term-l (abs L x) = lam L (λ x∉L → >>>-Term-l (x x∉L))
>>>-Term-l (beta L cf t>>>t') = app 
  (lam L (λ {x} x∉L → >>>-Term-l (cf x∉L))) 
  (>>>-Term-l t>>>t')
>>>-Term-l (Y t>>>t') = app Y (>>>-Term-l t>>>t')
\end{minted}

\end{Example}

\newpage

\chapter{Intersection types}\label{intersection-types-1}

\label{chap:itypes}

Having compared different mechanizations and implementation languages
for the simply typed \(\lamy\) calculus in the previous two chapters, we
arrived at the ``winning'' combination of a locally nameless
mechanization using Agda. Carrying on in this setting, in this chapter,
we present the formalization of intersection types for the \(\lamy\)
calculus along with the proof of subject invariance for intersection
types.\\
Whilst the proof is not novel, there is, to our knowledge, no known of
it for the \(\lamy\) calculus. The chapter mainly focuses on the
engineering choices that were made in order to simplify the proofs as
much as possible, as well as the necessary implementation overheads that
were necessary for the implementation.\\
The chapter is presented in sections, each explaining implementation
details that had to be considered and any tweaks to the definitions,
presented in \cref{itypesIntro}, that were needed to be made.

\section{Intersection types in Agda}\label{intersection-types-in-agda}

The first implementation detail we had to consider was the
implementation of the definition of intersection types themselves.
Unlike simple types, the definition of intersection-types is split into
two mutually recursive definitions of strict \texttt{ITypeS}
(\(\mathcal{T}_s\)) and non-strict \texttt{IType} (\(\mathcal{T}\))
intersection types:

\begin{minted}[xleftmargin=1em, linenos=true]{agda}
data IType : Set
data ITypeS : Set

data ITypeS where
  φ : ITypeS
  _~>_ : (s : IType) -> (t : ITypeS) -> ITypeS

data IType where
  ∩ : List ITypeS -> IType
\end{minted}

The reason why \texttt{IType} is defined as a list of strict types
\texttt{ITypeS} in line 9, is due to the (usually) implicit requirement
that the types in \(\mathcal{T}\) be finite. The decision to use lists
was taken because the Agda standard library includes a definition of
lists along with definitions of list membership \(\in\) for lists and
other associated lemmas, which proved to be useful for definitions of
the \(\subseteq\) relation on types.

From the above definition, it is obvious that the above definition is
redundant, in that \texttt{IType} only has one constructor \texttt{∩},
taking a list of strict types \texttt{ITypeS}, and therefore, any
instance of \texttt{IType} in the definition of \texttt{ITypeS} can
simply be replaced by \texttt{List ITypeS}:

\begin{minted}[]{agda}
data IType : Set where
  φ : IType
  _~>_ : List IType -> IType -> IType
\end{minted}

(Note that \texttt{ITypeS} was renamed to \texttt{IType} for
convenience.)

\section{Type refinement}\label{type-refinement}

One of the first things we needed to add to the notion of intersection
type assignment (and as a result also to the \(\subseteq\) relation on
intersection types) was the notion of simple-type refinement. The main
idea of intersection types for \(\lamy\) terms is for the intersection
types to somehow ``refine'' the simple types. Intuitively, this notion
should capture the relationship between the ``shape'' of the
intersection and simple types.

To demonstrate the reason for introducing type refinement, we look at
the initial formulation of the (intersection) typing rule \((Y)\):

\begin{center}
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \RightLabel{$(j \in \underline{n})$}
  \UnaryInfC{$\Gamma \Vdash Y_\sigma : (\taui \leadsto \tau_1 \cap\hdots\cap \taui \leadsto \tau_i) \leadsto \tau_j$}
  \DisplayProof
  \vskip 1.5em
\end{center}

The lack of connection between simple and intersection types in the
typing relation is especially apparent here, as \(\taui\) seems to be
chosen arbitrarily. Once we reformulate the above definition to include
type refinement, the choice of \(\taui\) makes more sense, since we know
that \(\tau_1, \hdots, \tau_i\) will somehow be related to the simple
type \(\sigma\):

\begin{center}
  \vskip 1.5em
  \AxiomC{$\taui :: \sigma$}
  \LeftLabel{$(Y)$}
  \RightLabel{$(j \in \underline{n})$}
  \UnaryInfC{$\Gamma \Vdash Y_\sigma : (\taui \leadsto \tau_1 \cap\hdots\cap \taui \leadsto \tau_i) \leadsto \tau_j$}
  \DisplayProof
  \vskip 1.5em
\end{center}

The refinement relation has been defined in Kobayashi
(\protect\hyperlink{ref-kobayashi09}{2009}) (amongst others) and is
presented below:

\begin{Definition}[Intersection type refinement]

Since intersection types are defined in terms of strict
(\(\mathcal{T}_s\)) and non-strict (\(\mathcal{T}\)) intersection types,
the definition of refinement (\(::\)) is split into two versions, one
for strict and another for non-strict types. In the definition below,
\(\tau\) ranges over strict intersection types \(\mathcal{T}_s\), with
\(\tau_i, \tau_j\) ranging over non-strict intersection types
\(\mathcal{T}\), and \(A, B\) range over simple types \(\sigma\):

\begin{center}
  \AxiomC{}
  \LeftLabel{$(base)$}
  \UnaryInfC{$\phi ::_s \mathsf{o}$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\tau_i :: A$}
  \AxiomC{$\tau_j :: B$}
  \LeftLabel{$(arr)$}
  \BinaryInfC{$\tau_i \leadsto \tau_j ::_s A \to B$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\omega :: A$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\tau ::_s A$}
  \AxiomC{$\tau_i :: A$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\tau , \tau_i :: A$}
  \DisplayProof
\end{center}

\end{Definition}

Having a notion of type refinement, we then modified the subset relation
on intersection types, s.t. \(\subseteq\) is defined only for pairs of
intersection types, which refine the same simple type:

\begin{Definition}[$\subseteq^A$]

In the definition below, \(\tau, \tau'\) range over \(\mathcal{T}_s\),
\(\tau_i, \hdots, \tau_n\) range over \(\mathcal{T}\) and \(A, B\) range
over \(\sigma\):

\begin{center}
  \AxiomC{}
  \LeftLabel{$(base)$}
  \UnaryInfC{$\phi \subseteq^\mathsf{o}_s \phi$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\tau_i \subseteq^A \tau_j$}
  \AxiomC{$\tau_m \subseteq^B \tau_n$}
  \LeftLabel{$(arr)$}
  \BinaryInfC{$\tau_j \leadsto \tau_m \subseteq^{A \to B}_s \tau_i \leadsto \tau_n$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$\tau_i :: A$}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\omega \subseteq^A \tau_i$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\exists \tau' \in \tau_j.\ \tau \subseteq^A_s \tau'$}
  \AxiomC{$\tau_i \subseteq^A \tau_j$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\tau , \tau_i \subseteq^A \tau_j$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$\tau_i \subseteq^A \tau_j$}
  \AxiomC{$\tau_j \subseteq^A \tau_k$}
  \LeftLabel{$(trans)$}
  \BinaryInfC{$\tau_i \subseteq^A \tau_k$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

The typing relation \textbf{add link def} was also modified to include
type refinement \textbf{\ldots{} didn't want to include it here because
it will differ significantly from the initial}

\section{Quantification}\label{quantification}

There are several differences between the presentation of the
\(\subseteq\) relation in \cref{Definition:subseteqOrig} and the one
above. The main differences arise mainly in

\footnotesize

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\hypertarget{ref-aydemir05}{}
Aydemir, Brian E., Aaron Bohannon, Matthew Fairbairn, J. Nathan Foster,
Benjamin C. Pierce, Peter Sewell, Dimitrios Vytiniotis, Geoffrey
Washburn, Stephanie Weirich, and Steve Zdancewic. 2005. ``Mechanized
Metatheory for the Masses: The Poplmark Challenge.'' In \emph{Theorem
Proving in Higher Order Logics: 18th International Conference, Tphols
2005, Oxford, Uk, August 22-25, 2005. Proceedings}, edited by Joe Hurd
and Tom Melham, 50--65. Berlin, Heidelberg: Springer Berlin Heidelberg.
doi:\href{https://doi.org/10.1007/11541868_4}{10.1007/11541868\_4}.

\hypertarget{ref-aydemir08}{}
Aydemir, Brian, Arthur Charguéraud, Benjamin C. Pierce, Randy Pollack,
and Stephanie Weirich. 2008. ``Engineering Formal Metatheory.'' In
\emph{Proceedings of the 35th Annual Acm Sigplan-Sigact Symposium on
Principles of Programming Languages}, 3--15. POPL '08. New York, NY,
USA: ACM.
doi:\href{https://doi.org/10.1145/1328438.1328443}{10.1145/1328438.1328443}.

\hypertarget{ref-bakel}{}
Bakel, Steffen van. 2003. ``Semantics with Intersection Types.''
\url{http://www.doc.ic.ac.uk/~svb/SemIntTypes/Notes.pdf}.

\hypertarget{ref-barendregt13}{}
Barendregt, Henk, Wil Dekkers, and Richard Statman. 2013. \emph{Lambda
Calculus with Types}. New York, NY, USA: Cambridge University Press.

\hypertarget{ref-berghofer06}{}
Berghofer, Stefan, and Christian Urban. 2006. ``A Head-to-Head
Comparison of de Bruijn Indices and Names.'' In \emph{IN Proc. Int.
Workshop on Logical Frameworks and Metalanguages: THEORY and Practice},
46--59.

\hypertarget{ref-clairambault13}{}
Clairambault, Pierre, and Andrzej S. Murawski. 2013. ``Böhm Trees as
Higher-Order Recursive Schemes.'' In \emph{IARCS Annual Conference on
Foundations of Software Technology and Theoretical Computer Science,
FSTTCS 2013, December 12-14, 2013, Guwahati, India}, 91--102.
doi:\href{https://doi.org/10.4230/LIPIcs.FSTTCS.2013.91}{10.4230/LIPIcs.FSTTCS.2013.91}.

\hypertarget{ref-harper93}{}
Harper, Robert, Furio Honsell, and Gordon Plotkin. 1993. ``A Framework
for Defining Logics.'' \emph{J. ACM} 40 (1). New York, NY, USA: ACM:
143--84.
doi:\href{https://doi.org/10.1145/138027.138060}{10.1145/138027.138060}.

\hypertarget{ref-kobayashi09}{}
Kobayashi, Naoki. 2009. ``Types and Higher-Order Recursion Schemes for
Verification of Higher-Order Programs.'' In \emph{Proceedings of the
36th Annual Acm Sigplan-Sigact Symposium on Principles of Programming
Languages}, 416--28. POPL '09. New York, NY, USA: ACM.
doi:\href{https://doi.org/10.1145/1480881.1480933}{10.1145/1480881.1480933}.

\hypertarget{ref-kobayashi13}{}
---------. 2013. ``Model Checking Higher-Order Programs.'' \emph{J. ACM}
60 (3). New York, NY, USA: ACM: 20:1--20:62.
doi:\href{https://doi.org/10.1145/2487241.2487246}{10.1145/2487241.2487246}.

\hypertarget{ref-shing-cheng}{}
Mu, Shin-Cheng. 2011. ``Proving the Church-Rosser Theorem Using a
Locally Nameless Representation.'' Blog.
\url{http://www.iis.sinica.edu.tw/~scm/2011/proving-the-church-rosser-theorem}.

\hypertarget{ref-ong06}{}
Ong, C.-H. L. 2006. ``On Model-Checking Trees Generated by Higher-Order
Recursion Schemes.'' In \emph{Proceedings of the 21st Annual Ieee
Symposium on Logic in Computer Science}, 81--90. LICS '06. Washington,
DC, USA: IEEE Computer Society.
doi:\href{https://doi.org/10.1109/LICS.2006.38}{10.1109/LICS.2006.38}.

\hypertarget{ref-pfenning88}{}
Pfenning, F., and C. Elliott. 1988. ``Higher-Order Abstract Syntax.'' In
\emph{Proceedings of the Acm Sigplan 1988 Conference on Programming
Language Design and Implementation}, 199--208. PLDI '88. New York, NY,
USA: ACM.
doi:\href{https://doi.org/10.1145/53990.54010}{10.1145/53990.54010}.

\hypertarget{ref-pfenning99}{}
Pfenning, Frank, and Carsten Schürmann. 1999. ``Automated Deduction ---
Cade-16: 16th International Conference on Automated Deduction Trento,
Italy, July 7--10, 1999 Proceedings.'' In, 202--6. Berlin, Heidelberg:
Springer Berlin Heidelberg.
doi:\href{https://doi.org/10.1007/3-540-48660-7_14}{10.1007/3-540-48660-7\_14}.

\hypertarget{ref-pollack95}{}
Pollack, Robert. 1995. ``Polishing up the Tait-Martin-Löf Proof of the
Church-Rosser Theorem.''

\hypertarget{ref-ramsay14}{}
Ramsay, Steven J., Robin P. Neatherway, and C.-H. Luke Ong. 2014. ``A
Type-Directed Abstraction Refinement Approach to Higher-Order Model
Checking.'' \emph{SIGPLAN Not.} 49 (1). New York, NY, USA: ACM: 61--72.
doi:\href{https://doi.org/10.1145/2578855.2535873}{10.1145/2578855.2535873}.

\hypertarget{ref-takahashi95}{}
Takahashi, M. 1995. ``Parallel Reductions in \(\lambda\)-Calculus.''
\emph{Information and Computation} 118 (1): 120--27.
\url{http://www.sciencedirect.com/science/article/pii/S0890540185710577}.

\hypertarget{ref-tsukada14}{}
Tsukada, Takeshi, and C.-H. Luke Ong. 2014. ``Compositional Higher-Order
Model Checking via \$\$-Regular Games over Böhm Trees.'' In
\emph{Proceedings of the Joint Meeting of the Twenty-Third Eacsl Annual
Conference on Computer Science Logic (Csl) and the Twenty-Ninth Annual
Acm/Ieee Symposium on Logic in Computer Science (Lics)}, 78:1--78:10.
CSL-Lics '14. New York, NY, USA: ACM.
doi:\href{https://doi.org/10.1145/2603088.2603133}{10.1145/2603088.2603133}.

% - Back matter ----------------------------------------------------------------




\end{document}
