\documentclass[a4paper, 12pt, twoside]{style/ociamthesis}
% - Customization --------------------------------------------------------------
% - These settings are changed in metadata.yaml
% - You should not touch anything here

\title{A formalization of the \(\lamy\) calculus}            % the title of the thesis

\author{Samuel Balco}          % your name

\college{GTC}        % your college

\supervisor{Faris Abou-Saleh, Luke Ong and Steven Ramsay}  % your supervisor

\degree{MSc in Computer Science}          % the degree
\degreedate{Trinity 2016}  % the degree date

\logofile{style/logobar}

%input macros (i.e. write your own macros file called mymacros.tex
%and uncomment the next line)
% \include{}

% -----------------------------------------------------------------------------
% -- PACKAGES -----------------------------------------------------------------
% -----------------------------------------------------------------------------
\usepackage{framed}
\usepackage{epstopdf}
\usepackage[usenames,dvipsnames]{xcolor}
% Set figure legends and captions to be smaller sized sans serif font
\usepackage[font={footnotesize,sf}]{caption}
\usepackage{float}

\usepackage[titletoc]{appendix}
\usepackage{pdfpages} % incluce pdf files
\usepackage{wallpaper}


% amsthm stuff --------------------------------------------------
\usepackage{amsthm}

\makeatletter
\def\th@definition{%
  \thm@headfont{\bfseries}% same as heading font
  \thm@notefont{\bfseries}% same as heading font
  \normalfont % body font
}
\makeatother

\theoremstyle{plain}
\theoremstyle{definition}
\newtheorem{Theorem}{Theorem}[chapter]
\newtheorem{Lemma}{Lemma}[chapter]
\newtheorem{Definition}{Definition}[chapter]
\newtheorem{Example}{Example}[chapter]
\theoremstyle{remark}
\newtheorem*{Remark}{Remark}
\newtheorem*{Note}{Note}

\makeatletter
\def\thm@space@setup{%
  \thm@preskip=2\parskip \thm@postskip=0pt
}
\makeatother

\makeatletter
\renewenvironment{proof}[1][\proofname]{\par
  \vspace{-\topsep}% remove the space after the theorem
  \pushQED{\qed}%
  \normalfont
  \topsep0pt \partopsep0pt % no space before
  \trivlist
  \item[\hskip\labelsep
        \itshape
    #1\@addpunct{.}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
  \addvspace{6pt plus 6pt} % some space after
}
\makeatother
% -------------------------------------------------------------


% - Font Stuff starts   ---------------------------------------------------------
% \usepackage{xltxtra}

% % \usepackage{lmodern}
% 
% % \usepackage{amssymb,amsmath}
% \usepackage{ifxetex,ifluatex}
% \usepackage{fixltx2e} % provides \textsubscript

% % use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% % use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Fira Sans Light}
    \setsansfont[]{Fira Sans}
    \setmonofont[Mapping=tex-ansi,Scale=0.8]{FreeMono}
    \setmathfont(Digits,Latin,Greek)[]{Fira Sans Light}

% - Geometry  ------------------------------------------------------------------
\usepackage[margin=3cm]{geometry}

% - Layout ---------------------------------------------------------------------
\usepackage{fancyhdr}
\usepackage{sectsty}
\usepackage{titlesec}
\titleformat{\chapter}{\bfseries\huge}{\thechapter.}{20pt}{\huge}

% - Links ----------------------------------------------------------------------
\PassOptionsToPackage{usenames,dvipsnames}{xcolor} % color is loaded by hyperref
\ifxetex
\usepackage[pdfusetitle,setpagesize=false, % page size defined by xetex
  unicode=false, % unicode breaks when used with xetex
xetex]{hyperref}
\else
  \usepackage[unicode=true]{hyperref}
\fi

% Make sure url breaks
\usepackage{url}
\makeatletter
\g@addto@macro{\UrlBreaks}{\UrlOrds}
\makeatother
\hypersetup{
            pdftitle={A formalization of the \textbackslash{}lamy calculus},
            pdfauthor={Samuel Balco},
            colorlinks=true,
            linkcolor=cyan,
            citecolor=cyan,
            urlcolor=cyan,
            breaklinks=true}
\urlstyle{same}   % don't use monospace font for urls
\usepackage[all]{hypcap}% improve link placement in floats


% better names when cross-referencing with cleveref
\usepackage[nameinlink]{cleveref}
\makeatother
\crefname{listing}{Figure}{Figures}
\Crefname{listing}{Figure}{Figures}
\crefname{chapter}{Chapter}{Chapters}
\Crefname{chapter}{Chapter}{Chapters}
\crefname{section}{Section}{Sections}
\Crefname{section}{Section}{Sections}
\crefname{subsection}{Section}{Sections}
\Crefname{subsection}{Section}{Sections}
\crefname{subsubsection}{Section}{Sections}
\Crefname{subsubsection}{Section}{Sections}
\crefname{figure}{Figure}{Figures} % changes default behavior to Figure. 1
\Crefname{figure}{Figure}{Figures} % changes default behavior to Figure. 1
\crefname{table}{Table}{Tables}
\Crefname{table}{Table}{Tables}
\crefname{subfigure}{Figure}{Figures}
\Crefname{subfigure}{Figure}{Figures}
\crefname{subsubfigure}{Figure}{Figures}
\Crefname{subsubfigure}{Figure}{Figures}
\crefname{appendix}{Appendix}{Appendices}
\Crefname{appendix}{Appendix}{Appendices}
\crefname{Definition}{Definition}{Definitions}
\Crefname{Definition}{Definition}{Definitions}
\crefname{Lemma}{Lemma}{Lemmas}
\Crefname{Lemma}{Lemma}{Lemmas}
\crefname{Theorem}{Theorem}{Theorems}
\Crefname{Theorem}{Theorem}{Theorems}
\crefname{Example}{Example}{Examples}
\Crefname{Example}{Example}{Examples}

% - Language -------------------------------------------------------------------

% - Bibliography  --------------------------------------------------------------

% - Listings -------------------------------------------------------------------

% - Graphics  ------------------------------------------------------------------
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}

% - Other Options --------------------------------------------------------------



\usepackage{longtable,booktabs}

% Make links footnotes instead of hotlinks:
\renewcommand{\href}[2]{#2\footnote{\url{#1}}}


\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}

\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\setcounter{secnumdepth}{5}

% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi


% - Add to Header --------------------------------------------------------------
\usepackage{bussproofs}
\usepackage{stackengine}
\usepackage{minted}
\usepackage{tikz}
\usetikzlibrary{arrows,automata, positioning}
\usepackage{isabelle,isabellesym}
\newcommand{\lamy}{\lambda\text{-}Y}
\newcommand{\concat}{\ensuremath{+\!\!\!\!+\,}}
\newcommand{\wf}{\textsf{Wf-ICtxt}\ }
\newcommand{\red}{\Rightarrow_Y}
\newcommand{\cls}{{}^{\backslash x}}
\newcommand{\tocap}{\leadsto\kern-.5ex\cap}
\newcommand{\conR}{\concat_{\kern-1ex R}}
\newcommand{\conL}{\concat_{\kern-1ex L}}
\newcommand{\poplm}{\textsc{PoplMark}}
\renewcommand{\max}{\textsf{max}\ }
\newcommand{\dip}{\textsf{dp}}
\newcommand{\trm}{\textsf{term}}
\newcommand{\fv}{\textsf{FV}}
\newcommand{\taui}{\bigcap\nolimits_{\underline{n}} \tau_i}
\newcommand{\dom}{\mathsf{dom}}
\newcommand{\amp}{\text{\&}}
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\usepackage{mdframed}
\mdfdefinestyle{example}{linewidth=2pt,topline=false,bottomline=false,rightline=false}
\let\OldExample\Example
\renewenvironment{Example}{\begin{OldExample}\begin{mdframed}[style=example, linecolor=yellow]}{\end{mdframed}\end{OldExample}}
\let\OldRemark\Remark
\renewenvironment{Remark}{\begin{OldRemark}\begin{mdframed}[style=example, linecolor=black]}{\end{mdframed}\end{OldRemark}}
\let\OldNote\Note
\renewenvironment{Note}{\begin{OldNote}\begin{mdframed}[style=example, linecolor=black]}{\end{mdframed}\end{OldNote}}
\let\OldDefinition\Definition
\renewenvironment{Definition}{\begin{OldDefinition}\begin{mdframed}[style=example, linecolor=cyan]}{\end{mdframed}\end{OldDefinition}}
\let\OldLemma\Lemma
\renewenvironment{Lemma}{\begin{OldLemma}\begin{mdframed}[style=example, linecolor=cyan]}{\end{mdframed}\end{OldLemma}}
\let\OldTheorem\Theorem
\renewenvironment{Theorem}{\begin{OldTheorem}\begin{mdframed}[style=example, linecolor=cyan]}{\end{mdframed}\end{OldTheorem}}
\BeforeBeginEnvironment{minted}{\vspace{1em}\begin{mdframed}[style=example, linecolor=magenta]}
\AfterEndEnvironment{minted}{\end{mdframed}}

\pagenumbering{roman}

\begin{document}

% - Title ----------------------------------------------------------------------
\maketitle
\null\newpage

% - Dedication -----------------------------------------------------------------

% - Acknowledgements -----------------------------------------------------------
\begin{acknowledgements}
Say thanks to whoever listened to your rants for 2 months
\end{acknowledgements}
\null\newpage


% - Originality ----------------------------------------------------------------


% - Abstract -------------------------------------------------------------------
\begin{abstract}
Higher order model checking (HOMC), has been intensively studied in
recent years (C.-H. L. Ong (\protect\hyperlink{ref-ong06}{2006}),
Kobayashi (\protect\hyperlink{ref-kobayashi13}{2013}), Ramsay,
Neatherway, and Ong (\protect\hyperlink{ref-ramsay14}{2014}), Tsukada
and Ong (\protect\hyperlink{ref-tsukada14}{2014})). A common approach to
studying HOMC is through higher order recursion schemes (HORS).\\
Recently, it was shown that \(\lamy\) calculus with intersection-types
can be used as an alternative to HORS, when studying higher order model
checking. Whilst the theory of HORS and \(\lamy\) has been formalized
``on paper'', there has been little done in mechanizing this theory in a
fully formal setting of a theorem prover.\\
The aim of this project was to start such a formalization, by first
mechanizing the \(\lamy\) calculus along with the proof of confluence
for simple types. This served as a benchmark for choosing the
implementation specifics, such as the binder representation strategy as
well as the implementation language.\\
The formalization of the terms of the calculus along with the proof of
confluence served as a benchmark for comparing the chosen mechanization
approaches. The best mechanization was then extended with the
formalization of intersection types for the \(\lamy\) calculus, along
with the proofs of subject invariance.
\end{abstract}
\null\newpage



% - Table of Contents  ---------------------------------------------------------
% \begin{romanpages}
 \hypersetup{linkcolor=black} 
\setcounter{page}{1}
\setcounter{tocdepth}{2}
\tableofcontents 
% \end{romanpages}
% - List of Tables -------------------------------------------------------------
% \end{romanpages}
\newpage

 \hypersetup{linkcolor=cyan} 

% - BODY -----------------------------------------------------------------------
\pagenumbering{arabic}
\chapter{Introduction}\label{introduction}

\label{chap:intro}

\section{Motivation}\label{motivation}

Formal verification of software has been essential in many safety
critical systems in the industry and is a field of active research in
computer science. One of the main approaches to verification is model
checking, wherein a system specification is checked against certain
correctness properties, by generating a model of the system, encoding
the desired correctness property as a logical formula and then
exhaustively checking whether the given formula is satisfiable in the
model of the system. Big advances in model checking of
1\textsuperscript{st} order (imperative) programs have been made, with
techniques like abstraction refinement and SAT/SMT-solver use, allowing
scalability.\\
Aspects of functional programming, such as anonymous/\(\lambda\)
functions have gained prominence in mainstream languages, such as C++ or
JavaScript and functional languages like Scala, F\# or Haskell have
garnered wider interest. With growing interest in using functional
programming, interest in verifying higher-order functional programs has
also grown. Current approaches to formal verification of such programs
usually involve the use of (automatic) theorem provers, which usually
require a lot of user interaction and as a result have not managed to
scale as well as model checking in the 1\textsuperscript{st} order
setting.\\
Using type systems is another way to ensure program safety, but using
expressive-enough types often requires explicit type annotations, since
type checking/inference usually becomes undecidable, as is the case for
dependent-type systems. Simpler type systems, where type inference is
decidable, can instead prove too coarse, i.e.~the required properties
are difficult if not impossible to capture in such type systems.\\
In recent years, advances in higher order model checking (HOMC) have
been made (C.-H. L. Ong (\protect\hyperlink{ref-ong06}{2006}), Kobayashi
(\protect\hyperlink{ref-kobayashi13}{2013}), Ramsay, Neatherway, and Ong
(\protect\hyperlink{ref-ramsay14}{2014}), Tsukada and Ong
(\protect\hyperlink{ref-tsukada14}{2014})), but whilst a lot of theory
has been developed for HOMC, there has been little done in
implementing/mechanizing these results in a fully formal setting of a
theorem prover.

\section{Aims}\label{aims}

The aim of this project is to make a start of mechanizing the proofs
underpinning HOMC approaches using type-checking of higher-order
recursion schemes, by formally proving certain key properties about the
\(\lamy\) calculus with an intersection-type system (Clairambault and
Murawski (\protect\hyperlink{ref-clairambault13}{2013}), Tsukada and Ong
(\protect\hyperlink{ref-tsukada14}{2014})), which can be used to study
HOMC as an alternative to higher order recursion schemes (HORS).\\
The project is roughly split into two main parts, with the first part
exploring and evaluating different formalizations of the simply-typed
\(\lamy\) calculus together with the proof of the Church Rosser
Theorem.\\
This part of the project focuses on the mechanization aspect of the
simply typed \(\lamy\) calculus, using a theorem prover in a fashion
similar to the \(\poplm\) challenge, namely exploring different theorem
provers and the possible encodings of binders. The reason why we chose
to do such a comparison was to evaluate and chose the best mechanization
approach for the \(\lamy\) calculus, as there is little information
available concerning the merits and disadvantages of different
implementation approaches of \(\lamy\) or indeed just the (simply typed)
\(\lambda\)-calculus. The comparison of different mechanizations focuses
on the engineering choices and formalization overheads which result from
translating the informal definitions into a fully-formal setting of a
theorem prover.\\
The reason why we chose to formalize the Church Rosser theorem was to to
test the implementation of a non-trivial, but simple enough proof in a
fully formal setting.\\
The second part focuses on implementing the intersection-type system for
the \(\lamy\) calculus and formalizing the proof of subject invariance
for this type system. The formalization and engineering choices made in
the implementation of the intersection-type system reflect the survey
and analysis of the different mechanization choices, explored in the
first part of the project.\\
All the code described in this project can be found a the git repository
at: \url{https://github.com/goodlyrottenapple/lamYcalc}.

\section{Main Achievements}\label{main-achievements}

\textbf{TODO: Expand on the points eventually\ldots{}.leaving for the
end}

\begin{itemize}
\tightlist
\item
  Formalization of the simply typed \(\lamy\) calculus and proofs of
  confluence in Isabelle, using both Nominal sets and locally nameless
  encoding of binders.
\item
  Formalization of the simply typed \(\lamy\) calculus and proofs of
  confluence in Agda, using a locally nameless encoding of binders
\item
  Analysis and comparison of binder encodings
\item
  Comparison of Agda and Isabelle
\item
  Formalization of an intersection-type system for the \(\lamy\)
  calculus and proof of subject invariance for intersection-types
\end{itemize}

\section{Dissertation Structure}\label{dissertation-structure}

The dissertation has 7 chapters. The first part of this document
(chapters 1-3) describes the domain and the goals of this project, the
second part (chapters 4 and 5) is a comparison of several mechanizations
of the simply typed \(\lamy\) calculus, the third part (chapter 6)
discusses the intersection typing and associated proofs.\\
\cref{chap:intro} is an overview of the aims and achievements of this
project.\\
\cref{chap:background} gives an introduction to the \(\lamy\) calculus,
together with an overview of the proof of confluence (Church Rosser).
The chapter also introduces intersection types and discusses an
important aspect of a \(\lambda\)-calculus mechanization, namely the
treatment of binders in a fully formal setting of a theorem prover.\\
\cref{chap:method} introduces the methodology used for comparing the
different mechanizations discussed in later chapters.\\
\cref{chap:compIsa} compares two mechanizations of the \(\lamy\)
calculus (nominal and locally nameless), which are focused around the
treatment of binders. The comparison looks at the overall length and
structure of the two formalizations, as well as using specific instances
of the same definitions/lemmas across the two mechanizations, to
illustrate the advantages and disadvantages of both approaches.\\
\cref{chap:compAgda} details the differences between using Isabelle and
Agda for formalizing the \(\lamy\) calculus.\\
\cref{chap:itypes} discusses the implementation details of intersection
types for the \(\lamy\) calculus and the various engineering choices
that were made in order to simplify the ensuing proof of subject
invariance.\\
\cref{chap:concl} summarizes the outcomes of the project and details
possible further work.

\chapter{Background}\label{background}

\label{chap:background}

\section{Binders}\label{binders}

\label{binders}

When describing the (untyped) \(\lambda\)-calculus on paper, the terms
of the \(\lambda\)-calculus are usually inductively defined in the
following way:

\begin{center}
$t::= x\ |\ tt\ |\ \lambda x.t \text{ where }x \in Var$
\end{center}

This definition of terms yields an induction/recursion principle, which
can be used to define functions over the \(\lambda\)-terms by structural
recursion and prove properties about the \(\lambda\)-terms using
structural induction (recursion and induction being two sides of the
same coin).\\
However, whilst the definition above describes valid terms of the
\(\lambda\)-calculus, there are implicit assumptions one makes about the
terms, namely, the \(x\) in the \(\lambda x.t\) case appears bound in
\(t\). This means that while \(x\) and \(y\) might be distinct terms of
the \(\lambda\)-calculus (i.e. \(x \neq y\)), \(\lambda x.x\) and
\(\lambda y.y\) represent the same term, as \(x\) and \(y\) are bound by
the \(\lambda\). Without the notion of \(\alpha\)-equivalence of terms,
one cannot prove any properties of terms involving bound variables, such
as saying that \(\lambda x.x \equiv \lambda y.y\).

In an informal setting, reasoning with \(\alpha\)-equivalence of terms
is often very implicit, however in a formal setting of theorem provers,
having an inductive definition of ``raw'' \(lambda\)-terms, which are
not \(alpha\)-equivalent, yet reasoning about \(\alpha\)-equivalent
\(\lambda\)-terms poses certain challenges.\\
One of the main problems is the fact that the inductive/recursive
definition does not easily lift to \(alpha\)-equivalent terms. Take a
trivial example of a function on raw terms, which checks whether a
variable appears bound in a given \(\lambda\)-term. Clearly, such
function is well formed for ``raw'' terms, but does not work (or even
make sense) for \(\alpha\)-equivalent terms.\\
Conversely, there are informal definitions over \(\alpha\)-equivalent
terms, which are not straight-forward to define over raw terms. Take the
usual definition of substitution, defined over \(\alpha\)-equivalent
terms, which actually relies on the following fact in the
\(\lambda\)-case:

\begin{center}
$(\lambda y'. s')[t/x] \equiv \lambda y'.(s'[t/x]) \text{ assuming } y' \not\equiv x\text{ and }y' \not\in FV(t)$
\end{center}

In this, the \(\lambda\) case, it is assumed, that a given
\(\lambda\)-term \(\lambda y. s\) can always be swapped out for an alpha
equivalent term \(\lambda y'. s'\), such that \(y'\) satisfies the side
condition. The assumption that a bound variable can be swapped out for a
``fresh'' one to avoid name clashes is often referred to as the
Barendregt Variable Convention.

The direct approach of defining ``raw'' terms and an additional notion
of \(\alpha\)-equivalence introduces a lot of overhead when defining
functions, as one either has to use the recursive principles for ``raw''
terms and then show that the function lifts to the \(\alpha\)-equivalent
terms or define functions on \(alpha\)-equivalence classes and prove
that it is well-founded, without being able to rely on the structurally
inductive principles that one gets ``for free'' with the ``raw''
terms.\\
Because of this, the usual informal representation of the
\(\lambda\)-calculus is rarely used in a fully formal setting.

To mitigate the overheads of a fully formal definition of the
\(\lambda\)-calculus, we want to have an encoding of \(\lambda\)-terms,
which includes the notion of \(\alpha\)-equivalence, whilst being
inductively defined, giving us the inductive/recursive principles for
\(alpha\)-equivalent terms directly. This can be achieved in several
different ways. In general, there are two main approaches taken in a
rigorous formalization of the terms of the lambda calculus, namely the
concrete approaches and the higher-order approaches, both described in
some detail below.

\subsection{Concrete approaches}\label{concrete-approaches}

The concrete or first-order approaches usually encode variables using
names (like strings or natural numbers). Encoding of terms and
capture-avoiding substitution must be encoded explicitly. A survey by B.
Aydemir et al. (\protect\hyperlink{ref-aydemir08}{2008}) details three
main groups of concrete approaches, found in formalizations of the
\(\lambda\)-calculus in the literature:

\subsubsection{Named}\label{named}

This approach generally defines terms in much the same way as the
informal inductive definition given above. Using a functional language,
such as Haskell or ML, such a definition might look like this:

\begin{minted}[]{isabelle}
datatype trm =
  Var name
| App trm trm
| Lam name trm
\end{minted}

As was mentioned before, defining ``raw'' terms and the notion of
\(\alpha\)-equivalence of ``raw'' terms separately carries a lot of
overhead in a theorem prover and is therefore not favored.

To obtain an inductive definition of \(\lambda\)-terms with a built in
notion of \(\alpha\)-equivalence, one can instead use nominal sets. The
theory of nominal sets captures the notion of bound variables and
freshness, as it is based around the notion of having properties
invariant in name permutation.\\
The nominal package in Isabelle provides tools to automatically define
terms with binders, which generate inductive definitions of
\(\alpha\)-equivalent terms. Using nominal sets in Isabelle results in a
definition of terms, which looks very similar to the informal
presentation of the lambda calculus:

\begin{minted}[]{isabelle}
nominal_datatype trm =
  Var name
| App trm trm
| Lam x::name l::trm  binds x in l
\end{minted}

Most importantly, this definition allows one to define functions over
\(\alpha\)-equivalent terms using structural induction. The nominal
package also provides freshness lemmas and a strengthened induction
principle with name freshness for terms involving binders.

\subsubsection{Nameless/de Bruijn}\label{namelessde-bruijn}

Using a named representation of the \(\lambda\)-calculus in a fully
formal setting can be inconvenient when dealing with bound variables.
For example, substitution, as described in the introduction, with its
side-condition of freshness of \(y\) in \(x\) and \(t\) is not
structurally recursive on ``raw'' terms, but rather requires
well-founded recursion over \(\alpha\)-equivalence classes of terms. To
avoid this problem in the definition of substitution, the terms of the
lambda calculus can be encoded using de Bruijn indices, instead of named
variables:

\begin{minted}[]{isabelle}
datatype trm =
  I nat
| App trm trm
| Lam trm
\end{minted}

The indices are natural numbers, which encode an occurrence of a
variable in a \(\lambda\)-term. For bound variables, the index indicates
which \(\lambda\) it refers to, by encoding the number of
\(\lambda\)-binders that are in the scope between the index and the
\(\lambda\)-binder the variable corresponds to.

\begin{Example}

The term \(\lambda x.\lambda y. yx\) will be represented as
\(\lambda\ \lambda\ 0\ 1\). Here, 0 stands for \(y\), as there are no
binders in scope between itself and the \(\lambda\) it corresponds to,
and \(1\) corresponds to \(x\), as there is one \(\lambda\)-binder in
scope. To encode free variables, one simply choses an index greater than
the number of \(\lambda\)'s currently in scope, for example,
\(\lambda\ 4\).

\end{Example}

To see that this representation of \(\lambda\)-terms is isomorphic to
the usual named definition, we can define two functions \(f\) and \(g\),
which translate the named representation to de Bruijn notation and vice
versa. More precisely, since we are dealing with \(\alpha\)-equivalence
classes, it is an isomorphism between the equivalence classes of named
\(\lambda\)-terms and de Bruijn terms.

To make things easier, we consider a representation of named terms,
where we map named variables, \(x, y, z,...\) to indexed variables
\(x_1,x_2,x_3,...\). Then, the mapping from named terms to de Bruijn
term is given by \(f\), which we define in terms of an auxiliary
function \(e\):

\begin{center}
$\begin{aligned}
e_k^m(x_n) &= \begin{cases}
k-m(x_n)-1 & x_n \in \text{dom }m\\
k+n & otherwise
\end{cases}\\
e_k^m(uv) &= e_k^m(u)\ e_k^m(v)\\
e_k^m(\lambda x_n.u) &= \lambda\ e_{k+1}^{m \oplus (x_n,k)}(u)
\end{aligned}$
\end{center}

Then \(f(t) = e_0^\emptyset(t)\).

The function \(e\) takes two additional parameters, \(k\) and \(m\).
\(k\) keeps track of the scope from the root of the term and \(m\) is a
map from bound variables to the levels they were bound at. In the
variable case, if \(x_n\) appears in \(m\), it is a bound variable, and
its index can be calculated by taking the difference between the current
index and the index \(m(x_k)\) (at which the variable was bound). If
\(x_n\) is not in \(m\), then the variable is encoded by adding the
current level \(k\) to \(n\).\\
In the abstraction case, \(x_n\) is added to \(m\) with the current
level \(k\), possibly overshadowing a previous binding of the same
variable at a different level (like in
\(\lambda x_1. (\lambda x_1. x_1)\)) and \(k\) is incremented, going
into the body of the abstraction.

The function \(g\), taking de Bruijn terms to named terms is a little
more tricky. We need to replace the indices encoding free variables
(those that have a value greater than or equal to \(k\), where \(k\) is
the number of binders in scope) with named variables, such that for
every index \(n\), we substitute \(x_m\), where \(m = n-k\), without
capturing these free variables.

We need two auxiliary functions to define \(g\):

\begin{center}
$\begin{aligned}
h_k^b(n) &= \begin{cases}
x_{n-k} & n \geq k\\
x_{k+b-n-1} & otherwise
\end{cases}\\
h_k^b(uv) &= h_k^b(u)\ h_k^b(v)\\
h_k^b(\lambda u) &= \lambda x_{k+b}.\ h_{k+1}^b(u)\\[2.5em]
\Diamond_k(n) &= \begin{cases}
n-k & n \geq k\\
0 & otherwise
\end{cases}\\
\Diamond_k(uv) &= \max (\Diamond_k(u),\ \Diamond_k(v))\\
\Diamond_k(\lambda u) &= \Diamond_{k+1}(u)
\end{aligned}$
\end{center}

The function \(g\) is then defined as
\(g(t) = h_0^{\Diamond_0(t)+1}(t)\). As mentioned above, the complicated
definition has to do with avoiding free variable capture. A term like
\(\lambda (\lambda\ 2)\) intuitively represents a named \(\lambda\)-term
with two bound variables and a free variable \(x_0\) according to the
definition above. If we started giving the bound variables names in a
naive way, starting from \(x_0\), we would end up with a term
\(\lambda x_0.(\lambda x_1.x_0)\), which is obviously not the term we
had in mind, as \(x_0\) is no longer a free variable. To ensure we start
naming the bound variables in such a way as to avoid this situation, we
use \(\Diamond\) to compute the maximal value of any free variable in
the given term, and then start naming bound variables with an index one
higher than the value returned by \(\Diamond\).

As one quickly notices, a term like \(\lambda x.x\) and \(\lambda y.y\)
have a single unique representation as a de Bruijn term \(\lambda\ 0\).
Indeed, since there are no named variables in a de Bruijn term, there is
only one way to represent any \(\lambda\)-term, and the notion of
\(\alpha\)-equivalence is no longer relevant. We thus get around our
problem of having an inductive principle and \(\alpha\)-equivalent
terms, by having a representation of \(\lambda\)-terms where every
\(\alpha\)-equivalence class of \(\lambda\)-terms has a single
representative term in the de Bruijn notation.

In their comparison between named vs.~nameless/de Bruijn representations
of \(\lambda\)-terms, Berghofer and Urban
(\protect\hyperlink{ref-berghofer06}{2006}) give details about the
definition of substitution, which no longer needs the variable
convention and can therefore be defined using primitive structural
recursion.\\
The main disadvantage of using de Bruijn indices is the relative
unreadability of both the terms and the formulation of properties about
these terms. For instance, take the substitution lemma, which in the
named setting would be stated as:

\begin{center}
$\text{If }x \neq y\text{ and }x \not\in FV(L)\text{, then }
M[N/x][L/y] \equiv M[L/y][N[L/y]/x].$
\end{center}

In de Bruijn notation, the statement of this lemma becomes:

\begin{center}
$\text{For all indices }i, j\text{ with }i \leq j\text{, }M[N/i][L/j] = M[L/j + 1][N[L/j - i]/i]$
\end{center}

Clearly, the first version of this lemma is much more intuitive.

\subsubsection{Locally Nameless}\label{locally-nameless}

The locally nameless approach to binders is a mix of the two previous
approaches. Whilst a named representation uses variables for both free
and bound variables and the nameless encoding uses de Bruijn indices in
both cases as well, a locally nameless encoding distinguishes between
the two types of variables.\\
Free variables are represented by names, much like in the named version,
and bound variables are encoded using de Bruijn indices. By using de
Bruijn indices for bound variables, we again obtain an inductive
definition of terms which are \(\alpha\)-equivalent.

While closed terms, like \(\lambda x.x\) and \(\lambda y.y\) are
represented as de Bruijn terms, the term \(\lambda x.xz\) and
\(\lambda y.yz\) are encoded as \(\lambda\ 0z\). The following
definition captures the syntax of the locally nameless terms:

\begin{minted}[]{isabelle}
datatype ptrm =
  Fvar name
  BVar nat
| App trm trm
| Lam trm
\end{minted}

Note however, that this definition doesn't quite fit the notion of
\(\lambda\)-terms, since a \texttt{ptrm} like \texttt{(BVar 0)} does not
represent a valid \(\lambda\)-term, since bound variables can only
appear in the context of a \(\lambda\), such as in
\texttt{(Lam (BVar 0))}.\\
The advantage of using a locally nameless definition of
\(\lambda\)-terms is a better readability of such terms, compared to
equivalent de Bruijn terms. Another advantage is the fact that
definitions of functions and reasoning about properties of these terms
is much closer to the informal setting.

\subsection{Higher-Order approaches}\label{higher-order-approaches}

Unlike concrete approaches to formalizing the \(\lambda\)-calculus,
where the notion of binding and substitution is defined explicitly in
the host language, higher-order formalizations use the function space of
the implementation language, which handles binding. HOAS, or
higher-order abstract syntax (Pfenning and Elliott
\protect\hyperlink{ref-pfenning88}{1988}, Harper, Honsell, and Plotkin
(\protect\hyperlink{ref-harper93}{1993})), is a framework for defining
logics based on the simply typed \(\lambda\)-calculus. \\
Using HOAS for encoding the \(\lambda\)-calculus comes down to encoding
binders using the meta-language binders. This way, the definitions of
capture avoiding substitution or notion of \(\alpha\)-equivalence are
offloaded onto the meta-language. As an example, take the following
definition of terms of the \(\lambda\)-calculus in Haskell:

\begin{minted}[]{haskell}
data Term where
  Var :: Int -> Term
  App :: Term -> Term -> Term
  Lam :: (Term -> Term) -> Term
\end{minted}

This definition avoids the need for explicitly defining substitution,
because it encodes a \(\lambda\)-term as a Haskell function
\texttt{(Term -> Term)}, relying on Haskell's internal substitution and
notion of \(\alpha\)-equivalence. As with the de Bruijn and locally
nameless representations, this encoding gives us inductively defined
terms with a built in notion of \(\alpha\)-equivalence.\\
However, using HOAS only works if the notion of \(\alpha\)-equivalence
and substitution of the meta-language coincide with these notions in the
object-language.

\newpage

\section{Simple types}\label{simple-types}

The simple types presented throughout this work (except for
\cref{chap:itypes}) are often referred to as simple types \emph{a la
Curry}, where a simply typed \(\lambda\)-term is a triple
\((\Gamma, M, \sigma)\) written as \(\Gamma \vdash M : \sigma\), where
\(\Gamma\) is the typing context, \(M\) is a term of the untyped
\(\lambda\)-calculus and \(\sigma\) is a simple type. A well typed term
is valid, if one can construct a typing tree from the given type and
typing context, using the following deduction system:

\begin{Definition}[Simple-type assignment]

\begin{center}
  \vskip 1.5em
  \AxiomC{$x : A \in \Gamma$}
  \LeftLabel{$(var)$}
  \UnaryInfC{$\Gamma \vdash x : A$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$\Gamma \vdash u : A \to B$}
  \AxiomC{$\Gamma \vdash v : A$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$\Gamma \vdash uv : B$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{$x : A,\Gamma \vdash m : B$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\Gamma \vdash \lambda.m : A \to B$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$\Gamma \vdash Y_{A} : (A \to A) \to A$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

\begin{Example}

Take the following simply typed term
\(\{y:\tau\} \vdash \lambda x.xy : (\tau \to \phi) \to \phi\). To show
that this is a well-typed \(\lambda\)-term, we construct the following
typing tree:

\begin{center}
    \vskip 1.5em
    \AxiomC{}
    \LeftLabel{$(var)$}
    \UnaryInfC{$\{x: \tau \to \phi,\ y:\tau\} \vdash x : \tau \to \phi$}
    \AxiomC{}
    \LeftLabel{$(var)$}
    \UnaryInfC{$\{x: \tau \to \phi,\ y:\tau\} \vdash y : \tau$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\{x: \tau \to \phi,\ y:\tau\} \vdash xy : \phi$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\{y:\tau\} \vdash \lambda x.xy : (\tau \to \phi) \to \phi$}
    \DisplayProof
\end{center}

\end{Example}

In the simple typing \emph{a la Curry}, simple types and
\(\lambda\)-terms are completely separate, brought together only through
the typing relation \(\vdash\). The definition of \(\lamy\) terms,
however, is dependent on the simple types in the case of the \(Y\)
constants, which are indexed by simple types. When talking about the
\(\lamy\) calculus, we tend to conflate the ``untyped'' \(\lamy\) terms,
which are just the terms defined in \cref{Definition:lamyTrms}, with the
``typed'' \(\lamy\) terms, which are simply-typed terms \emph{a la
Curry} of the form \(\Gamma \vdash M : \sigma\), where \(M\) is an
``untyped'' \(\lamy\) term. Thus, results about the \(\lamy\) calculus
in this work are in fact results about the ``typed'' \(\lamy\)
calculus.\\
However, the proofs of the Church Rosser theorem, as presented in the
next section, use the untyped definition of \(\beta\)-reduction. Whilst
it is possible to define a typed version of \(\beta\)-reduction, it
turned out to be much easier to first prove the Church Rosser theorem
for the so called ``untyped'' \(\lamy\) calculus and then additionally
restrict this result to only well-types \(\lamy\) terms.\\
Thus, the definition of the Church Rosser Theorem, formulated for the
\(\lamy\) calculus, is the following one:

\begin{Theorem}[Church Rosser]

\(\Gamma \vdash M : \sigma \land M \red^* M' \land M \red^* M'' \implies \exists M'''.\ \ M' \red^* M''' \land M'' \red^* M''' \land \Gamma \vdash M''' : \sigma\)

\end{Theorem}

In order to prove this typed version of the Church Rosser Theorem, we
need to prove an additional result of subject reduction for \(\lamy\)
calculus, namely:

\begin{Theorem}[Subject reduction for $\red^*$]

\label{Theorem:subRedSimp}
\(\Gamma \vdash M : \sigma \land M \red^* M' \implies \Gamma \vdash M' : \sigma\)

\end{Theorem}

\section{\texorpdfstring{\(\lamy\)
calculus}{\textbackslash{}lamy calculus}}\label{lamy-calculus}

Originally, the field of higher order model checking mainly involved
studying higher order recursion schemes (HORS), but more recently,
exploring the \(\lamy\) calculus, which is an extension of the simply
typed \(\lambda\)-calculus, in the context of HOMC has gained traction
(Clairambault and Murawski
(\protect\hyperlink{ref-clairambault13}{2013})). We therefore present
the \(\lamy\) calculus, along with the proofs of the Church Rosser
theorem and the formalization of intersection types for the \(\lamy\)
calculus, as the basis for formalizing the theory of HOMC.

\subsection{Definitions}\label{definitions}

The first part of this project focuses on formalizing the simply typed
\(\lamy\) calculus and the proof of confluence for this calculus (proof
of the Church Rosser Theorem is sometimes also referred to as proof of
confluence). The usual/informal definition of the \(\lamy\) terms and
the simple types are given below:\\
\(\ \)

\begin{Definition}[$\lamy$ types and terms]

The set of simple types \(\sigma\) is built up inductively form the
\(\mathsf{o}\) constant and the arrow type \(\to\).\\
Let \(Var\) be a countably infinite set of atoms in the definition of
the set of \(\lamy\) terms \(M\): \label{Definition:lamyTrms}

\begin{center}
$\begin{aligned}
\sigma ::=&\ \mathsf{o}\ |\ \sigma \to \sigma \\
M ::=&\ x\ |\ MM\ |\ \lambda x.M\ |\ Y_\sigma \text{ where }x \in Var
\end{aligned}$
\end{center}

\end{Definition}

The \(\lamy\) calculus differs from the simply typed
\(\lambda\)-calculus only in the addition of the \(Y\) constant family,
indexed at every simple type \(\sigma\), where the (simple) type of a
\(Y_A\) constant (indexed with the type \(A\)) is \((A \to A) \to A\).
The usual definition of \(\beta\)-reduction is then augmented with the
\((Y)\) rule (this is the typed version of the rule):

\begin{center}
  \vskip 1em
  \AxiomC{$\Gamma \vdash M : \sigma \to \sigma$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$\Gamma \vdash Y_\sigma M \red M (Y_\sigma M) : \sigma$}
  \DisplayProof
  \vskip 1em
\end{center}

In essence, the \(Y\) rule allows (some) well-typed recursive
definitions over simply typed \(\lambda\)-terms.\\
\(\ \)

\begin{Example}

Take for example the term \(\lambda x.x\), commonly referred to as the
\emph{identity}. The \emph{identity} term can be given a type
\(\sigma \to \sigma\) for any simple type \(\sigma\). We can therefore
perform the following (well-typed) reduction in the \(\lamy\) calculus:

\begin{center}
$Y_\sigma (\lambda x.x) \red (\lambda x.x)(Y_\sigma (\lambda x.x))$
\end{center}

\end{Example}

The typed version of the rule illustrates the restricted version of
recursion clearly, since a recursive ``\(Y\)-reduction'' will only occur
if the term \(M\) in \(Y_\sigma M\) has the matching type
\(\sigma \to \sigma\) (to \(Y_\sigma\)'s type
\((\sigma \to \sigma) \to \sigma\)), as in the example above.

\subsection{Church-Rosser Theorem}\label{church-rosser-theorem}

\label{cr-def}

The Church-Rosser Theorem states that the \(\beta\)-reduction of the
\(\lambda\)-calculus is confluent, that is, the reflexive-transitive
closure of the \(\beta\)-reduction has the \emph{diamond property}, i.e.
\(\dip(\red^*)\), where:

\begin{Definition}[$\dip(R)$]

A binary relation \(R\) has the \emph{diamond property}, \(\dip(R)\),
iff

\begin{center}
$\forall a, b, c.\ aRb \land aRc \implies \exists d.\ bRd \land cRd$
\end{center}

\end{Definition}

The proof of confluence of \(\red\), the \(\beta Y\)-reduction defined
as the standard \(\beta\)-reduction with the addition of the
aforementioned \((Y)\) rule, follows a variation of the Tait-Martin-LÃ¶f
Proof originally described in Takahashi
(\protect\hyperlink{ref-takahashi95}{1995}) (specifically using the
notes by R. Pollack (\protect\hyperlink{ref-pollack95}{1995})). To show
why following this proof over the traditional proof is beneficial, we
first give a high level overview of how the usual proof proceeds.

\subsubsection{Overview}\label{overview}

In the traditional proof of the Church Rosser theorem, we define a new
reduction relation, called the \emph{parallel} \(\beta\)-reduction
(\(\gg\)), which, unlike the ``plain'' \(\beta\)-reduction satisfies the
\emph{diamond property} (note that we are talking about the ``single
step'' \(\beta\)-reduction and not the reflexive transitive closure).
Once we prove the \emph{diamond property} for \(\gg\), the proof of
\(\dip(\gg^*)\) follows easily. The reason why we prove \(\dip(\gg)\) in
the first place is because the reflexive-transitive closure of \(\gg\)
coincides with the reflexive transitive closure of \(\red\) and it is
much easier to prove \(\dip(\gg)\) than trying to prove \(\dip(\red^*)\)
directly. The usual proof of the \emph{diamond property} for \(\gg\)
involves a double induction on the shape of the two parallel
\(\beta\)-reductions from \(M\) to \(P\) and \(Q\), where we try to show
that the following diamond always exists, that is, given any reductions
\(M \gg P\) and \(M \gg Q\), there is some \(M'\) s.t. \(P \gg M'\) and
\(Q \gg M'\):

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,semithick]
  \tikzstyle{every state}=[fill=none,draw=none,text=black]

  \node[state] (A)                                     {$M$};
  \node[state] (B) [below left= 1.3cm and 1.3cm of A]  {$P$};
  \node[state] (C) [below right= 1.3cm and 1.3cm of A] {$Q$};
  \node[state] (D) [below= 3cm of A]                   {$M'$};
 
  \path (A) edge [left]          node [pos=0.4] {$\gg$} (B)
            edge                 node [pos=0.59]           {$\gg$} (C)
        (B) edge [left, dashed]  node           {$\gg$} (D)
        (C) edge [right, dashed] node           {$\gg$} (D);
\end{tikzpicture}
\end{center}
\caption{The diamond property of $\gg$, visualized}
\end{figure}

The Takahashi (\protect\hyperlink{ref-takahashi95}{1995}) proof
simplifies this proof by eliminating the need to do simultaneous
induction on the \(M \gg P\) and \(M \gg Q\) reductions. This is done by
introducing another reduction, referred to as the \emph{maximal
parallel} \(\beta\)-reduction (\(\ggg\)). The idea of using \(\ggg\) is
to show that for every term \(M\) there is a reduct term \(M_{max}\)
s.t. \(M \ggg M_{max}\) and that any \(M'\), s.t. \(M \gg M'\), also
reduces to \(M_{max}\). We can then separate the ``diamond'' diagram
above into two instances of the following triangle, where \(M'\) from
the previous diagram is \(M_{max}\):

\begin{figure}
\begin{center}
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,semithick]
  \tikzstyle{every state}=[fill=none,draw=none,text=black]

  \node[state] (A)                                    {$M$};
  \node[state] (B) [below left= 1.3cm and 1.3cm of A] {$M'$};
  \node[state] (D) [below= 3cm of A]                  {$M_{max}$};
 
  \path (A) edge [left]         node [pos=0.4] {$\gg$}  (B)
            edge                node           {$\ggg$} (D)
        (B) edge [left, dashed] node           {$\gg$}  (D);
\end{tikzpicture}
\end{center}
\caption{The proof of $\dip(\gg)$ is split into two instances of this triangle}
\label{figure:gggTriangle}
\end{figure}

\newpage

\subsubsection{\texorpdfstring{Parallel
\(\beta Y\)-reduction}{Parallel \textbackslash{}beta Y-reduction}}\label{parallel-beta-y-reduction}

Having described the high-level overview of the classical proof and the
reason for following the Takahashi
(\protect\hyperlink{ref-takahashi95}{1995}) proof, we now present some
of the major lemmas in more detail.\\
Firstly, we give the definition of \emph{parallel \(\beta Y\)-reduction}
\(\gg\) formulated for the terms of the \(\lamy\) calculus, which allows
simultaneous reduction of multiple parts of a term:\\

\begin{Definition}[$\gg$]

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$x \gg x$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(refl_Y)$}
  \UnaryInfC{$Y_\sigma \gg Y_\sigma$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \gg M'$}
  \AxiomC{$N \gg N'$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$MN \gg M'N'$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$M \gg M'$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\lambda x. M \gg \lambda x. M'$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \gg M'$}
  \AxiomC{$N \gg N'$}
  \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda x. M)N \gg M'[N'/x]$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \gg M'$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$Y_\sigma M \gg M' (Y_\sigma M')$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

The first difference between the normal \(\beta\)-reduction and
\emph{parallel} \(\beta Y\)-reduction is the \((refl)/(refl_Y)\) rule,
where \(x \gg x\), for example, is a valid reduction, but we have
\(x \nRightarrow_Y x\) for the normal \(\beta Y\)-reduction
(\(x \red^* x\) is valid, since \(\red^*\) is the reflexive transitive
closure of \(\red\)). THe addition of these two rules then allows us to
derive the general reflexivity rule \((refl^*): \forall M.\ M \gg M\)
(see \cref{Lemma:reflM}).

\begin{Example}

\label{Example:ggVsGgg} Another example where the two reductions differ
is the simultaneous reduction of multiple sub-terms. \emph{Parallel}
\(\beta\)-reduction, unlike \(\red\), allows the reduction of the term
\(((\lambda xy.x)z)(\lambda x.x)y\) to \((\lambda y.z)y\), by
simultaneously reducing the two sub-terms \((\lambda xy.x)z\) and
\((\lambda x.x)y\) to \(\lambda y.z\) and \(y\) respectively:\\

\begin{center}
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(refl^*)$}
  \UnaryInfC{$\lambda xy.x \gg \lambda xy.x$}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$z \gg z$}
  \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda xy.x)z \gg \lambda y.z$}
  \AxiomC{}
  \LeftLabel{$(refl^*)$}
  \UnaryInfC{$\lambda x.x \gg \lambda x.x$}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$y \gg y$}
  \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda x.x)y \gg y$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$((\lambda xy.x)z)(\lambda x.x)y \gg (\lambda y.z)y$}
  \DisplayProof
  \vskip 1.5em
\end{center}

If we try to construct a similar tree for \(\beta\)-reduction, we
quickly discover that the only two rules we can use are \((red_L)\) or
\((red_R)\). We can thus only perform the right-side or the left side
reduction of the two sub-terms, but not both.

\end{Example}

Now that we have described the intuition behind the \emph{parallel}
\(\beta\)-reduction, we proceed to define the \emph{maximum parallel}
\(\beta\)-reduction \(\ggg\), which contracts all redexes in a given
term with a single step:

\begin{Definition}[$\ggg$]

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$x \ggg x$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(refl_Y)$}
  \UnaryInfC{$Y_\sigma \ggg Y_\sigma$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \ggg M'$}
  \AxiomC{$N \ggg N'$}
  \LeftLabel{$(app)$}
  \RightLabel{($M$ is not a $\lambda$ or $Y$)}
  \BinaryInfC{$MN \ggg M'N'$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$M \ggg M'$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\lambda x. M \ggg \lambda x. M'$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \ggg M'$}
  \AxiomC{$N \ggg N'$}    \LeftLabel{$(\beta)$}
  \BinaryInfC{$(\lambda x. M)N \ggg M'[N'/x]$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$M \ggg M'$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$Y_\sigma M \ggg M' (Y_\sigma M')$}
  \DisplayProof
\end{center}

\end{Definition}

This relation only differs from \(\gg\) in the \((app)\) rule, which can
only be applied if \(M\) is not a \(\lambda\) or \(Y\) term.

\begin{Example}

To demonstrate the difference between \(\gg\) and \(\ggg\), we take a
look at the term \((\lambda xy.x)((\lambda x.x)z)\). Whilst
\((\lambda xy.x)((\lambda x.x)z) \gg (\lambda xy.x)z\) or
\((\lambda xy.x)((\lambda x.x)z) \gg \lambda y.z\) (amongst others) are
valid reductions, the reduction
\((\lambda xy.x)((\lambda x.x)z) \ggg (\lambda xy.x)z\) is not.\\
To see why this is the case, we observe that the last rule applied in
the derivation tree must have been the \((app)\) rule, since we see that
a reduction on the sub-term \((\lambda x.x)z \ggg z\) occurs:

\begin{center}
  \AxiomC{$\vdots$}
  \UnaryInfC{$\lambda xy.x \ggg \lambda xy.x$}

  \AxiomC{$\vdots$}
  \UnaryInfC{$(\lambda x.x)z \ggg z$}
  \LeftLabel{$(app)$}
  \RightLabel{($\lambda xy.x$ is not a $\lambda$ or $Y$)}
  \BinaryInfC{$(\lambda xy.x)(\lambda x.x)z \ggg (\lambda xy.x)z$}
  \DisplayProof
\end{center}

However, this clearly could not happen, because \(\lambda xy.x\) is in
fact a \(\lambda\)-term.

\end{Example}

To prove \(\dip(\gg)\), we first show that there always exists a term
\(M_{max}\) for every term \(M\), where \(M \ggg M_{max}\) is the
maximal parallel reduction which contracts all redexes in \(M\):

\begin{Lemma}

\label{Lemma:maxEx} \(\forall M.\ \exists M_{max}.\ M \ggg M_{max}\)

\begin{proof}

By induction on M.

\end{proof}

\end{Lemma}

Finally, we show that any parallel reduction \(M \gg M'\) can be
``closed'' by reducing to the term \(M_{max}\) where all redexes have
been contracted (as seen in \cref{figure:gggTriangle}):

\begin{Lemma}

\label{Lemma:maxClose}
\(\forall M, M', M_{max}.\ M \ggg M_{max} \land M \gg M' \implies M' \gg M_{max}\)

\begin{proof}

Omitted. Can be found on p.~8 of the R. Pollack
(\protect\hyperlink{ref-pollack95}{1995}) notes.

\end{proof}

\end{Lemma}

\begin{Lemma}

\(\dip(\gg)\)

\begin{proof}

We can now prove \(\dip(\gg)\) by simply applying \cref{Lemma:maxClose}
twice, namely for any term \(M\) there is an \(M_{max}\) s.t.
\(M \ggg M_{max}\) (by \cref{Lemma:maxEx}) and for any \(M', M''\) where
\(M \gg M'\) and \(M \gg M''\), it follows by two applications of
\cref{Lemma:maxClose} that \(M' \gg M_{max}\) and \(M'' \gg M_{max}\).

\end{proof}

\end{Lemma}

\newpage

\section{Intersection types}\label{intersection-types}

\label{itypesIntro}

For the formalization of intersection types, we initially chose a
\emph{strict} intersection-type system, presented in the Bakel
(\protect\hyperlink{ref-bakel}{2003}) notes. Intersection types,
classically presented by Barendregt, Dekkers, and Statman
(\protect\hyperlink{ref-barendregt13}{2013}) as \(\lambda_\cap^{BCD}\),
extend simple types by adding a conjunction to the definition of types:

\begin{Definition}[$\lambda_\cap^{BCD}$ types]

\begin{center}
$\mathcal{T} ::= \phi\ |\ \mathcal{T} \leadsto \mathcal{T}\ |\ \mathcal{T} \cap \mathcal{T}$
\end{center}

\end{Definition}

We restrict ourselves to a version of intersection types often called
\emph{strict} intersection types. \emph{Strict} intersection types are a
restriction on \(\lambda_\cap^{BCD}\) types, where an intersection of
types can only appear on the left side of an ``arrow'' type:

\begin{Definition}[Strict intersection types]

\label{Definition:itypes} In the definition below, \(\phi\) is a
constant (analogous to the constant \(\mathsf{o}\), introduced for the
simple types in \cref{Definition:lamyTrms}).

\begin{center}
$\begin{aligned}
\mathcal{T}_s &::= \phi\ |\ \mathcal{T} \leadsto \mathcal{T}_s \\ 
\mathcal{T} &::= (\mathcal{T}_s \cap\hdots\cap \mathcal{T}_s)
\end{aligned}$
\end{center}

\end{Definition}

The following conventions for intersection types are adopted throughout
this section; \(\omega\) stands for the empty intersection and we write
\(\taui\) for the type \(\tau_1 \cap\hdots\cap \tau_n\). We also define
a subtype relation \(\subseteq\) for intersection types, which
intuitively captures the idea of one intersection of types being a
subset of another, where we think of \(\tau_1 \cap \hdots \cap \tau_i\)
as a finite set \(\{\tau_1, \hdots , \tau_i\}\), wherein \(\subseteq\)
for intersection types roughly corresponds to subset inclusion e.g.
\(\tau \subseteq \tau \cap \psi\) because
\(\{\tau\} \subseteq \{\tau, \psi\}\).

\(\ \)

\begin{Remark}

The reason for defining the subset relation in this way, rather than
taking the usual view of \(\tau \cap \phi \leq \tau\), was due the
implementation of intersection types in Agda. Since intersection types
\(\mathcal{T}\) ended up being defined as lists of strict types
\(\mathcal{T}_s\) (the definition of lists in Agda included the notion
of list inclusion \(\in\) and by extension the \(\subseteq\) relation),
the above convention seemed more natural.

\end{Remark}

The formal definition of this relation is given below:

\begin{Definition}[$\subseteq$]

\label{Definition:subseteqOrig} This relation is the least pre-order on
intersection types s.t.:

\begin{center}
$\begin{aligned}
\forall\ i \in \underline{n}.\ \ \tau_i \subseteq& \taui \\ 
\forall\ i \in \underline{n}.\ \ \tau_i \subseteq \tau \implies& \taui \subseteq \tau \\
\rho \subseteq \psi \land \tau \subseteq \mu \implies& \psi \leadsto \tau \subseteq \rho \leadsto \mu\\
\end{aligned}$
\end{center}

(This relation is equivalent the \(\leq\) relation, defined in R.
Pollack (\protect\hyperlink{ref-pollack95}{1995}) notes, i.e.
\(\tau \leq \psi = \psi \subseteq \tau\).)

\end{Definition}

In this presentation, \(\lamy\) terms are typed with the strict types
\(\mathcal{T}_s\) only. Much like the simple types, presented in the
previous sections, an intersection-typing judgment is a triple
\(\Gamma, M, \tau\), written as \(\Gamma \vDash M : \tau\), where
\(\Gamma\) is the intersection-type context, similar in construction to
the simple typing context, \(M\) is a \(\lamy\) term and \(\tau\) is a
strict intersection type \(\mathcal{T}_s\).\\
The definition of the intersection-typing system, like the \(\subseteq\)
relation, has also been adapted from the typing system found in the R.
Pollack (\protect\hyperlink{ref-pollack95}{1995}) notes, by adding the
typing rule for the \(Y\) constants:

\begin{Definition}[Intersection-type assignment]

\(\ \)

\begin{center}
  \AxiomC{$x: \taui \in \Gamma$}
  \AxiomC{$\tau \subseteq \taui$}
  \LeftLabel{$(var)$}
  \BinaryInfC{$\Gamma \Vdash x : \tau$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$\Gamma \Vdash M : \taui \leadsto \tau$}
  \AxiomC{$\forall\ i \in \underline{n}.\ \ \Gamma \Vdash N : \tau_i$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$\Gamma \Vdash MN : \tau$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{$x: \taui,\Gamma \Vdash M : \tau$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\Gamma \Vdash \lambda x.M : \taui \leadsto \tau$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \RightLabel{$(j \in \underline{n})$}
  \UnaryInfC{$\Gamma \Vdash Y_\sigma : (\taui \leadsto \tau_1 \cap\hdots\cap \taui \leadsto \tau_i) \leadsto \tau_j$}
  \DisplayProof
\end{center}

\end{Definition}

The definition above also assumes that the context \(\Gamma\) is
\emph{well-formed}:

\begin{Definition}[Well-formed intersection-type context]

Assuming that \(\Gamma\) is a finite list, consisting of pairs of atoms
\(Var\) and intersection types \(\mathcal{T}\), \(\Gamma\) is a
\emph{well-formed} context iff:\(\\\)

\begin{center}
  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\wf [\ ]$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$x \not\in \dom\ \Gamma$}
  \AxiomC{$\wf \Gamma$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\wf (x: \bigcap\tau_i,\Gamma)$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

The definitions presented in this section are the initial definitions,
used as a basis for the mechanization discussed in \cref{chap:itypes}.
Due to different obstacles in the formalization of the subject
invariance proofs, these definitions were amended several times. The
reasons for these changes are also documented in \cref{chap:itypes}.

\chapter{Methodology}\label{methodology}

\label{chap:method}

The idea of formalizing a functional language in multiple theorem
provers and objectively assessing the merits and pitfalls of the
different formalizations is definitely not a new idea. The most well
known attempt to do so on a larger scale is the \(\poplm\) challenge,
proposed in the ``Mechanized Metatheory for the Masses: The \(\poplm\)
Challenge'' paper by B. E. Aydemir et al.
(\protect\hyperlink{ref-aydemir05}{2005}).\\
This paper prompted several formalizations of the benchmark typed
\(\lambda\)-calculus, proposed by the authors of the challenge, in
multiple theorem provers, such as Coq, Isabelle, Matita or Twelf.
However, to the best of our knowledge, there has been no published
follow-up work, drawing conclusions about the aptitude of different
mechanizations, which would be useful in deciding on the best
mechanization approach to take in formalizing the \(\lamy\) calculus.\\
This project definitely does not aim to answer the same question as the
original challenge, namely:

\begin{quote}
``How close are we to a world where every paper on programming languages
is accompanied by an electronic appendix with machine- checked proofs?''
(B. E. Aydemir et al. (\protect\hyperlink{ref-aydemir05}{2005}))
\end{quote}

Instead, it draws inspiration from the criteria for the ``benchmark
mechanization'', specified by the challenge, to find the best
mechanization approach as well as the right set of tools for our purpose
of effectively mechanizing the theory underpinning HOMC.

Our comparison proceeded in two stages of elimination, where the first
stage was a comparison of the two chosen mechanizations of binders for
the \(\lamy\) calculus (\cref{chap:compIsa}), namely nominal set and
locally nameless representations of binders. The main reason for the
fairly narrow selection of only two binder mechanizations was the
limited time available for this project. In order to at least partially
achieve the goal of mechanizing the intersection type theory for the
\(\lamy\) calculus, we decided to cut down the number of comparisons to
the two (seemingly) most popular binder mechanizations (chosen by word
of mouth and literature review of the field).\\
After comparing and choosing the optimal mechanization of binders,
\cref{chap:compAgda} then goes on to compare this mechanization in two
different theorem provers, Isabelle and Agda (again, only two choices
due to limited time).\\
The ``winning'' theorem prover from this round was finally used to
formalize intersection-types and the proofs of subject invariance.

\section{Evaluation criteria}\label{evaluation-criteria}

The \(\poplm\) challenge stated three main criteria for evaluating the
submitted mechanizations of the benchmark calculus:

\begin{itemize}
\tightlist
\item
  Mechanization/implementation overheads
\item
  Technology transparency
\item
  Cost of entry
\end{itemize}

This project focuses mainly on the two criteria of mechanization
overheads and technology transparency, since the focus of our comparison
is to choose the best mechanization and theorem prover to use for
implementing intersection types for the \(\lamy\) calculus, rather than
asses the viability of theorem provers in general, which was the
original goal of the \(\poplm\) challenge. These criteria are described
in greater detail below:

\subsection{Technology transparency}\label{technology-transparency}

Technology transparency, within the context of this work, is mostly
concerned with the presentation of the theory inside a proof assistant,
such as Isabelle or Agda. Whilst there is no direct measure of
transparency, per se, it is almost always immediately obvious which
presentation is more transparent, when one is presented with comparative
examples. This work makes a case for transparency, or the lack thereof,
by providing side-by-side snippets from different mechanizations of the
same theory.

\begin{Example}

\label{Example:sqareOdd} To demonstrate this, we examine the two
different (though not completely distinct) styles of writing proofs in
Isabelle, namely using apply-style proofs or the Isar proof language.
First, to demonstrate the Isar proof language and showcase the
technology transparency it affords, we examine the proof that a square
of an odd number is itself odd\footnotemark and then present the
mechanized version of this proof in Isar.

\begin{Lemma}[The square of an odd number is also odd]

\begin{proof}

By definition, if \(n\) is an odd integer, it can be expressed as

\begin{center}
$n=2k+1$
\end{center}

for some integer \(k\). Thus

\begin{center}
$\begin{aligned}
n^{2}&=(2k+1)^{2}\\
&=(2k+1)(2k+1)\\
&=4k^{2}+2k+2k+1\\
&=4k^{2}+4k+1\\
&=2(2k^{2}+2k)+1.
\end{aligned}$
\end{center}

Since \(2k^2 + 2k\) is an integer, \(n^2\) is also odd.

\end{proof}

\end{Lemma}

Now, the same (albeit slightly simplified) proof is presented using the
Isar language:

\begin{minted}[]{isabelle}
lemma sq_odd:
  fixes n and odd :: "nat â bool"
  defines "odd x â¡ âk. x = 2 * k + 1"
  assumes "odd n"
  shows "odd (n*n)"
proof -
  from assms obtain k where n_def: "n = 2 * k + 1" 
    unfolding odd_def by auto
  then have "n * n = (2 * k + 1) * (2 * k + 1)" by simp
  then have "n * n = (4 * k * k) + (4 * k) + 1" by simp
  hence     "n * n = 2 * ((2 * k * k) + (2 * k)) + 1" by simp
  thus "odd (n * n)" unfolding odd_def by blast
qed
\end{minted}

Clearly, this mechanized proof reads much like the rigorous paper proof
that precedes it.\\
When the same proof is presented using the apply-style proof in
Isabelle, it is immediately apparent that it is much less transparent,
as we obfuscate the natural flow of the informal proof, hiding most of
the reasoning in automation (the last line \texttt{by simp+}):

\begin{minted}[]{isabelle}
lemma sq_odd: "â n ::nat. (âk. n = 2 * k + 1) â¹ âk. n*n = 2 * k + 1"
apply (erule_tac P="Î»k. n = 2 * k + 1" in exE)
apply (rule_tac x="(2 * x * x) + (2 * x)" in exI)
apply (rule_tac s="(2 * x + 1) * (2 * x + 1)" in subst)
by simp+
\end{minted}

While this example might be slightly exaggerated, it clearly
demonstrates the relative lack of human readability, comapred to the
Isar proof.\\
\(\ \)

\begin{Note}

The whole apply-style script can in fact just be substituted by the
single line command: \texttt{by (auto, presburger)})

\end{Note}

\end{Example}

\footnotetext{The informal proof was copied from \url{https://en.wikipedia.org/wiki/Direct_proof}}
The example given above demonstrates, that transparency is a comparative
measure, as it depends directly on some point of reference. As is also
apparent from the example, transparency can often come at a cost of
brevity. The reason why apply-style proofs exist and are used, even
though Isar proofs are generally regarded as the better alternative, is
the fact that they can be significantly faster to write, as they are a
lot less verbose. Of course, relying more on automation, these proofs
naturally tend to be harder to follow. However, much like in an informal
setting, where we rarely write proofs in a completely rigorous detail,
especially those which are ``uninteresting'' from point of the whole
theory, so the different styles of proofs are used for different proofs.
The short, ``boring'' ones are often written using apply-style scripts,
whereas longer more interesting lemmas use the Isar language, to make
the reasoning intuitive, i.e.~transparent.\\
This trade-off brings us to the second criterion, namely the
mechanization overheads.

\subsection{Mechanization/implementation
overheads}\label{mechanizationimplementation-overheads}

\label{mechOverheads}

When talking about mechanization overheads, we usually mean the
additional theory needed to translate the informal definitions we reason
about on paper into the fully formal setting of a theorem prover.

\begin{Example}

\label{Example:listITypes} To demonstrate what we mean by this, we will
take the definition of intersection types and its implmentation in Agda
(further discussed in \cref{itypesAgda}). Taking the
\cref{Definition:itypes} as a starting point, namely defining
intersection types as:

\begin{center}
$\begin{aligned}
\mathcal{T}_s &::= \phi\ |\ \mathcal{T} \leadsto \mathcal{T}_s \\ 
\mathcal{T} &::= (\mathcal{T}_s \cap\hdots\cap \mathcal{T}_s)
\end{aligned}$
\end{center}

we translate the strict types \(\mathcal{T}_s\) to a definition in Agda
in a straightforward way, since we only need to
translate\(\mathcal{T}_s\) into a GADT (generalized algebraic datatype)
definition:

\begin{minted}[escapeinside=||]{agda}
data T|$_\texttt{s}$| where
  Ï : T|$_\texttt{s}$|
  _~>_ : (Ï : T) -> (Ï : T|$_\texttt{s}$|) -> T|$_\texttt{s}$|
\end{minted}

\(\ \)

\begin{Remark}

The definition above is perhaps more obvious, when \(\mathcal{T}_s\) is
presented inductively as:

\begin{center}
  \AxiomC{}
  \UnaryInfC{$\phi \in \mathcal{T}_s$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\tau \in \mathcal{T}$}
  \AxiomC{$\psi \in \mathcal{T}_s$}
  \BinaryInfC{$\tau \leadsto \psi \in \mathcal{T}_s$}
  \DisplayProof
\end{center}

\end{Remark}

The informal definition of \(\mathcal{T}\), however, is slightly more
complicated, since intuitively,
\(\mathcal{T}_s \cap\hdots\cap \mathcal{T}_s\) represents a finite set
of elements of \(\mathcal{T}_s\). We can describe the set of
intersection terms \(\mathcal{T}\) with the following inductive
definition:

\begin{center}
  \AxiomC{$\{\tau_1, \hdots, \tau_n\} \subset \mathcal{T}_s$}
  \UnaryInfC{$\tau_1 \cap\hdots\cap \tau_n \in \mathcal{T}$}
  \DisplayProof
\end{center}

In order to encode this definition in Agda, we will have to rely on some
definition of a finite set (since the rule above assumes knowledge of
finite sets and the subset relation \(\subset\) in its precondition).\\
Whilst the notion of a finite set is so trivial, we rarely bother
axiomatizing it, Agda does not actually know about finite sets by
default and its standard library only includes the definition of finite
sets of natural numbers. We can instead use lists to ``simulate'' finite
sets, as they are similar in many regards, i.e.~the Agda implementation
of lists includes the notion of subset inclusion for lists, so that one
can write a proof of \([1,2] \subseteq [2,2,1]\) easily. Thus, for
\(\mathcal{T}\), we get:

\begin{minted}[escapeinside=||]{agda}
data T where
  â© : List T|$_\texttt{s}$| -> T
\end{minted}

Whilst this definition is now largely equivalent to the informal
inductive definition, we have lost quite a bit of transparency as a
result. Consider the strict type \(\tau \cap \psi \leadsto \tau\), is
written as \texttt{â© (Ï â· Ï â· [])  \textasciitilde > Ï} in Agda. We can
improve things somewhat by getting rid of the pointless constructor
\texttt{â©} and merging the two definitions of \(\mathcal{T}\) and
\(\mathcal{T}_s\) into a single definition, namely:

\begin{minted}[escapeinside=||]{agda}
data T|$_\texttt{s}$| where
  Ï : T|$_\texttt{s}$|
  _~>_ : (Ï : List T|$_\texttt{s}$|) -> (Ï : T|$_\texttt{s}$|) -> T|$_\texttt{s}$|
\end{minted}

\(\ \)

\begin{Remark}

This definition now corresponds to the merging of the two previously
given inductive definitions of \(\mathcal{T}\) and \(\mathcal{T}_s\):

\begin{center}
  \AxiomC{}
  \UnaryInfC{$\phi \in \mathcal{T}_s$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\{\tau_1, \hdots, \tau_n\} \subset \mathcal{T}_s$}
  \AxiomC{$\psi \in \mathcal{T}_s$}
  \BinaryInfC{$\tau_1 \cap\hdots\cap \tau_n \leadsto \psi \in \mathcal{T}_s$}
  \DisplayProof
\end{center}

\end{Remark}

Now, \(\tau \cap \psi \leadsto \tau\), corresponds to the Agda term
\texttt{(Ï â· Ï â· [])  \textasciitilde > Ï}, which is still not ideal. We
can, however, define some simple sugar notation:

\begin{minted}[escapeinside=||]{agda}
_â©_ : T|$_\texttt{s}$| -> T|$_\texttt{s}$| -> List T|$_\texttt{s}$|
Ï â© Ï = Ï â· Ï â· []
\end{minted}

Thus, we finally get the Agda term \texttt{Ï â© Ï \textasciitilde > Ï}
which now clearly corresponds to \(\tau\ \cap\ \psi \leadsto \tau\).

\end{Example}

As the above example clearly shows, the first/simplest measure of the
amount of implementation overheads, is simply the length of the
code/proof scrips, defining the terms and lemmas of a theory. Whilst the
length of code might provide an indication of the possible level of
implementation overheads, it is important to keep in mind, that brevity
of code can often also depend on the level of transparency, as evidenced
by both \cref{Example:sqareOdd} and the one above, where the shorter
code turned out to also be the less transparent one. Depending on the
priorities, we therefore often sacrifice either transparency for brevity
or vice versa (which can greatly impact this simple metric for
overheads).\\
Therefore, instead of simply looking at the length of the produced
document, we also compare the number of lemmas, disregarding the length
of each one. Even though this measure also carries disadvantages (one
could, for example, in-line the whole Church Rosser proof into one giant
lemma) it is less sensitive in regard to transparency.

Another aspect which ties into both transparency and mechanization
overheads is the level of automation. As was demonstrated by
\cref{Example:sqareOdd}, wherein the lemma could in fact be proved
automatically with almost no user input, having low implementation
overheads (in proofs) is often tied to the level of automation the tool
provides.\\
More concretely, a tool with good automation will include a standard
library of common definitions and theorems, so that the user does not
have to re-define and re-prove basic mathematical object and properties
and instead can focus on the specific theory she/he wants to
implement.\\
This is indeed largely the reason why we used Isabelle along with the
\href{http://www.inf.kcl.ac.uk/staff/urbanc/Nominal/}{nominal sets
library}, maintained by Christian Urban, where the theory was
conveniently hidden away and managed for us by Isabelle's automatic
provers, so that our mechanization overheads were minimal. However,
there were several caveats to this, which we discuss in the next
chapter.\\
On the other hand, the choice of locally nameless encoding, as opposed
to using pure de Bruijn indices, was motivated by the claim that locally
nameless encoding largely mitigates the disadvantages of de Bruijn
indices especially when it comes to technology transparency. The LN
encoding is also a lot more bare-bones than the nominal set theory (if
there was not library and one had to formalize the theory from scratch),
carrying relatively manageable overheads.\\
In order to keep our comparison balanced, we often didn't leverage
Isabelle's automation to it's fullest, choosing instead to keep some
lemmas (especially in the nominal implementation) deliberately verbose,
so as to keep them both more transparent and easier to compare with the
locally nameless versions. Another reason for this was the comparison
between Isabelle and Agda, which doesn't include as much automation.

\chapter{Nominal vs.~Locally nameless}\label{comp-isa}

\label{chap:compIsa}

This chapter looks at the two different mechanizations of the \(\lamy\)
calculus, introduced in the previous chapter, namely an implementation
of the calculus using nominal sets and a locally nameless (LN)
mechanization. Having presented the two approaches to formalizing
binders in \cref{binders}, this chapter explores the consequences of
choosing either mechanization, especially in terms of technology
transparency and overheads introduced as a result of the chosen
mechanization.

Whilst we found that the nominal version of the definitions and proofs
turned out to be more transparent than the locally nameless
mechanization, there were some large overheads associated with the
implementation of certain features in the \(\lamy\) calculus. The LN
mechanization, on the other hand, carried a small but consistent level
of overhead throughout the formalization, proving that it was indeed a
good compromise between implementation overheads and transparency.

\section{Overview}\label{overview-1}

We chose the length of the implemented theory files as a simple measure
of implementation overheads. As expected, the Locally nameless version
of the calculus (1143 lines) was about 50\% longer than the Nominal
encoding (723 lines). However, this measure is not always ideal (due to
the reasons outlined in \cref{mechOverheads}), and we therefore also
present the comparison between the two versions in terms of the
individual definitions and lemmas that correspond to each other in the
two mechanizations and the informal definitions/lemmas:

\newcommand{\lem}[1]{\bf{lemma}\ \it{#1}}
\newcommand{\fun}[1]{\bf{fun}\ \it{#1}}
\newcommand{\nfun}[1]{\bf{nominal\_function}\ \it{#1}}
\newcommand{\dat}[1]{\bf{datatype}\ \it{#1}}
\newcommand{\ndat}[1]{\bf{nominal\_datatype}\ \it{#1}}
\newcommand{\induct}[1]{\bf{inductive}\ \it{#1}}


\begin{longtable}[]{@{}lll@{}}
\toprule
\begin{minipage}[b]{0.21\columnwidth}\raggedright\strut
Informal\strut
\end{minipage} & \begin{minipage}[b]{0.35\columnwidth}\raggedright\strut
Nominal\strut
\end{minipage} & \begin{minipage}[b]{0.35\columnwidth}\raggedright\strut
Locally nameless\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Definition of terms\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\ndat{trm}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\dat{ptrm} \newline \induct{trm}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
Definition of substitution\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\nfun{subst}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\fun{opn} \newline \fun{cls} \newline \fun{subst}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\cref{Lemma:maxEx} (\(\forall M.\ \exists M'.\ M \ggg M'\))\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\lem{pbeta\_max\_ex}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\lem{pbeta\_max\_ex} \newline \lem{fv\_opn\_cls\_id2}
\newline \lem{pbeta\_max\_cls}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\cref{Lemma:maxClose}
(\(\forall M, M', M''.\ M \ggg M'' \land M \gg M' \implies M' \gg M''\))\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\lem{pbeta\_max\_closes\_pbeta} \newline \lem{pbeta\_cases\_2}
\newline \lem{Lem2\_5\_1} \newline \lem{pbeta\_lam\_case\_ex}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\lem{pbeta\_max\_closes\_pbeta} \newline \lem{Lem2\_5\_1opn}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.21\columnwidth}\raggedright\strut
\cref{Theorem:subRedSimp} (Subject reduction for \(\red^*\))\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\lem{beta\_Y\_typ} \newline \lem{subst\_typ}
\newline \lem{wt\_terms\_impl\_wf\_ctxt}
\newline \lem{wt\_terms\_cases\_2}\strut
\end{minipage} & \begin{minipage}[t]{0.35\columnwidth}\raggedright\strut
\lem{beta\_Y\_typ} \newline \lem{opn\_typ}
\newline \lem{wt\_terms\_impl\_wf\_ctxt}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

The table above lists the major lemmas discussed throughout this thesis,
along with the names of these lemmas in the concrete implementations
(these can be found in the Appendix), as well as the additional lemmas
the proofs of these depend on. For example, the lemma
\emph{pbeta\_max\_ex} depends on \emph{fv\_opn\_cls\_id2} and
\emph{pbeta\_max\_cls} (which may themselves depend on other smaller
lemmas). Overall, the mechanization using nominal sets includes 33
lemmas, whereas the locally nameless has 71 individual lemmas. The fact
that whilst the LN mechanization includes more than twice as many lemmas
as the nominal formalization, its only roughly 50\% longer, meaning that
many of these lemmas are short simple proofs, which supports our
assertion that using the locally nameless representation of binders
carries larger overhead, but keeps the difficulty of proving these
additional lemmas low.

The rest of this chapter provides an overview of some of the technical
points of the \(\lamy\) calculus mechanization which highlight the
differences between the two mechanizations. However, we conclude that on
the whole, neither mechanization proved to be significantly better than
the other.\\
This is especially true when it comes to proofs in both mechanizations.
As the code printout in the Appendix clearly shows, both mechanizations
have the same structure and largely the same syntax and formulation of
lemmas.\\
Additionally, when taking a finer grained look at the length of code by
section, rather than as a whole, the lengths of the main lemmas in both
mechanizations are much closer, as the overheads of the locally nameless
encoding occur mainly in the definitions of terms and
substitution/open/close operations:

\begin{longtable}[]{@{}lcc@{}}
\toprule
\(\ \) & Nominal & Locally nameless\tabularnewline
\midrule
\endhead
Definition of \(\lamy\) terms & 15 & 11\tabularnewline
Definition of well formed terms & - & 15\tabularnewline
Definition of the open operation & - & 18\tabularnewline
Definition of substitution & 56 & 124\tabularnewline
Definition of the close operation & - & 86\tabularnewline
\(\beta Y\)-reduction & 17 & 25\tabularnewline
Parallel \(\beta Y\)-reduction & 17 & 27\tabularnewline
Maximal parallel \(\beta Y\)-reduction & 49 & 60\tabularnewline
\cref{Lemma:maxEx} & 24 & 107\tabularnewline
\cref{Lemma:maxClose} & 156 & 145\tabularnewline
Proof of \(\dip(\gg)\) & 18 & 18\tabularnewline
Reflexive-transitive closure of \(\beta Y\) & 116 & 231\tabularnewline
Simple-typing relation \(\vdash\) & 238 & 258\tabularnewline
Church Rosser Theorem & 12 & 12\tabularnewline
\bottomrule
\end{longtable}

Whilst the LN mechanization proved to have significantly higher
``obvious'' mechanization overheads in terms of code length, the
implementation using the nominal library proved to be more difficult to
use at certain points, due to the more complex nominal sets theory that
implicitly underpinned the mechanization. The LN mechanization proved to
be much more simple in practice, even without any library support and
the automation, which comes with using Nominal Isabelle.\\

\section{Definitions}\label{definitions-1}

We give a brief overview of the basic definitions of well-typed terms
and \(\beta\)-reduction, specific to both mechanizations.
Unsurprisingly, the main differences in these definitions involve
\(\lambda\)-binders.

\subsection{Nominal sets
representation}\label{nominal-sets-representation}

As was shown already in \cref{binders}, nominal set representation of
terms is largely identical with the informal definition, which is the
main reason why this representation was chosen. This section will
examine the implementation of \(\lamy\) calculus in Isabelle, using the
Nominal package.

The declaration of the terms and types in Nominal Isabelle is handled
using the reserved keywords \textbf{\texttt{atom\_decl}} and
\textbf{\texttt{nominal\_datatype}}, which are special versions of the
\textbf{\texttt{typedecl}} and \textbf{\texttt{datatype}} primitives,
used in the usual Isabelle/HOL session:

\begin{minted}[]{isabelle}
atom_decl name

nominal_datatype type = O | Arr type type ("_ â _")

nominal_datatype trm =
  Var name
| App trm trm
| Lam x::name t::trm  binds x in t ("Lam [_]. _" [100, 100] 100)
| Y type
\end{minted}

The special \textbf{\texttt{binds \_ in \_}} syntax in the \texttt{Lam}
constructor declares \texttt{x} to be bound in the body \texttt{t},
telling Nominal Isabelle that \texttt{Lam} terms should be identified up
to \(\alpha\)-equivalence, where a term \(\lambda x. x\) and
\(\lambda y. y\) are considered identical/equal, because both \(x\) and
\(y\) are bound in the two respective terms, and can both be
\(\alpha\)-converted to the same term, for example \(\lambda z. z\). In
fact, proving such a lemma in Nominal Isabelle is trivial:

\begin{minted}[]{isabelle}
lemma "Lam [x]. Var x = Lam [y]. Var y" by simp
\end{minted}

The special \textbf{\texttt{nominal\_datatype}} declaration also
generates definitions of free variables/freshness and other
simplification rules. (Note: These can be inspected in Isabelle, using
the \textbf{\texttt{print\_theorems}} command.)

Other definitions, such as \(\beta\)-reduction and the notion of
substitution are also unchanged with regards to the usual definition
(except for the addition of the \(Y\) case, which is trivial):

\begin{Definition}[Capture-avoiding substitution]

\begin{center}
$\begin{aligned}
x[S/y] &= \begin{cases}
S & \text{if }x \equiv y\\
x & otherwise
\end{cases}\\
(MN)[S/y] &= (M[S/y])(N[S/y])\\
x\ \sharp\ y , S \implies (\lambda x.M)[S/y] &= \lambda x.(M[S/y])\\
(Y_\sigma)[S/y] &= Y_\sigma
\end{aligned}$
\end{center}

\end{Definition}

The side-condition \(x\ \sharp\ y , S\) in the definition above can be
read as ``\(x\) is fresh in \(N\)'', namely, the atom \(x\) is not the
same as \(y\) and does not appear in \(S\), i.e.~for a \(\lambda\)-term
\(M\), we have \(x\ \sharp\ M\) iff \(x \not\in \fv(M)\).

Whilst on paper, all this definition is unchanged from the informal
presentation, there are a few caveats when it comes to actually
implementing these definitions in Isabelle, using the Nominal package.
Since this definition of substitution includes the freshness condition,
it cannot be defined using the usual structural recursion via the
\textbf{\texttt{primrec}} or \textbf{\texttt{fun}} keywords, generally
used for this purpose. Instead we have to define capture avoiding
substitution using a \textbf{\texttt{nominal\_function}} declaration:

\begin{minted}[]{isabelle}
nominal_function
  subst :: "trm â name â trm â trm"  ("_ [_ ::= _]" [90, 90, 90] 90)
where
  "(Var x)[y ::= s] = (if x = y then s else (Var x))"
| "(App t1 t2)[y ::= s] = App (t1[y ::= s]) (t2[y ::= s])"
| "atom x â¯ (y, s) â¹ (Lam [x]. t)[y ::= s] = Lam [x].(t[y ::= s])"
| "(Y t)[y ::= s] = Y t"
\end{minted}

Unlike using the usual \textbf{\texttt{fun}} declaration of a recursive
function in Isabelle, where Isabelle automatically checks the definition
for pattern completeness (for the term being pattern matched on) and
overlap. The \textbf{\texttt{fun}} definition also automatically
checks/proves termination of such recursive functions and generates
simplification rules, which can be used for equational reasoning
involving the function.\\
Unfortunately, this isn't the case for the
\textbf{\texttt{nominal\_function}} declaration, where there are several
goals (13 in the case of the \texttt{subst} definition) which the user
has to manually prove about the function definition, including proving
termination, and pattern disjointness and completeness. This turned out
to be a bit problematic, as the goals involved proving properties like:

\begin{minted}[]{idris}
âx t xa ya sa ta.
  eqvt_at subst_sumC (t, ya, sa) â¹
  eqvt_at subst_sumC (ta, ya, sa) â¹
  atom x â¯ (ya, sa) â¹ atom xa â¯ (ya, sa) â¹ 
  [[atom x]]lst. t = [[atom xa]]lst. ta â¹ 
  [[atom x]]lst. subst_sumC (t, ya, sa) = 
    [[atom xa]]lst. subst_sumC (ta, ya, sa)
\end{minted}

Whilst most of the goals were trivial, proving cases involving
\(\lambda\)-terms involved a substantial understanding of the internal
workings of Isabelle and the Nominal package early on into the
mechanization and as a novice to using Nominal Isabelle, understanding
and proving these properties proved challenging.\\
Whilst our formalization required only a handful of other recursive
function definitions, in a different theory with significantly more
function definitions, proving such goals from scratch would prove a
challenge to a Nominal Isabelle newcomer as well as a tedious
implementation overhead.

\subsection{Locally nameless
representation}\label{locally-nameless-representation}

As we have seen, on paper at least, the definitions of terms and
capture-avoiding substitution, using nominal sets, are unchanged from
the usual informal definitions. The situation is somewhat different for
the locally nameless mechanization. Since the LN approach combines the
named and de Bruijn representations, there are two different
constructors for free and bound variables:

\subsubsection{Pre-terms}\label{pre-terms}

\begin{Definition}[LN pre-terms]

\label{Definition:pterms}

\begin{center}
$M::= x\ |\ n\ |\ MM\ |\ \lambda M\ |\ Y_\sigma \text{ where }x \in Var \text{ and } n \in \mathbb{N}$
\end{center}

\end{Definition}

Similarly to the de Bruijn presentation of binders, the \(\lambda\)-term
no longer includes a bound variable, so a named representation term
\(\lambda x.x\) becomes \(\lambda 0\) in LN. As was mentioned in
\cref{binders}, the set of pre-terms, defined in
\cref{Definition:pterms}, is a superset of \(\lamy\) terms and includes
terms which are not well formed \(\lamy\) terms.

\begin{Example}

The pre-term \(\lambda 3\) is not a well-formed \(\lamy\) term, since
the bound variable index is out of scope. In other words, there is no
corresponding (named) \(\lamy\) term to \(\lambda 3\).

\end{Example}

Since we don't want to work with terms that do not correspond to
\(\lamy\) terms, we have to introduce the notion of a \emph{well-formed
term}, which restricts the set of pre-terms to only those that
correspond to \(\lamy\) terms (i.e.~this inductive definition ensures
that there are no ``out of bounds'' indices in a given pre-term):

\begin{Definition}[Well-formed terms]

\begin{center}
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (x)$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{}
    \LeftLabel{$(Y)$}
    \UnaryInfC{$\trm (Y_\sigma)$}
    \DisplayProof
    \vskip 1.5em
    \AxiomC{$x \not\in FV(M)$}
    \AxiomC{$\trm (M^x)$}
    \LeftLabel{$(lam)$}
    \BinaryInfC{$\trm (\lambda M)$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (M)$}
    \AxiomC{$\trm (M)$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm (MN)$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Definition}

Already, we see that this formalization introduces some overheads with
respect to the informal/nominal encoding of the \(\lamy\) calculus.\\
The upside of this definition of \(\lamy\) terms becomes apparent when
we start thinking about \(\alpha\)-equivalence and capture-avoiding
substitution. Since the LN terms use de Bruijn levels for bound
variables, there is only one way to write the term \(\lambda x.x\) or
\(\lambda y.y\) as a LN term, namely \(\lambda 0\). As the
\(\alpha\)-equivalence classes of named \(\lamy\) terms collapse into a
singleton \(\alpha\)-equivalence class in a LN representation, the
notion of \(\alpha\)-equivalence becomes trivial.

As a result of using LN representation of binders, the notion of
substitution is split into two distinct operations. One operation is the
substitution of bound variables, called \emph{opening}. The other is
substitution, defined only for free variables.

\begin{Definition}[Opening and substitution]

We will usually assume that \(S\) is a well-formed LN term when proving
properties about substitution and opening. The abbreviation
\(M^N \equiv \{0 \to N\}M\) is used throughout this chapter.

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  Opening:

  \begin{center}
  $\begin{aligned}
  \{k \to S\}x &= x\\
  \{k \to S\}n &= \begin{cases}
  S & \text{if }k \equiv n\\
  n & otherwise
  \end{cases}\\
  \{k \to S\}(MN) &= (\{k \to S\}M)(\{k \to S\}N)\\
  \{k \to S\}(\lambda M) &= \lambda (\{k+1 \to S\}M)\\
  \{k \to S\}Y_\sigma &= Y_\sigma
  \end{aligned}$
  \end{center}
\item
  Substitution:

  \begin{center}
  $\begin{aligned}
  x[S/y] &= \begin{cases}
  S & \text{if }x \equiv y\\
  x & otherwise
  \end{cases}\\
  n[S/y] &= n \\
  (MN)[S/y] &= (M[S/y])(N[S/y])\\
  (\lambda M)[S/y] &= \lambda. (M[S/y])\\
  Y_\sigma[S/y] &= Y_\sigma
  \end{aligned}$
  \end{center}
\end{enumerate}

\end{Definition}

Having defined the \emph{open} operation, we turn back to the definition
of well formed terms, specifically to the \((lam)\) rule, which has the
precondition \(\trm (M^x)\). Intuitively, for the given term
\(\lambda M\), the term \(M^x\) is obtained by replacing all indices
bound to the outermost \(\lambda\) by \(x\). Then, if \(M^x\) is well
formed, so is \(\lambda M\).

\begin{Example}

For example, taking the term \(\lambda\lambda 0(z\ 1)\), we can
construct the following proof-tree, showing that the term is well
formed:

\begin{center}
    \vskip 1.5em
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (y)$}

    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (z)$}
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (x)$}

    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm (z\ x)$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm ((0(z\ x))^y)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm ((\lambda 0(z\ 1))^x)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm (\lambda\lambda 0(z\ 1))$}
    \DisplayProof
    \vskip 1.5em
\end{center}

We assumed that \(x \not\equiv y \not\equiv z\) in the proof tree above
and thus omitted the \(x \not\in \fv \hdots\) branches, as they are not
important for this example.\\
If on the other hand, we try construct a similar tree for a term which
is obviously not well formed, such as \(\lambda \lambda 2(z\ 1)\), we
get a proof tree with a branch which cannot be closed (\(\trm (2)\)):
\newpage
\(\ \)

\begin{center}
    \AxiomC{$\trm (2)$}

    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (z)$}
    \AxiomC{}
    \LeftLabel{$(fvar)$}
    \UnaryInfC{$\trm (x)$}

    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm (z\ x)$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$\trm ((2(z\ x))^y)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm ((\lambda 2(z\ 1))^x)$}
    \LeftLabel{$(lam)$}
    \UnaryInfC{$\trm (\lambda\lambda 2(z\ 1))$}
    \DisplayProof
\end{center}

\end{Example}

\subsubsection{\texorpdfstring{\(\beta\)-reduction for LN
terms}{\textbackslash{}beta-reduction for LN terms}}\label{beta-reduction-for-ln-terms}

Finally, we examine the formulation of \(\beta\)-reduction in the LN
presentation of the \(\lamy\) calculus. Since we only want to perform
\(\beta\)-reduction on valid \(\lamy\) terms, the inductive definition
of \(\beta\)-reduction in the LN mechanization now includes the
precondition that the terms appearing in the reduction are well
formed:\(\\\)

\begin{Definition}[$\beta$-reduction (LN)]

\begin{center}
    \AxiomC{$M \red M'$}
    \AxiomC{$\trm (N)$}
    \LeftLabel{$(red_L)$}
    \BinaryInfC{$MN \red M'N$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (M)$}
    \AxiomC{$N \red N'$}
    \LeftLabel{$(red_R)$}
    \BinaryInfC{$MN \red M'N$}
    \DisplayProof
    \vskip 1.5em
    \AxiomC{$ x \not\in \fv(M) \cup \fv(M')$}
    \AxiomC{$M^x \red (M')^x$}
    \LeftLabel{$(abs)$}
    \BinaryInfC{$\lambda M \red \lambda M'$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (\lambda M)$}
    \AxiomC{$\trm (N)$}
    \LeftLabel{$(\beta)$}
    \BinaryInfC{$(\lambda M)N \red M^N$}
    \DisplayProof
    \hskip 1.5em
    \AxiomC{$\trm (M)$}
    \LeftLabel{$(Y)$}
    \UnaryInfC{$Y_\sigma M \red M (Y_\sigma M)$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Definition}

As expected, the \emph{open} operation is now used instead of
substitution in the \((\beta)\) rule.\\
The \((abs)\) rule is also slightly different, also using the
\emph{open} in its precondition. Intuitively, the usual formulation of
the \((abs)\) rule states that in order to prove that \(\lambda x. M\)
reduces to \(\lambda x. M'\), we can simply ``un-bind'' \(x\) in both
\(M\) and \(M'\) and show that \(M\) reduces to \(M'\) (reasoning
bottom-up from the conclusion to the premises). Since in the usual
formulation of the \(\lambda\)-calculus, there is no distinction between
free and bound variables, this change (where \(x\) becomes free) is
implicit. In the LN presentation, however, this operation is made
explicit by opening both \(M\) and \(M'\) with some free variable \(x\)
(not appearing in either \(M\) nor \(M'\)), which replaces the bound
variables/indices (bound to the outermost \(\lambda\)) with \(x\).\\
While this definition is equivalent to the usual/informal definition,
the induction principle this definition yields may not always be
sufficient, especially in situations where we want to open up a term
with a free variable which is not only fresh in \(M\) and \(M'\), but
possibly in a wider context. We therefore followed the approach of B.
Aydemir et al. (\protect\hyperlink{ref-aydemir08}{2008}) and re-defined
the \((abs)\) rule (and other definitions involving the choice of
fresh/free variables) using \emph{cofinite quantification}:

\begin{center}
    \vskip 1.5em
    \AxiomC{$\forall x \not\in L.\ M^x \red M'^x$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\lambda M \red \lambda M'$}
    \DisplayProof
    \vskip 1.5em
\end{center}

For an example, where this formulation using \emph{cofinite
quantification} was necessary, see \cref{Lemma:opnClsSubst}).

\section{Proofs}\label{proofs}

Having described the implementations of the two binder representations
along with the definitions of capture-avoiding substitution using
nominal sets and the corresponding \emph{subsitution} and \emph{open}
operations in the LN mechanization, we come the the main part of the
comparison, namely the proof of the Church Rosser theorem. This section
examines specific instances of some of the major lemmas which form parts
of this bigger result. The general outline of the proof has been
described in \cref{cr-def}.

\subsection{\texorpdfstring{\cref{Lemma:maxEx}}{}}\label{section}

The first major result in both implementations is \cref{Lemma:maxEx},
which states that for every \(\lamy\) term \(M\), there is a term
\(M'\), s.t. \(M \ggg M'\).\\
\(\ \)

\begin{Remark}

This result is trivial for \(\gg\), as we can easily prove the derived
rule \((refl^*)\), but not for \(\ggg\):

\begin{Lemma}[$\gg$ admits $(refl^*)$]

The following rule is admissible in the deduction system \(\gg\):
\label{Lemma:reflM}

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl^*)$}
  \UnaryInfC{$M \gg M$}
  \DisplayProof
 \end{center}

\begin{proof}

By induction on \(M\).

\end{proof}

\end{Lemma}

\end{Remark}

Since \(\ggg\) restricts the use of the \((app)\) rule to terms which do
not contain a \(\lambda\) or \(Y\) as its left-most sub term,
\cref{Lemma:reflM} does not hold in \(\ggg\) for terms like
\((\lambda x.x)y\), namely, \((\lambda x.x)y \ggg (\lambda x.x)y\) is
not a valid reduction (see \cref{Example:ggVsGgg}). It is, however, not
difficult to see that such terms can simply be \(\beta\)-reduced until
all the redexes have been contracted, so that we have
\((\lambda x.x)y \ggg y\) for the term above.\\
Seen as a weaker version of \cref{Lemma:reflM}, the proof of
\cref{Lemma:maxEx}, at least in theory, should then only differ in the
case of an application, where we have do a case analysis on the left
sub-term of any given \(M\).

This is indeed the case when using the nominal mechanization, where the
proof looks like this:

\begin{minted}[xleftmargin=1em, linenos=true, escapeinside=||]{isabelle}
lemma pbeta_max_ex:
  fixes M
  shows "âM'. M >>> M'"
apply (induct M rule:trm.induct)
apply auto
apply (case_tac "not_abst S")
apply (case_tac "not_Y S")
apply auto[1]
proof goal_cases
case (1 P Q P' Q')
  then obtain |Ï| where 2: "P = Y Ï" using not_Y_ex by auto
  have "App (Y Ï) Q >>> App Q' (App (Y Ï) Q')"
  apply (rule_tac pbeta_max.Y)
  by (rule 1(2))
  thus ?case unfolding 2 by auto
next
case (2 P Q P' Q')
  thus ?case
  apply (nominal_induct P P' avoiding: Q Q' rule:pbeta_max.strong_induct)
  by auto
qed
\end{minted}

After applying induction and calling \texttt{auto}, which is Isabelle's
automatic prover that does simple term rewriting and basic proof search,
we can inspect the remaining goals at line 5, to see that the only goal
that remains is the case of \(M\) being an application, naley we have to
prove the following:

\begin{center}
$\forall S\ T\ U\ V.\ S \ggg U \implies T \ggg V \implies \exists M'.\ ST \ggg M'$
\end{center}

Lines 6 and 7 in the proof script then correspond to doing a case
analysis on \(S\) (where \(M = ST\)). We end up with 3 goals,
corresponding to \(S\) either being a \(\lambda\)-term, \(Y\)-term or
neither (shown below in reverse order):

\begin{minted}[]{idris}
 1. ... not_abst S â¹ not_Y S â¹ âM'. App S T >>> M'
 2. ... not_abst S â¹ Â¬ not_Y S â¹ âM'. App S T >>> M'
 3. ... Â¬ not_abst S â¹ âM'. App S T >>> M'
\end{minted}

The first goal is discharged by calling \texttt{auto} again (line 8),
since we can simply apply the \((app)\) rule in this instance. The two
remaining cases are discharged with the additional information that
\(S\) is either a \(\lambda\)-term or a \(Y\)-term.

So far, we have looked at the version of the proof using nominal
Isabelle and this is especially apparent in line 19, where we use the
stronger \texttt{nominal\_induct} rule, with the extra parameter
\texttt{avoiding: Q Q'}, which ensures that any new bound variables will
be sufficiently fresh with regards to \texttt{Q} and \texttt{Q'}, in
that the fresh variables won't appear in either of the terms.\\
Since bound variables are distinct in the LN representation, the
equivalent proof simply uses the usual induction rule (line 19):

\begin{minted}[xleftmargin=1em, linenos=true, escapeinside=||]{isabelle}
lemma pbeta_max_ex:
  fixes M assumes "trm M"
  shows "âM'. M >>> M'"
using assms apply (induct M rule:trm.induct)
apply auto
apply (case_tac "not_abst t1")
apply (case_tac "not_Y t1")
apply auto[1]
proof goal_cases
case (1 P Q P' Q')
  then obtain |Ï| where 2: "P = Y Ï" using not_Y_ex by auto
  have "App (Y Ï) Q >>> App Q' (App (Y Ï) Q')"
  apply (rule_tac pbeta_max.Y)
  by (rule 1(4))
  thus ?case unfolding 2 by auto
next
case (2 P Q P' Q')
  from 2(3,4,5,1,2) show ?case
  apply (induct P P' rule:pbeta_max.induct)
  by auto
next
case (3 L M)
  then obtain x where 4:"x â L âª FV M" by (meson FV_finite finite_UnI x_Ex)
  with 3 obtain M' where 5: "M^FVar x >>> M'" by auto

  have 6: "ây. y â FV M' âª FV M âª {x} â¹ M^FVar y >>> (\\x^M')^FVar y"
  unfolding opn'_def cls'_def 
  apply (subst(3) fv_opn_cls_id2[where x=x])
  using 4 apply simp
  apply (rule_tac pbeta_max_cls)
  using 5 opn'_def by (auto simp add: FV_simp)

  show ?case
  apply rule
  apply (rule_tac L="FV M' âª FV M âª {x}" in pbeta_max.abs)
  using 6 by (auto simp add: FV_finite)
qed
\end{minted}

As one can immediately see, this proof proceeds exactly in the same
fashion, as the nominal one, up to line 20. However, unlike in the
nominal version of the proof, in the LN proof, the \texttt{auto} call at
line 8 could not automatically prove the case where \(M\) is a
\(\lambda\)-term.\\
This is perhaps not too surprising, since the LN encoding is a lot more
``bare bones'', and thus there is little that would aid Isabelle's
automation. The nominal package, on the other hand, was designed to make
reasoning with binders as painless as possible, which definitely shows
in this example.

When we compare the two goals for the \(\lambda\) case in both versions
of the proof, we clearly see the differences in the treatment of
binders:

\begin{center}
$\begin{aligned}
\textbf{Nominal:}\ \ &\forall x\ M.\ \exists M'.\ M \ggg M' \implies \exists M'. \lambda x. M \ggg M'\\
\textbf{Locally nameless:}\ \ &\forall L\ M.\ \textbf{fin}\ L \implies \trm(\lambda.M) \implies (\forall x \not\in L.\ \exists M''.\ M^x \ggg M'')\\
&\implies \exists M'.\ \lambda.M \ggg M'
\end{aligned}$
\end{center}

Unlike in the nominal proof, where from \(M \ggg M'\) we get
\(\lambda x.M \ggg \lambda x.M'\) by \((abs)\) immediately, the proof of
\(\exists M'.\ \lambda.M \ggg M'\) in the LN mechanization is not as
trivial.\\
The difficulty arises with the precondition
\(\forall x \not\in L.\ M^x \ggg (M')^x\) in the LN version of the
\((abs)\) rule:

\begin{center}
    \vskip 1.5em
    \AxiomC{$\exists M'.\ \forall x \not\in L.\ M^x \ggg (M')^x$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\exists M'.\ \lambda M \ggg \lambda M'\footnotemark$}
    \DisplayProof
    \vskip 1.5em

\footnotetext{ 
While the original goal is $\exists M'.\ \lambda.M \ggg M'$, since there is only one possible ``shape'' for the right-hands side term, namely $M'$ must be a \(\lambda\)-term, we can easily rewrite this goal as $\exists M'.\ \lambda.M \ggg \lambda.M'$.
}
\end{center}

This version of the rule with the existential quantification shows the
subtle difference between the inductive hypothesis
\(\forall x \not\in L.\ \exists M'.\ M^x \ggg (M')^x\)\footnote{It can
  easily be shown that any pre-term \(M\) can be written using another
  pre-term \(N\) s.t. \(M \equiv N^x\) for some \(x\).} we have, and the
premise \(\exists M'.\ \forall x \not\in L.\ M^x \ggg (M')^x\) that we
want to show. In order to prove the latter, we assume that there is some
\(M'\) for a specific \(x \not\in L\) s.t. \(M^x \ggg (M')^x\).

At this point, we cannot proceed without re-examining the definition of
\emph{opening}, especially in that this operation lacks an inverse.
Whereas in a named representation, where bound variables are bound via
context only, LN terms have specific constructors for free and bound
variables together with an operation for turning bound variables into
free variables, namely the \emph{open} function. In this proof, however,
we need the inverse operation, wherein we turn a free variable into a
bound one. We call this the \emph{close} operation:

\begin{Definition}[Close operation]

This definition was adapted from the B. Aydemir et al.
(\protect\hyperlink{ref-aydemir08}{2008}) paper. We adopt the following
convention, writing \(\cls M \equiv \{0 \leftarrow x\}M\).

\begin{center}
$\begin{aligned}
\{k \leftarrow x\}y &= \begin{cases}
k & \text{if }x \equiv y\\
y & otherwise
\end{cases}\\
\{k \leftarrow S\}n &= n\\
\{k \leftarrow S\}(MN) &= (\{k \leftarrow S\}M)(\{k \leftarrow S\}N)\\
\{k \leftarrow S\}(\lambda M) &= \lambda (\{k+1 \leftarrow S\}M)\\
\{k \leftarrow S\}Y_\sigma &= Y_\sigma
\end{aligned}$
\end{center}

\end{Definition}

\begin{Example}

To demonstrate the close operation, take the term \(\lambda xy\).
Applying the close operation with the free variable \(x\), we get
\(\cls (\lambda xy) = \lambda 1y\). Whilst the original term might have
been well formed, the closed term, as is the case here, may not be.

\end{Example}

Intuitively, it is easy to see that closing a well formed term and then
opening it with the same free variable produces the original term,
namely \((\cls M)^x \equiv M\). This can be made even more general with
the following lemma about the relationship between the open, close and
substitution operations:

\begin{Lemma}

\(\trm(M) \implies \{k \to y\}\{k \leftarrow x\} M = M[y/x]\)
\label{Lemma:opnClsSubst}

\begin{proof}

By induction on the relation \(\trm(M)\). The rough outline of the
\((lam)\) case, which is the only non-trivial case, is shown below:

By \emph{IH}, we have
\(\forall z \not\in L.\ \{k+1 \to y\} \{k+1 \leftarrow x\} M^z = (M^z)[y/x]\).
Then:

\begin{align}
\{k \to y\} \{k \leftarrow x\} (\lambda M) = (\lambda M)[y/x] &\iff\\
\lambda(\{k+1 \to y\} \{k+1 \leftarrow x\} M) = \lambda (M[y/x])&\iff\\
\{k+1 \to y\} \{k+1 \leftarrow x\} M = M[y/x]&\iff\\
\{0 \to z\} \{k+1 \to y\} \{k+1 \leftarrow x\} M = \{0 \to z\} (M[y/x])&\iff\\
\{k+1 \to y\} \{k+1 \leftarrow x\} \{0 \to z\} M = \{0 \to z\} (M[y/x])&\iff\\
\{k+1 \to y\} \{k+1 \leftarrow x\} \{0 \to z\} M = (\{0 \to z\} M)[y/x]&
\end{align}

Starting from the goal (4.1), we expand the definitions of \emph{open},
\emph{close} and substitution for the \(\lambda\) case in (4.2). (4.3)
holds by injectivity of \(\lambda\). Then, by choosing a sufficiently
fresh \(z\) that does not appear in the given context \(L\) as well as
in neither \(\fv(M)\) nor \(\{x, y\}\), we have (4.4). We can reorder
the open and close operations in (4.5) because it can never be the case
that \(k+1 = 0\) and \(z\) is different from both \(x\) and \(y\).
Finally, (4.6) follows from the fact that we have chosen a \(z\) that
does not appear in \(M\) and is different from \(y\).\\
We can now see that
\(\{k+1 \to y\} \{k+1 \leftarrow x\} \{0 \to z\} M = (\{0 \to z\} M)[y/x]\)
is in fact the \emph{IH}
\(\{k+1 \to y\} \{k+1 \leftarrow x\} M^z = (M^z)[y/x]\).

\end{proof}

\end{Lemma}

Having defined the \emph{close} operation and shown that it satisfies
certain properties with respect to the \emph{open} operation and
substitution, we can now ``close'' the term \(M'\), with respect to the
\(x\) we fixed earlier and thus show that
\(\forall y \not\in L.\ M^y \ggg (\cls M')^y\).

\subsection{\texorpdfstring{\cref{Lemma:maxClose}}{}}\label{section-1}

While it may seem that the nominal mechanization was universally more
concise and easier to work in than the locally nameless implementation,
there were a few instances where using the nominal library turned out to
be more difficult to understand and use. One such instance, namely
defining a \textbf{\texttt{nominal\_function}}, was already discussed.
Another example can be found in the implmentation of
\cref{Lemma:maxClose}, which is stated as:

\begin{center}
$\forall M, M', M_{max}.\ M \ggg M_{max} \land M \gg M' \implies M' \gg M_{max}$
\end{center}

The proof of this lemma proceeds by induction on the relation \(\ggg\).
Here we will focus on the \((\beta)\) case, i.e.~when we have
\(M \ggg M_{max}\) by the application of \((\beta)\), first giving an
informal proof and then focusing on the implementation specifics in both
mechanizations:

\subsubsection{\texorpdfstring{\((\beta)\)
case}{(\textbackslash{}beta) case}}\label{beta-case}

We have \(M \equiv (\lambda x. P) Q\) and
\(M_{max} \equiv P_{max}[Q_{max}/x]\), and therefore
\((\lambda x. P) Q \ggg P_{max}[Q_{max}/x]\) and
\((\lambda x. P) Q \gg M'\).\\
By performing case analysis on the reduction
\((\lambda x. P) Q \gg M'\), we know that
\(M' \equiv (\lambda x. P') Q'\) or \(M' \equiv P'[Q'/x]\) for some
\(P', Q'\), since only these two trees are valid:

\begin{center}
    \AxiomC{$\vdots$}
    \UnaryInfC{$P \gg P'$}
    \LeftLabel{$(abs)$}
    \UnaryInfC{$\lambda x. P \gg \lambda x. P'$}
    \AxiomC{$\vdots$}
    \UnaryInfC{$Q \gg Q'$}
    \LeftLabel{$(app)$}
    \BinaryInfC{$(\lambda x. P) Q \gg (\lambda x. P') Q'$}
    \DisplayProof
    \ \ \ \ \ \ or\ \ \ \ \ \ 
    \AxiomC{$\vdots$}
    \UnaryInfC{$P \gg P'$}
    \AxiomC{$\vdots$}
    \UnaryInfC{$Q \gg Q'$}
    \LeftLabel{$(\beta)$}
    \BinaryInfC{$(\lambda x. P) Q \gg P'[Q'/x]$}
    \DisplayProof
    \vskip 1.5em
\end{center}

For the first case, where \(M' \equiv (\lambda x. P') Q'\), by
\emph{IH}, we have \(P' \gg P_{max}\) and \(Q' \gg Q_{max}\). Thus, we
can prove that \(M' \gg P_{max}[Q_{max}/x]\):

\begin{center}
    \vskip 1.5em
    \AxiomC{}
    \LeftLabel{$(IH)$}
    \UnaryInfC{$P' \gg P_{max}$}
    \AxiomC{}
    \LeftLabel{$(IH)$}
    \UnaryInfC{$Q' \gg Q_{max}$}
    \LeftLabel{$(\beta)$}
    \BinaryInfC{$(\lambda x. P') Q' \gg P_{max}[Q_{max}/x]$}
    \DisplayProof
    \vskip 1.5em
\end{center}

In the case where \(M' \equiv P'[Q'/x]\), we also have
\(P' \gg P_{max}\) and \(Q' \gg Q_{max}\) by \emph{IH}. The result
\(M' \gg P_{max}[Q_{max}/x]\) follows from the following auxiliary
lemma:

\begin{Lemma}[Parallel substitution]

\label{Lemma:parRed}The following rule is admissible in \(\gg\):

\begin{center}
    \vskip 1.5em
    \AxiomC{$M \gg M'$}
    \AxiomC{$N \gg N'$}
    \LeftLabel{$(||_{subst})$}
    \BinaryInfC{$M[N/x] \gg M'[N'/x]$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Lemma}

\subsubsection{Nominal implementation}\label{nominal-implementation}

The code below shows the proof of the \((\beta)\) case, described above:

\begin{minted}[xleftmargin=1em, linenos=true]{isabelle}
case (beta x Q Qmax P Pmax)
  from beta(1,7) show ?case
  apply (rule_tac pbeta_cases_2)
  apply (simp, simp)
  proof -
  case (goal2 Q' P')
    with beta have "P' â« Pmax" "Q' â« Qmax" by simp+
    thus ?case unfolding goal2 apply (rule_tac Lem2_5_1) by simp+
  next
  case (goal1 P' Q')
    with beta have ih: "P' â« Pmax" "Q' â« Qmax" by simp+
    show ?case unfolding goal1 
    apply (rule_tac pbeta.beta) using goal1 beta ih
    by simp_all
  qed
\end{minted}

There were a few quirks when implementing this proof in the nominal
mechanization, specifically in line 3, where the case analysis on the
shape of \(M'\) needed to be performed. Applying the automatically
generated \texttt{pbeta.cases} rule yielded the following goal for the
case where \(M' \equiv P'[Q'/x]\):

\begin{minted}[]{idris}
 2. âxa Q' R P'.
       [[atom x]]lst. P = [[atom xa]]lst. R â¹
       M' = P' [xa ::= Q'] â¹
       atom xa â¯ Q â¹ atom xa â¯ Q' â¹ R â« P' â¹ Q â« Q' â¹ 
       M' â« Pmax [x ::= Qmax]
\end{minted}

Obviously, this is not the desired shape of the goal, because we
obtained a weaker premise, where we have some \(R\), such that
\(\lambda x. P \equiv_\alpha \lambda xa. R\) (this is essentially what
\texttt{[[atom x]]lst. P = [[atom xa]]lst. R} states) and therefore we
get a \(P'\) where \(M' \equiv P'[Q'/xa]\). What we actually want is a
term \(P'\) s.t. \(M' \equiv P'[Q'/x]\), i.e. \(x = xa\). In order to
``force'' \(x\) and \(xa\) to actually be the same atom, we had to prove
the following ``cases'' lemma:

\begin{minted}[escapeinside=||]{isabelle}
lemma pbeta_cases_2:
  shows "atom x â¯ t â¹ App (Lam [x]. s) t â« a2 â¹ 
    (âs' t'. a2 = App (Lam [x]. s') t' â¹ atom x â¯ t' â¹ 
        s â« s' â¹ t â« t' â¹ P) â¹
    (ât' s'. a2 = s' [x ::= t'] â¹ atom x â¯ t â¹ atom x â¯ t' â¹ 
        s â« s' â¹ t â« t' â¹ P) â¹ P"
|$\texttt{\vdots}$|
\end{minted}

In the lemma above,
\texttt{(ât' s'. a2 = s' [x ::= t'] â¹ atom x â¯ t â¹ atom x â¯ t' â¹ s â« s' â¹ t â« t' â¹ P) â¹ P}
corresponds to the case with the premises we want to have, instead of
the ones we get from the ``cases'' lemma generated as part of the
definition of \(\gg\).

The proof of this lemma required proving another lemma shown below,
which required descending into nominal set theory that was previously
mostly hidden away from the mechanization (the proofs of the
\texttt{have} lemmas were omitted for brevity):

\begin{minted}[]{isabelle}
lemma "(Lam [x]. s) â« s' â¹ ât. s' = Lam [x]. t â§ s â« t"
proof (cases "(Lam [x]. s)" s' rule:pbeta.cases, simp)
  case (goal1 _ _ x')
    then have 1: "s â« ((x' â x) â M')" ...
    from goal1 have 2: "(x' â x) â s' = Lam [x]. ((x' â x) â M')" ...
    from goal1 have "atom x â¯ (Lam [x']. M')"  using fresh_in_pbeta ...
    with 2 have "s' = Lam [x]. ((x' â x) â M')" ...
    with 1 show ?case by auto
qed
\end{minted}

Clearly, the custom ``cases'' lemma was necessary from a purely
technical view, as it would be deemed too trivial to bother proving in
an informal setting. The need for such a lemma also demonstrates that
whilst the nominal package package tries to hide away the details of the
theory, every once in a while, the user has to descent into nominal set
theory, to prove certain properties about binders, not handled by the
automation.\\
For us, the nominal package thus proved to be a double edged sword, as
it initially provided a fairly low cost of entry (there was practically
no need to understand any nominal set theory to get started), but proved
to be much more challenging to understand in certain places, such as
when proving \texttt{pbeta\_cases\_2}.\\
Whilst the finial \texttt{pbeta\_cases\_2} proof turned out to be fairly
short thanks to automation of the nominal set theory, it took some time
to work out the proof outline in such a ways as to leverage Isabelle's
automation to a high degree.\\
The LN mechanization, whilst having bigger overheads in terms of extra
definitions and lemmas that had to be proven ``by hand'', was in fact a
lot more transparent as a result, as the degree of difficulty after the
initial cost of entry did not rise significantly with more complicated
lemmas.

\subsubsection{LN implementation}\label{ln-implementation}

The troublesome case analysis in the Nominal version of the proof was
much more straight forward in the LN proof. In fact, there was no need
to prove a separate lemma similar to \texttt{pbeta\_cases\_2}, since the
auto-generated \texttt{pbeta.cases} was sufficient. The only overhead in
this version of the lemma came from the use of \cref{Lemma:parRed}, in
that the lemma was first proved in it's classical formulation using
substitution, but due to the way substitution of bound terms is handled
in the LN mechanization (using the \emph{open function}), a ``helper''
lemma was proved to convert this result to one using \emph{open}:

\begin{Lemma}[Parallel open]

\label{Lemma:parOpn}The following rule is admissible in the LN version
of \(\gg\):

\begin{center}
    \vskip 1.5em
    \AxiomC{$\forall x \not\in L.\ M^x \gg M'^x$}
    \AxiomC{$N \gg N'$}
    \LeftLabel{$(||_{open})$}
    \BinaryInfC{$M^N \gg M'^{N'}$}
    \DisplayProof
    \vskip 1.5em
\end{center}

\end{Lemma}

The reason why \cref{Lemma:parOpn} wasn't proved directly is partially
due to the order of implementation of the two mechanizations of the
\(\lamy\) calculus. Since the nominal version, along with all the proofs
was carried out first, the LN version of the calculus ended up being
more of a port of the nominal theory into a locally nameless setting.\\
The LN mechanization, being a port of the nominal theory, has both
advantages and disadvantages. On the one hand, it ensures a greater
consistency between the two theories and easier direct comparison of
lemmas, but on the other hand, it meant that certain lemmas could have
been made shorter and more ``tailored'' to the LN mechanization.

\chapter{Isabelle vs.~Agda}\label{comp-agda}

\label{chap:compAgda}

Having previously looked at two approaches to mechanizing binders in
Isabelle and having concluded that there were only minor differences
between the nominal and LN approaches, we proceeded to the next round of
our comparison, by implementing the locally nameless version of the
\(\lamy\) calculus along with proofs of confluence in Agda.\\
Whilst using Nominal sets in Isabelle turned out to be slightly more
transparent and shorter, the big disadvantage, especially for Agda, was
the fact that the nominal set theory would have been much more tedious
to use, if we had to, say, implement it from scratch. Whilst Nominal
Isabelle is a fairly mature library, formalizing the theory of nominal
sets and providing ample sugar for the user to hide away the theory,
Agda is a lot more bare-bones and the ``penalty'' incurred by
formalizing and then using nominal sets would have been significantly
higher.\\
Instead, we chose to implement the locally nameless version, which
proved to have consistent, but fairly minimal overheads, requiring
relatively little extra theory. This is supported by the fact that the
LN version of the calculus and proofs in Isabelle was roughly 1140 lines
of code, whereas the Agda version was only slightly longer at 1350
lines, which, when adjusted to the same spacing/formatting comes down to
roughly 1230 lines. This rough metric also demonstrates that whilst
Isabelle's automation can be a lot more powerful than Agda's, in our
case (partially by design), there were only a few instances where
Isabelle clearly had the upper hand. Over all, it turned out, once
again, that there were only small, often just cosmetic, differences
between the two implementations.

\section{Overview}\label{overview-2}

One of the most apparent differences between Agda and Isabelle is the
treatment of functions and proofs in both languages. Whilst in Isabelle,
there is always a clear syntactic distinction between programs and
proofs, Agda's richer dependent-type system allows constructing proofs
as programs. This distinction is especially visible in inductive proofs,
which have a completely distinct syntax in Isabelle. As proofs are not
objects which can be directly manipulated in Isabelle, to modify the
proof goal, user commands such as \texttt{apply rule} or
\texttt{by auto} are used:

\begin{minted}[]{isabelle}
lemma subst_fresh: "x â FV t â¹ t[x ::= u] = t"
apply (induct t)
by auto
\end{minted}

In the proof above, the command \texttt{apply (induct t)} takes a proof
object with the goal \texttt{x â FV t â¹ t[x ::= u] = t}, and applies the
induction principle for \texttt{t}, generating 5 new proof obligations:

\begin{minted}[]{idris}
proof (prove)
goal (5 subgoals):
1. âxa. x â FV (FVar xa) â¹ FVar xa [x ::= u] = FVar xa
2. âxa. x â FV (BVar xa) â¹ BVar xa [x ::= u] = BVar xa
3. ât1 t2.
    (x â FV t1 â¹ t1 [x ::= u] = t1) â¹
    (x â FV t2 â¹ t2 [x ::= u] = t2) â¹
    x â FV (App t1 t2) â¹ App t1 t2 [x ::= u] = App t1 t2
4. ât. (x â FV t â¹ t [x ::= u] = t) â¹ x â FV (Lam t) â¹ 
    Lam t [x ::= u] = Lam t
5. âxa. x â FV (Y xa) â¹ Y xa [x ::= u] = Y xa
\end{minted}

These can then discharged by the call to \texttt{auto}, which is a
command that invokes the automatic solver, that tries to prove all the
goals in the given context.

In contrast to this, in an Agda proof, the proof objects are available
to the user directly. Instead of using commands modifying the proof
state, one begins with a definition of the lemma:

\begin{minted}[]{agda}
subst-fresh : â x t u -> (xâFVt : x â (FV t)) -> (t [ x ::= u ]) â¡ t
subst-fresh x t u xâFVt = ?
\end{minted}

The \texttt{?} acts as a `hole' which the user fills in, by
incrementally constructing the proof. Using the Emacs/Atom editor's
``agda-mode'', once can apply a case split to \texttt{t} (corresponding
to the \texttt{apply (induct t)} call in Isabelle), generating the
following definition:

\begin{minted}[]{agda}
subst-fresh : â x t u -> (xâFVt : x â (FV t)) -> (t [ x ::= u ]) â¡ t
subst-fresh x (bv i) u xâFVt = {!   0!}
subst-fresh x (fv xâ) u xâFVt = {!   1!}
subst-fresh x (lam t) u xâFVt = {!   2!}
subst-fresh x (app t tâ) u xâFVt = {!   3!}
subst-fresh x (Y tâ) u xâFVt = {!   4!}
\end{minted}

When the above definition is compiled, Agda generates 5 goals needed to
`fill' each hole:

\begin{minted}[]{agda}
?0  :  (bv i [ x ::= u ]) â¡ bv i
?1  :  (fv xâ [ x ::= u ]) â¡ fv xâ
?2  :  (lam t [ x ::= u ]) â¡ lam t
?3  :  (app t tâ [ x ::= u ]) â¡ app t tâ
?4  :  (Y tâ [ x ::= u ]) â¡ Y tâ
\end{minted}

As one can see, there is a clear correspondence between the 5 generated
goals in Isabelle and the cases of the Agda proof above.

Due to this correspondence, reasoning in both systems is often largely
similar. Whereas in Isabelle, one modifies the proof indirectly by
issuing commands to modify proof goals, in Agda, one generates proofs
directly by writing a program-as-proof, which satisfies the type
constraints given in the definition.

\section{Automation}\label{automation}

As seen in the first example, Isabelle relies on automation in its
proofs. It includes several automatic provers of varying complexity,
including \texttt{simp}, \texttt{auto}, \texttt{blast}, \texttt{metis}
and others. These are usually tactics/programs which automatically apply
rewrite-rules, until the goal is discharged. If the tactic fails to
discharge a goal within a set number of steps, it stops and lets the
user direct the proof. The use of tactics in Isabelle is commonly used
to prove trivial goals, which usually follow from simple rewriting of
definitions or case analysis of certain variables.\\
\(\ \)

\begin{Example}

\label{Example:substFresh} For example, the proof goal

\begin{minted}[]{idris}
âxa. x âÌ¸ FV (FVar xa) â¹ FVar xa [x ::= u] = FVar xa
\end{minted}

will be proved by first unfolding the definition of substitution for
\texttt{FVar}

\begin{minted}[]{idris}
(FVar xa)[x ::= u] = (if xa = x then u else FVar xa)
\end{minted}

and then deriving \texttt{x â  xa} from the assumption
\texttt{x âÌ¸ FV (FVar xa)}.\\
Applying these steps explicitly, we get:

\begin{minted}[escapeinside=||]{isabelle}
lemma subst_fresh: "x â FV t â¹ t[x ::= u] = t"
apply (induct t)
apply (subst subst.simps(1))
apply (drule subst[OF FV.simps(1)])
apply (drule subst[OF Set.insert_iff])
apply (drule subst[OF Set.empty_iff])
apply (drule subst[OF HOL.simp_thms(31)])
|$\texttt{\vdots}$|
\end{minted}

where the goal now has the following shape:

\begin{minted}[]{idris}
1. âxa. x â  xa â¹ (if xa = x then u else FVar xa) = FVar xa
\end{minted}

From this point, the simplifier rewrites \texttt{xa = x} to
\texttt{False} and \texttt{(if False then u else FVar xa)} to
\texttt{FVar xa} in the goal.\\
The use of tactics and automated tools is heavily ingrained in Isabelle
and it is actually impossible (i.e.~impossible for me) to not use
\texttt{simp} at this point in the proof, partly because one gets so
used to discharging such trivial goals automatically and partly because
it becomes nearly impossible to do the last two steps explicitly without
having a detailed knowledge of the available commands and tactics in
Isabelle (i.e.~I don't).\\
Doing these steps explicitly quickly becomes cumbersome, as one needs to
constantly look up the names of basic lemmas, such as
\texttt{Set.empty\_iff}, which is a simple rewrite rule
\texttt{(?c â \{\}) = False}.

\end{Example}

Unlike Isabelle, Agda does not include nearly as much automation. The
only proof search tool included with Agda is Agsy, which is similar,
albeit often weaker than the \texttt{simp} tactic. It may therefore seem
that Agda will be much more cumbersome to reason in than Isabelle. This,
however, turns out not to be the case (at least in this formalization),
in part due to Agda's type system and the powerful pattern matching as
well as direct access to the proof goals. Automation did not play as
major a part in this project as it might have, especially in this round
of the comparison, since the LN mechanization had to be implemented from
scratch and thus, the proofs written in Isabelle were only later
modified to leverage some automation. However, since most proofs
required induction, which theorem provers are generally not very good at
performing without user guidance, the only place where automation was
really apparent was in the case of a few lemmas involving equational
reasoning, like the ``open-swap'' lemma:

\begin{Lemma}

\label{Lemma:opnSwap}
\(k \neq n \implies x \neq y \implies \{k \to x\}\{n \to y\}M = \{n \to y\}\{k \to x\}M\)

\end{Lemma}

Whilst in Isabelle, this was a trivial case of applying induction on the
term \(M\) and letting \texttt{auto} prove all the remaining cases. In
Agda, this was a lot more painful, as the cases had to be constructed
and proved more or less manually, yielding this rather long(er) proof:

\begin{minted}[]{agda}
^-^-swap : â k n x y m -> Â¬(k â¡ n) -> Â¬(x â¡ y) -> 
  [ k >> fv x ] ([ n >> fv y ] m) â¡ [ n >> fv y ] ([ k >> fv x ] m)
^-^-swap k n x y (bv i) kâ n xâ y with n â i
^-^-swap k n x y (bv .n) kâ n xâ y | yes refl with k â n
^-^-swap n .n x y (bv .n) kâ n xâ y | yes refl | yes refl = â¥-elim (kâ n refl)
^-^-swap k n x y (bv .n) kâ n xâ y | yes refl | no _ with n â n
^-^-swap k n x y (bv .n) kâ n xâ y | yes refl | no _ | yes refl = refl
^-^-swap k n x y (bv .n) kâ n xâ y | yes refl | no _ | no nâ n = 
  â¥-elim (nâ n refl)
^-^-swap k n x y (bv i) kâ n xâ y | no nâ i with k â n
^-^-swap n .n x y (bv i) kâ n xâ y | no nâ i | yes refl = â¥-elim (kâ n refl)
^-^-swap k n x y (bv i) kâ n xâ y | no nâ i | no _ with k â i
^-^-swap k n x y (bv .k) kâ n xâ y | no nâ i | no _ | yes refl = refl
^-^-swap k n x y (bv i) kâ n xâ y | no nâ i | no _ | no kâ i with n â i
^-^-swap k i x y (bv .i) kâ n xâ y | no nâ i | no _ | no kâ i | yes refl = 
  â¥-elim (nâ i refl)
^-^-swap k n x y (bv i) kâ n xâ y | no nâ i | no _ | no kâ i | no _ = refl
^-^-swap k n x y (fv z) kâ n xâ y = refl
^-^-swap k n x y (lam m) kâ n xâ y = 
  cong lam (^-^-swap (suc k) (suc n) x y m (Î» skâ¡sn â kâ n (â¡-suc skâ¡sn)) xâ y)
^-^-swap k n x y (app t1 t2) kâ n xâ y rewrite
  ^-^-swap k n x y t1 kâ n xâ y | ^-^-swap k n x y t2 kâ n xâ y = refl
^-^-swap k n x y (Y _) kâ n xâ y = refl
\end{minted}

\section{Proofs-as-programs in Agda}\label{proofs-as-programs-in-agda}

\label{proofAsProg}

As was already mentioned, Agda treats proofs as programs, and therefore
provides direct access to proof objects. In Isabelle, the proof goal is
usually of the form:

\begin{minted}[]{idris}
lemma x: "assm-1 â¹ ... â¹ assm-n â¹ concl"
\end{minted}

Using the `apply-style' reasoning in Isabelle can become burdensome, if
one needs to modify or reason with the assumptions, as was seen in
\cref{Example:substFresh}. In this example, the \texttt{drule} tactic,
which is used to apply rules to the premises rather than the conclusion,
was applied repeatedly. Other times, we might have to use structural
rules for exchange or weakening, which are necessary purely for
\texttt{organizational} purposes of the proof.\\
In Agda, such rules are not necessary, since the example above looks
like a function definition:

\begin{minted}[]{idris}
x assm-1 ... assm-n = ?
\end{minted}

Here, \texttt{assm-1} to \texttt{assm-n} are simply arguments to the
function x, which expects something of type \texttt{concl} in the place
of \texttt{?}. This presentation allows one to use the given assumptions
arbitrarily, perhaps passing them to another function/proof or
discarding them if not needed.\\
This way of reasoning is also supported in Isabelle to some extent, via
the use of the Isar proof language, where (the snippet of) the proof of
\texttt{subst\_fresh} can be expressed in the following way:

\begin{minted}[escapeinside=||]{isabelle}
lemma subst_fresh': 
  assumes "x â FV t"
  shows "t[x ::= u] = t"
using assms proof (induct t)
case (FVar y)
  from FVar.prems have "x â {y}" unfolding FV.simps(1) .
  then have "x â  y" unfolding Set.insert_iff Set.empty_iff HOL.simp_thms(31) .
  then show ?case unfolding subst.simps(1) by simp
next
|$\texttt{\vdots}$|
qed
\end{minted}

This representation is more natural (and readable) to humans, as the
assumptions have been separated and can be referenced and used in a
clearer manner. For example, in the line

\begin{minted}[]{isabelle}
from FVar.prems have "x â {y}"
\end{minted}

the premise \texttt{FVar.prems} is added to the context of the goal
\texttt{x â \{y\}}:

\begin{minted}[]{idris}
proof (prove)
using this:
  x â FV (FVar y)

goal (1 subgoal):
 1. x â {y}
\end{minted}

The individual reasoning steps described in the previous section have
also been separated out into `mini-lemmas' (the command \texttt{have}
creates an new proof goal which has to be proved and then becomes
available as an assumption in the current context) along the lines of
the intuitive reasoning discussed initially. While this proof is more
human readable, it is also more verbose and potentially harder to
automate, as generating valid Isar style proofs is more difficult, due
to `Isar-style' proofs being obviously more complex than `apply-style'
proofs.

Whilst using the Isar proof language gives us a finer control and better
structuring of proofs, one still references proofs only indirectly.
Looking at the same proof in Agda, we have the following definition for
the case of free variables:

\begin{minted}[]{agda}
subst-fresh' x (fv y) u xâFVt = {!   0!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  fv y [ x ::= u ] â¡ fv y
\end{minted}

The proof of this case is slightly different from the Isabelle proof. In
order to understand why, we need to look at the definition of
substitution for free variables in Agda:

\begin{minted}[]{agda}
fv y [ x ::= u ] with x â y
... | yes _ = u
... | no _ = fv y
\end{minted}

This definition corresponds to the Isabelle definition, however, instead
of using an if-then-else conditional, the Agda definition uses the
\texttt{with} abstraction to pattern match on \texttt{x â y}. The
\texttt{\_â\_} function takes the arguments \texttt{x} and \texttt{y},
which are natural numbers, and decides syntactic equality, returning a
\texttt{yes p} or \texttt{no p}, where \texttt{p} is the proof object
showing their in/equality.\\
Since the definition of substitution does not require the proof object
of the equality of \texttt{x} and \texttt{y}, it is discarded in both
cases. If \texttt{x} and \texttt{y} are equal, \texttt{u} is returned
(case \texttt{... | yes \_ = u}), otherwise \texttt{fv y} is returned.

In order for Agda to be able to unfold the definition of
\texttt{fv y [ x ::= u ]}, it needs the case analysis on \texttt{x â y}:

\begin{minted}[]{agda}
subst-fresh' x (fv y) u xâFVt with x â y
... | yes p = {!   0!}
... | no Â¬p = {!   1!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  (fv y [ x ::= u ] | yes p) â¡ fv y
?1  :  (fv y [ x ::= u ] | no Â¬p) â¡ fv y
\end{minted}

In the second case, when \texttt{x} and \texttt{y} are different, Agda
can automatically fill in the hole with \texttt{refl}. Notice that
unlike in Isabelle, where the definition of substitution had to be
manually unfolded (the command \texttt{unfolding subst.simps(1)}), Agda
performs type reduction automatically and can rewrite the term
\texttt{(fv y [ x ::= u ] | no .Â¬p)} to \texttt{fv y} when type-checking
the expression.

For the case where \texttt{x} and \texttt{y} are equal, one can
immediately derive a contradiction from the fact that \texttt{x} cannot
be equal to \texttt{y}, since \texttt{x} is not a free variable in
\texttt{fv y}. The type of false propositions is \texttt{â¥} in Agda.
Given \texttt{â¥}, one can derive any proposition. To derive \texttt{â¥},
we first inspect the type of \texttt{xâFVt}, which is
\texttt{x â y â· []}. Further examining the definition of \texttt{â}, we
find that \texttt{x â xs = Â¬ x â xs}, which further unfolds to
\texttt{x â xs = x â xs â â¥}. Thus to obtain \texttt{â¥}, we simply have
to show that \texttt{x â xs}, or in this specific instance
\texttt{x â y â· []}. The definition of \texttt{â} is itself just sugar
for \texttt{x â xs = Any (\_â\_ x) xs}, where \texttt{Any P xs} means
that there is an element of the list \texttt{xs} which satisfies
\texttt{P}. In this instance, \texttt{P = (\_â\_ x)}, thus an inhabitant
of the type \texttt{Any (\_â\_ x) (y â· [])} can be constructed if one
has a proof that at least one element in \texttt{y â· []} is equivalent
to \texttt{x}. As it happens, such a proof was given as an argument in
\texttt{yes p}:

\begin{minted}[]{agda}
False : â¥
False = xâFVt (here p)
\end{minted}

The finished case looks like this (note that \texttt{â¥-elim} takes
\texttt{â¥} and produces something of arbitrary type):

\begin{minted}[]{agda}
subst-fresh' x (fv y) u xâFVt with x â y
... | yes p = â¥-elim False
  where
  False : â¥
  False = xâFVt (here p)
... | no Â¬p = refl
\end{minted}

We can even transform the Isabelle proof to closer match the Agda proof:

\begin{minted}[]{isabelle}
case (FVar y)
  show ?case
  proof (cases "x = y")
  case True
    with FVar have False by simp
    thus ?thesis ..
  next
  case False then show ?thesis unfolding subst.simps(1) by simp
  qed
\end{minted}

Thus, we can see that using Isar style proofs and Agda reasoning ends up
being rather similar in practice.

\section{Pattern matching}\label{pattern-matching}

Another reason why automation in the form of explicit proof search
tactics needn't play such a significant role in Agda, is the more
sophisticated type system of Agda (compared to Isabelle). Since Agda
uses a dependent type system, there are often instances where the type
system imposes certain constraints on the arguments/assumptions in a
definition/proof and partially acts as a proof search tactic, by guiding
the user through simple reasoning steps. Since Agda proofs are programs,
unlike Isabelle `apply-style' proofs, which are really proof scripts,
one cannot intuitively view and step through the intermediate reasoning
steps done by the user to prove a lemma. The way one proves a lemma in
Agda is to start with a lemma with a `hole', which is the proof goal,
and iteratively refine the goal until this proof object is constructed.
The way Agda's pattern matching makes constructing proofs easier can be
demonstrated with the following example.

\begin{Example}

The following lemma states that the parallel-\(\beta\) maximal reduction
preserves local closure:

\begin{center}
$t \ggg t' \implies \trm(t) \land \trm(t')$
\end{center}

For simplicity, we will prove a slightly simpler version, namely:
\(t \ggg t' \implies \trm(t)\). For comparison, this is a short, highly
automated proof in Isabelle:

\begin{minted}[]{isabelle}
lemma pbeta_max_trm_r : "t >>> t' â¹ trm t"
apply (induct t t' rule:pbeta_max.induct)
apply (subst trm.simps, simp)+
by (auto simp add: lam trm.Y trm.app)
\end{minted}

In Agda, we start with the following definition:

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l t>>>t' = {!   0!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term .t
\end{minted}

Construction of this proof follows the Isabelle script, in that the
proof proceeds by induction on \(t \ggg t'\), which corresponds to the
command \texttt{apply (induct t t' rule:pbeta\_max.induct)}. As seen
earlier, induction in Agda simply corresponds to a case split. The
agda-mode in Emacs/Atom can perform a case split automatically, if
supplied with the variable which should be used for the case analysis,
in this case \texttt{t>>>t'}.

\vspace{1em}

\begin{Remark}

Note that Agda is very liberal with variable names, allowing almost any
ASCII or Unicode characters, and it is customary to give descriptive
names to the variables, usually denoting their type. In this instance,
\texttt{t>>>t'} is a variable of type \texttt{t >>> t'}.\\
Due to Agda's relative freedom in variable names, whitespace is
important, as \texttt{t>>> t'} is very different from \texttt{t >>> t'}
(the first is parsed a two variables \texttt{t>>>} and \texttt{t'},
whereas the second is parsed as the variable \texttt{t}, the relation
symbol \texttt{>>>} and another variable \texttt{t'}).

\end{Remark}

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = {!   0!}
>>>-Term-l reflY = {!   1!}
>>>-Term-l (app x t>>>t' t>>>t'') = {!   2!}
>>>-Term-l (abs L x) = {!   3!}
>>>-Term-l (beta L cf t>>>t') = {!   4!}
>>>-Term-l (Y t>>>t') = {!   5!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term (fv .x)
?1  :  Term (Y .Ï)
?2  :  Term (app .m .n)
?3  :  Term (lam .m)
?4  :  Term (app (lam .m) .n)
?5  :  Term (app (Y .Ï) .m)
\end{minted}

The newly expanded proof now contains 5 `holes', corresponding to the 5
constructors for the \(>>>\) reduction. The first two goals are trivial,
since any free variable or Y is a closed term. Here, one can use the
agda-mode again, applying `Refine', which is like a simple proof search,
in that it will try to advance the proof by supplying an object of the
correct type for the specified `hole'. Applying `Refine' to
\texttt{\{!\ \ \ 0!\}} and \texttt{\{!\ \ \ 1!\}} yields:

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x t>>>t' t>>>t'') = {!   0!}
>>>-Term-l (abs L x) = {!   1!}
>>>-Term-l (beta L cf t>>>t') = {!   2!}
>>>-Term-l (Y t>>>t') = {!   3!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term (app .m .n)
?1  :  Term (lam .m)
?2  :  Term (app (lam .m) .n)
?3  :  Term (app (Y .Ï) .m)
\end{minted}

Since the constructor for \texttt{var} is
\texttt{var : â {x} -> Term (fv x)}, it is easy to see that the
\texttt{hole} can be closed by supplying \texttt{var} as the proof of
\texttt{Term (fv .x)}.\\
A more interesting case is the \texttt{app} case, where using `Refine'
yields:

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x t>>>t' t>>>t'') = app {!   0!} {!   1!}
>>>-Term-l (abs L x) = {!   2!}
>>>-Term-l (beta L cf t>>>t') = {!   3!}
>>>-Term-l (Y t>>>t') = {!   4!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term .m
?1  :  Term .n
?2  :  Term (lam .m)
?3  :  Term (app (lam .m) .n)
?4  :  Term (app (Y .Ï) .m)
\end{minted}

Here, the refine tactic supplied the constructor \texttt{app}, as it's
type \texttt{app : â {eâ eâ} -> Term eâ -> Term eâ -> Term (app eâ eâ)}
fit the `hole' (\texttt{Term (app .m .n)}), generating two new `holes',
with the goal \texttt{Term .m} and \texttt{Term .n}. However, trying
`Refine' again on either of the `holes' yields no result. This is where
one applies the induction hypothesis, by adding
\texttt{>>>-Term-l t>>>t'} to \texttt{\{!\ \ \ 0!\}} and applying
`Refine' again, which closes the `hole' \texttt{\{!\ \ \ 0!\}}. Perhaps
confusingly, \texttt{>>>-Term-l t>>>t'} produces a proof of
\texttt{Term .m}. To see why this is, one has to inspect the type of
\texttt{t>>>t'} in this context. Helpfully, the agda-mode provides just
this function, which infers the type of \texttt{t>>>t'} to be
\texttt{.m >>> .m'}. Similarly, \texttt{t>>>t''} has the type
\texttt{.n >>> .n'}. Renaming \texttt{t>>>t'} and \texttt{t>>>t''} to
\texttt{m>>>m'} and \texttt{n>>>n'} respectively, now makes the
recursive call obvious:

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = app (>>>-Term-l m>>>m') {!   0!}
>>>-Term-l (abs L x) = {!   1!}
>>>-Term-l (beta L cf t>>>t') = {!   2!}
>>>-Term-l (Y t>>>t') = {!   3!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  Term .n
?1  :  Term (lam .m)
?2  :  Term (app (lam .m) .n)
?3  :  Term (app (Y .Ï) .m)
\end{minted}

The goal \texttt{Term .n} follows in exactly the same fashion. Applying
`Refine' to the next `hole' yields:

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = 
  app (>>>-Term-l m>>>m') (>>>-Term-l n>>>n')
>>>-Term-l (abs L x) = lam {!   0!} {!   1!}
>>>-Term-l (beta L cf t>>>t') = {!   2!}
>>>-Term-l (Y t>>>t') = {!   3!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  FVars
?1  :  {x = xâ : â} â xâ â ?0 L x â Term (.m ^' xâ)
?2  :  Term (app (lam .m) .n)
?3  :  Term (app (Y .Ï) .m)
\end{minted}

At this stage, the interesting goal is \texttt{?1}, due to the fact that
it is dependent on \texttt{?0}. Indeed, replacing \texttt{?0} with
\texttt{L} (which is the only thing of the type \texttt{FVars} available
in this context) changes goal \texttt{?1} to
\texttt{\{x = xâ : â\} â xâ â L â Term (.m \textasciicircum' xâ)}:

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = 
  app (>>>-Term-l m>>>m') (>>>-Term-l n>>>n')
>>>-Term-l (abs L x) = lam L {!   0!}
>>>-Term-l (beta L cf t>>>t') = {!   1!}
>>>-Term-l (Y t>>>t') = {!   2!}
\end{minted}

\noindent\rule{8cm}{0.4pt}

\begin{minted}[]{agda}
?0  :  {x = xâ : â} â xâ â L â Term (.m ^' xâ)
?1  :  Term (app (lam .m) .n)
?2  :  Term (app (Y .Ï) .m)
\end{minted}

Since the goal/type of \texttt{\{!\ \ \ 0!\}} is
\texttt{\{x = xâ : â\} â xâ â L â Term (.m \textasciicircum' xâ)},
applying `Refine' will generate a lambda expression
\texttt{(Î» xâL â \{!\ \ \ 0!\})}, as this is obviously the only
`constructor' for a function type. Again, confusingly, we supply the
recursive call \texttt{>>>-Term-l (x xâL)} to \texttt{\{!\ \ \ 0!\}}. By
examining the type of \texttt{x}, we get that \texttt{x} has the type
\texttt{\{x = xâ : â\} â xâ â L â (.m \textasciicircum' xâ) >>> (.m' \textasciicircum' xâ)}.
Then \texttt{(x xâL)} is clearly of the type
\texttt{(.m \textasciicircum' xâ) >>> (.m' \textasciicircum' xâ)}. Thus
\texttt{>>>-Term-l (x xâL)} has the desired type
\texttt{Term (.m \textasciicircum' .x)} (note that \texttt{.x} and
\texttt{x} are not the same in this context).

Doing these steps explicitly was not in fact necessary, as the automatic
proof search `Agsy' is capable of automatically constructing proof
objects for all of the cases above. Using `Agsy' in both of the last two
cases, the completed proof is given below:

\begin{minted}[]{agda}
>>>-Term-l : â {t t'} -> t >>> t' -> Term t
>>>-Term-l refl = var
>>>-Term-l reflY = Y
>>>-Term-l (app x m>>>m' n>>>n') = 
  app (>>>-Term-l m>>>m') (>>>-Term-l n>>>n')
>>>-Term-l (abs L x) = lam L (Î» xâL â >>>-Term-l (x xâL))
>>>-Term-l (beta L cf t>>>t') = app 
  (lam L (Î» {x} xâL â >>>-Term-l (cf xâL))) 
  (>>>-Term-l t>>>t')
>>>-Term-l (Y t>>>t') = app Y (>>>-Term-l t>>>t')
\end{minted}

\end{Example}

\chapter{Intersection types}\label{intersection-types-1}

\label{chap:itypes}

Having compared different mechanizations and implementation languages
for the simply typed \(\lamy\) calculus in the previous two chapters, we
arrived at the ``winning'' combination of a locally nameless
mechanization using Agda. Carrying on in this setting, we present the
formalization of intersection types for the \(\lamy\) calculus along
with the proof of subject invariance for intersection types.\\
Whilst the theory formalized so far ``only'' includes the basic
definitions of intersection type assignment and the proof of subject
invariance, these proofs turned out to be significantly more difficult
than their simply typed counterparts (e.g.~in case of sub-tying and
subject reduction lemmas). Indeed the whole formalization of simple
types, along with the proof of the Church Rosser theorem, is roughly
only 1350 lines of code in Agda, in comparison to about 1890 lines, for
the intersection typing together with proofs of subject invariance.\\
Even though the proof is not novel, there is, to our knowledge, no known
fully formal version of it for the \(\lamy\) calculus. The chapter
mainly focuses on the engineering choices that were made in order to
simplify the proofs as much as possible, as well as the necessary
implementation overheads and compromises that were made.\\
The chapter is presented in sections, each explaining implementation
details for a specific lemma or definition, introduced in
\cref{itypesIntro}. Some of the definitions presented early on in this
chapter undergo several revisions, as we discuss the necessities for
these changes in a pseudo-chronological manner, in which they arose
during the implementation stage of this project.

\section{Intersection types in Agda}\label{intersection-types-in-agda}

\label{itypesAgda}

The first implementation detail we had to consider was the
implementation of the definition of intersection types themselves.
Unlike simple types, the definition of intersection-types is split into
two mutually recursive definitions of strict (\texttt{IType}
\(/\mathcal{T}_s\)) and intersection (\texttt{IType}\(_\ell\)
\(/\mathcal{T}\)) types:

\begin{minted}[escapeinside=||, xleftmargin=1em, linenos=true]{agda}
data IType|$_\ell$| : Set
data IType : Set

data IType where
  Ï : IType
  _~>_ : (s : IType|$_\ell$|) -> (t : IType) -> IType

data IType|$_\ell$| where
  â© : List IType -> IType|$_\ell$|
\end{minted}

The reason why the intersection \texttt{IType}\(_\ell\) is defined as a
list of strict types \texttt{IType} in line 9, is due to the (usually)
implicit requirement that the types in \(\mathcal{T}\) be finite. The
decision to use lists as an implementation of fine sets was taken,
because the Agda standard library includes a definition of lists with
definitions of list membership \(\in\) and other associated lemmas,
which proved to be useful for definitions of the \(\subseteq\) relation
on types.

From the above definition, it is obvious that the split definitions of
\texttt{IType} and \texttt{IType}\(_\ell\) are somewhat redundant, in
that \texttt{IType}\(_\ell\) only has one constructor \texttt{â©} and
therefore, any instance of \texttt{IType}\(_\ell\) in the definition of
\texttt{IType} can simply be replaced by \texttt{List IType}:

\begin{minted}[]{agda}
data IType : Set where
  Ï : IType
  _~>_ : List IType -> IType -> IType
\end{minted}

\section{Type refinement}\label{type-refinement}

One of the first things we needed to add to the notion of intersection
type assignment (and as a result also to the \(\subseteq\) relation on
intersection types) was the notion of simple-type refinement. The main
idea of intersection types for \(\lamy\) terms is for the intersection
types to somehow ``refine'' the simple types. Intuitively, this notion
should capture the relationship between the ``shape'' of the
intersection and simple types.

To demonstrate the reason for introducing type refinement, we look at
the initial formulation of the (intersection) typing rule \((Y)\):

\begin{center}
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \RightLabel{$(j \in \underline{n})$}
  \UnaryInfC{$\Gamma \Vdash Y_\sigma : (\taui \leadsto \tau_1 \cap\hdots\cap \taui \leadsto \tau_i) \leadsto \tau_j$}
  \DisplayProof
  \vskip 1.5em
\end{center}

The lack of connection between simple and intersection types in the
typing relation is especially apparent here, as \(\taui\) seems to be
chosen arbitrarily. Once we reformulate the above definition to include
type refinement, the choice of \(\taui\) makes more sense, since we know
that \(\tau_1, \hdots, \tau_i\) will somehow be related to the simple
type \(\sigma\):

\begin{center}
  \vskip 1.5em
  \AxiomC{$\taui :: \sigma$}
  \LeftLabel{$(Y)$}
  \RightLabel{$(j \in \underline{n})$}
  \UnaryInfC{$\Gamma \Vdash Y_\sigma : (\taui \leadsto \tau_1 \cap\hdots\cap \taui \leadsto \tau_i) \leadsto \tau_j$}
  \DisplayProof
  \vskip 1.5em
\end{center}

The refinement relation has been defined in Kobayashi
(\protect\hyperlink{ref-kobayashi09}{2009}) (amongst others) and is
presented below:

\begin{Definition}[Intersection type refinement]

Since intersection types are defined in terms of strict
(\(\mathcal{T}_s\)) and non-strict (\(\mathcal{T}\)) intersection types,
the definition of refinement (\(::\)) is split into two versions, one
for strict and another for non-strict types. In the definition below,
\(\tau\) ranges over strict intersection types \(\mathcal{T}_s\), with
\(\tau_i, \tau_j\) ranging over non-strict intersection types
\(\mathcal{T}\), and \(A, B\) range over simple types \(\sigma\):

\begin{center}
  \AxiomC{}
  \LeftLabel{$(base)$}
  \UnaryInfC{$\phi :: \mathsf{o}$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\tau_i ::_\ell A$}
  \AxiomC{$\tau_j ::_\ell B$}
  \LeftLabel{$(arr)$}
  \BinaryInfC{$\tau_i \leadsto \tau_j :: A \to B$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\omega ::_\ell A$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\tau :: A$}
  \AxiomC{$\tau_i ::_\ell A$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\tau , \tau_i ::_\ell A$}
  \DisplayProof
\end{center}

\end{Definition}

Having a notion of type refinement, we then modified the subset relation
on intersection types, s.t. \(\subseteq\) is defined only for pairs of
intersection types, which refine the same simple type:

\begin{Definition}[$\subseteq^A$]

\label{Definition:subseteqNew} In the definition below, \(\tau, \tau'\)
range over \(\mathcal{T}_s\), \(\tau_i, \hdots, \tau_n\) range over
\(\mathcal{T}\) and \(A, B\) range over \(\sigma\):

\begin{center}
  \AxiomC{}
  \LeftLabel{$(base)$}
  \UnaryInfC{$\phi \subseteq^\mathsf{o} \phi$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\tau_i \subseteq^A_\ell \tau_j$}
  \AxiomC{$\tau_m \subseteq^B \tau_n$}
  \LeftLabel{$(arr)$}
  \BinaryInfC{$\tau_j \leadsto \tau_m \subseteq^{A \to B} \tau_i \leadsto \tau_n$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$\tau_i ::_\ell A$}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\omega \subseteq^A_\ell \tau_i$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\exists \tau' \in \tau_j.\ \tau \subseteq^A \tau'$}
  \AxiomC{$\tau_i \subseteq^A_\ell \tau_j$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\tau , \tau_i \subseteq^A_\ell \tau_j$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

\section{\texorpdfstring{Well typed
\(\subseteq\)}{Well typed \textbackslash{}subseteq}}\label{well-typed-subseteq}

The presentation of the \(\subseteq\) relation in
\cref{Definition:subseteqOrig} differs quite significantly from the one
presented above. The main difference is obviously the addition of type
refinement, but the definition now also includes the \((base)\) rule,
which allows one to derive the previously implicitly stated reflexivity
and transitivity rules. \\
Another departure from the original definition is the formulation of the
following two properties as the \((nil)\) and \((cons)\) rules:

\begin{center}
$\begin{aligned}
\forall\ i \in \underline{n}.\ \ \tau_i \subseteq& \taui \\ 
\forall\ i \in \underline{n}.\ \ \tau_i \subseteq \tau \implies& \taui \subseteq \tau \\
\end{aligned}$
\end{center}

To give a motivation as to why we chose a different formulation of these
properties, we first examine the original definition and show why it's
not rigorous enough for a well typed Agda definition.\\
As we've shown in \cref{itypesAgda}, the definition of intersection
types is implicitly split into strict \texttt{IType}s and intersections,
encoded as \texttt{List IType}s. All the preceding definitions follow
this split with the strict and non strict versions of the type
refinement (\(::\) and \(::_\ell\) respectively) and sub-typing
relations (\(\subseteq\) and \(\subseteq_\ell\) respectively).\\
If we tried to turn the first property above into a rule, such as:

\begin{center}
  \AxiomC{$\tau \in \tau_i$}
  \LeftLabel{$(prop'\ 1)$}
  \UnaryInfC{$\tau \subseteq \tau_i$}
  \DisplayProof
\end{center}

where \(\tau\) is a strict type \texttt{IType} and \(\tau_i\) is an
intersection \texttt{List IType}, we would immediately get a type error,
because the type signature of \(\subseteq\) (which does not include type
refinement) is:

\begin{minted}[]{agda}
data _â_ : IType -> IType -> Set
\end{minted}

In order to get a well typed version of this rule, we would have to
write something like:

\begin{center}
  \AxiomC{$\tau \in \tau_i$}
  \LeftLabel{$(prop'\ 1)$}
  \UnaryInfC{$[\tau] \subseteq_\ell \tau_i$}
  \DisplayProof
\end{center}

Similarly for the second property, the well typed version might be
formulated as:

\begin{center}
  \AxiomC{$\forall \tau' \in \tau_i.\ [\tau'] \subseteq_\ell \tau$}
  \LeftLabel{$(prop'\ 2)$}
  \UnaryInfC{$\tau_i \subseteq_\ell \tau$}
  \DisplayProof
\end{center}

However, in the rule above, we assumed/forced \(\tau\) to be an
intersection, yet the property does not enforce this, and thus the two
rules above do not actually capture the two properties from
\cref{Definition:subseteqOrig}.

\begin{Example}

To demonstrate this, take the two intersection types
\(((\psi \cap \tau) \to \psi) \cap ((\psi \cap \tau \cap \rho) \to \psi)\)
and \((\psi \cap \tau) \to \psi\). According to the original definition,
we will have:

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$(\psi \cap \hdots$}

  \AxiomC{}
  \LeftLabel{$(prop\ 1)$}
  \UnaryInfC{$\psi \subseteq \psi \cap \tau \cap \rho$}
  \AxiomC{}
  \LeftLabel{$(prop\ 1)$}
  \UnaryInfC{$\tau \subseteq \psi \cap \tau \cap \rho$}
  \LeftLabel{$(prop\ 2)$}
  \BinaryInfC{$\psi \cap \tau \subseteq \psi \cap \tau \cap \rho$}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$\psi \subseteq \psi$}
  \LeftLabel{$(prop\ 3)$}
  \BinaryInfC{$(\psi \cap \tau \cap \rho) \to \psi \subseteq (\psi \cap \tau) \to \psi$}
  \LeftLabel{$(prop\ 2)$}
  \BinaryInfC{$((\psi \cap \tau) \to \psi) \cap ((\psi \cap \tau \cap \rho) \to \psi) \subseteq (\psi \cap \tau) \to \psi$}
  \DisplayProof
\end{center}

When we try to prove the above using the well typed rules, we first need
to coerce \((\psi \cap \tau) \to \psi\) into an intersection. Then, we
try to construct the derivation tree:

\begin{center}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$[[\psi , \tau] \to \psi] \subseteq_\ell [[\psi , \tau] \to \psi]$}

  \AxiomC{$[[\psi , \tau , \rho] \to \psi] \subseteq_\ell [[\psi , \tau] \to \psi]$}
  \LeftLabel{$(prop'\ 2)$}
  \BinaryInfC{$[[\psi , \tau] \to \psi, [\psi , \tau , \rho] \to \psi] \subseteq_\ell [[\psi , \tau] \to \psi]$}
  \DisplayProof
\end{center}

The open branch
\([[\psi , \tau , \rho] \to \psi] \subseteq_\ell [[\psi , \tau] \to \psi]\)
in the example clearly demonstrates that the current formulation of the
two properties clearly doesn't quite capture the intended meaning.

\end{Example}

Since we know by reflexivity that \(\tau \subseteq \tau\), we can
reformulate \((prop'\ 1)\) as:

\begin{center}
  \AxiomC{$\exists \tau' \in \tau_i.\ \tau \subseteq \tau'$}
  \LeftLabel{$(prop''\ 1)$}
  \UnaryInfC{$[\tau] \subseteq_\ell \tau_i$}
  \DisplayProof
\end{center}

Using this rule, we can now complete the previously open branch in the
example above:

\begin{center}
  \AxiomC{$\vdots$}

  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$\psi \subseteq \psi$}
  \LeftLabel{$(prop''\ 1)$}
  \UnaryInfC{$[\psi] \subseteq_\ell [\psi , \tau , \rho]$}

  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$\tau \subseteq \tau$}
  \LeftLabel{$(prop''\ 1)$}
  \UnaryInfC{$[\tau] \subseteq_\ell [\psi , \tau , \rho]$}
  \LeftLabel{$(prop'\ 2)$}
  \BinaryInfC{$[\psi , \tau] \subseteq_\ell [\psi , \tau , \rho]$}
  
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$\psi \subseteq \psi$}
  \LeftLabel{$(arr)$}
  \BinaryInfC{$[\psi , \tau , \rho] \to \psi \subseteq [\psi , \tau] \to \psi$}
  \LeftLabel{$(prop''\ 1)$}
  \UnaryInfC{$[[\psi , \tau , \rho] \to \psi] \subseteq_\ell [[\psi , \tau] \to \psi]$}
  \LeftLabel{$(prop'\ 2)$}
  \BinaryInfC{$[[\psi , \tau] \to \psi, [\psi , \tau , \rho] \to \psi] \subseteq_\ell [[\psi , \tau] \to \psi]$}
  \DisplayProof
\end{center}

Also, since the only rules that can proceed \((prop'\ 2)\) in the
derivation tree are \((refl)\) or \((prop'\ 1)\), and it's easy to see
that in case of \((refl)\) preceding, we can always apply \((prop'\ 1)\)
before \((refl)\), we can in fact merge \((prop'\ 1)\) and
\((prop'\ 2)\) into the single rule:

\begin{center}
  \AxiomC{$\forall \tau' \in \tau_i.\ \exists \tau'' \in \tau.\ \tau' \subseteq \tau''$}
  \LeftLabel{$(prop'\ 12)$}
  \UnaryInfC{$\tau_i \subseteq_\ell \tau$}
  \DisplayProof
\end{center}

The final version of this rule, as it appears in
\cref{Definition:subseteqNew}, is simply an iterated version, split into
the \((nil)\) and \((cons)\) cases, to mirror the constructors of lists,
since these rules ``operate'' with \texttt{List IType}. This iterated
style of rules was adopted throughout this chapter for all definitions
involving \texttt{List IType}, wherever possible, since it is more
natural to work with, in Agda.

\begin{Example}

To illustrate this, take the following lemma about type refinement:

\begin{Lemma}

The following rule is admissible in the typing refinement relation
\(::_\ell\):

\begin{center}
  \AxiomC{$\tau_i ::_\ell A$}
  \AxiomC{$\tau_j ::_\ell A$}
  \LeftLabel{$(\ \ \ensuremath{+\!\!\!\!\!\!\!+\,}\ )$}
  \BinaryInfC{$\tau_i \concat \tau_j ::_\ell A$}
  \DisplayProof
\end{center}

\begin{proof}

By induction on \(\tau_i ::_\ell A\):

\begin{itemize}
\tightlist
\item
  \((nil)\): Therefore \(\tau_i \equiv []\) and
  \([] \concat \tau_j \equiv \tau_j\). Thus \(\tau_j ::_\ell A\) holds
  by assumption.
\item
  \((cons)\): We have \(\tau_i \equiv \tau , \tau_s\). Thus we know that
  \(\tau :: A\) and \(\tau_s ::_\ell A\). Then, by IH, we have
  \(\tau_s \concat \tau_j ::_\ell A\) and thus:
\end{itemize}

\begin{center}
  \AxiomC{}
  \LeftLabel{$(assm)$}
  \UnaryInfC{$\tau :: A$}
  \AxiomC{}
  \LeftLabel{$(IH)$}
  \UnaryInfC{$\tau_s \concat \tau_j ::_\ell A$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\tau , \tau_s \concat \tau_j ::_\ell A$}
\DisplayProof
\end{center}

\end{proof}

\end{Lemma}

For comparison, the same proof in Agda reads much the same as the
``paper'' one, given above:

\begin{minted}[escapeinside=||]{agda}
++-â·'|$_\ell$| : â {A Ï|$_\texttt{i}$| Ï|$_\texttt{j}$|} -> Ï|$_\texttt{i}$| â·'|$_\ell$| A -> Ï|$_\texttt{j}$| â·'|$_\ell$| A -> (Ï|$_\texttt{i}$| ++ Ï|$_\texttt{j}$|) â·'|$_\ell$| A
++-â·'|$_\ell$| nil Ï|$_\texttt{j}$|â·'A = Ï|$_\texttt{j}$|â·'A
++-â·'|$_\ell$| (cons Ïâ·'A Ï|$_\texttt{s}$|â·'A) Ï|$_\texttt{j}$|â·'A = cons Ïâ·'A (++-â·'|$_\ell$| Ï|$_\texttt{s}$|â·'A Ï|$_\texttt{j}$|â·'A)
\end{minted}

\end{Example}

\section{Intersection-type
assignment}\label{intersection-type-assignment}

Having modified the initial definition of sub-typing and added the
notion of type refinement, we now take a look at the definition of
intersection type assignment and the modifications that were needed for
the mechanization.

Whilst before, intersection typing consisted of the triple
\(\Gamma \Vdash M : \tau)\), where \(\Gamma\) was the intersection type
context, \(M\) was an untyped \(\lamy\) term and \(\tau\) was an
intersection type, this information is not actually sufficient when we
introduce type refinement. As we've shown with the \((Y)\) rule, the
refinement relation \(::\) provides a connection between intersection
and simple types. We therefore want \(M\) in the triple to be a simply
typed \(\lamy\) term.\\
Even though, we could use the definition of simple types from the
previous chapters, this notation would be rather cumbersome.

\begin{Example}

Consider the simply typed term \(\{\} \vdash \lambda x.x : A \to A\)
being substituted for the untyped \(\lambda x.x\) in
\(\{\} \Vdash \lambda x.x : (\tau \cap \phi) \leadsto \phi\) (where
\(\tau :: A\) and \(\phi :: A\)):

\begin{center}
$\{\} \Vdash (\{\} \vdash \lambda x.x : A \to A) : (\tau \cap \phi) \leadsto \phi$
\end{center}

Already, this simple example demonstrates the clutter of using
\emph{Curry}-style simple types in conjunction with the intersection
typing.

\end{Example}

Instead of using the \emph{a la Curry} simple typing, presented in the
example above, we chose to define typed \(\lamy\) terms \emph{a la
Church}. However, since we are using the Locally Nameless representation
of binders, we actually give the definition of simply typed pre-terms:

\begin{Definition}[Simply typed pre-terms a la Church]

For every simple type \(A\), the set of simply typed pre-terms
\(\Lambda_A\) is inductively defined in the following way:

\begin{center}
  \AxiomC{}
  \LeftLabel{$(fv)$}
  \UnaryInfC{$x \in \Lambda_A$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(bv)$}
  \UnaryInfC{$n \in \Lambda_A$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$s \in \Lambda_{B \to A}$}
  \AxiomC{$t \in \Lambda_B$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$st \in \Lambda_A$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$s \in \Lambda_B$}
  \LeftLabel{$(lam)$}
  \UnaryInfC{$\lambda_A.s \in \Lambda_{A \to B}$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$Y_A \in \Lambda_{(A \to A) \to A}$}
  \DisplayProof
\end{center}

\end{Definition}

It's easy to see that the definition of Church-style simply typed
\(\lamy\) pre-terms differs form the untyped pre-terms only in the
\(\lambda\) case, with the addition of the extra typing information,
much like in the case of \(Y\). We also adopt a typing convention, where
we write \(M_{\{A\}}\) to mean \(M \in \Lambda_A\).

The next hurdle we faced in defining the intersection typing relation
was the formulation of the \((Y)\) rule. The intuition behind this rule
is to type a \(Y_A\) constant with a type \(\tau\) s.t.
\(\tau :: (A \to A) \to A\). If we used the \(\lambda_\cap^{BCD}\) types
(introduced in \cref{itypesIntro}), we could easily have
\(\tau \equiv (\taui \leadsto \taui) \leadsto \taui\), where
\(\taui :: A\). However, as we have restricted ourselves to
strict-intersection types, the initial definition for the \((Y)\) rule
was the somewhat cumbersome:

\begin{center}
  \vskip 1.5em
  \AxiomC{$\taui :: \sigma$}
  \LeftLabel{$(Y)$}
  \RightLabel{$(j \in \underline{n})$}
  \UnaryInfC{$\Gamma \Vdash Y_\sigma : (\taui \leadsto \tau_1 \cap\hdots\cap \taui \leadsto \tau_i) \leadsto \tau_j$}
  \DisplayProof
  \vskip 1.5em
\end{center}

The implementation of this rule clearly demonstrates the complexity,
which made it difficult to reason with in proofs:

\begin{minted}[escapeinside=||]{agda}
Y :    â {Î A Ï|$_\texttt{i}$| Ï} -> (Ï|$_\texttt{i}$|â·A : Ï|$_\texttt{i}$| â·'|$_\ell$| A) -> (ÏâÏ|$_\texttt{i}$| : Ï â Ï|$_\texttt{i}$|) ->
    ----------------------------------------------------------
    Î â© Y A â¶ (â© (Data.List.map (Î» Ï|$_\texttt{k}$| -> (â© Ï|$_\texttt{i}$| ~> Ï|$_\texttt{k}$|)) Ï|$_\texttt{i}$|) ~> Ï)
\end{minted}

Even though Agda's main strength is its the powerful pattern matching,
it was quickly realized that pattern matching on the type
\texttt{(â© (Data.List.map (Î» Ï\textsubscript{k} -> (â© Ï\textsubscript{i} \textasciitilde > Ï\textsubscript{k})) Ï\textsubscript{i}) \textasciitilde > Ï)}
is difficult due to the map function, which appears inside the
definition.\\
Several modifications were made to the rule, until we arrived at it's
current form. To create a compromise between the unrestricted
intersection-types of \(\lambda_\cap^{BCD}\), which made expressing the
\((Y)\) rule much simpler, and the strict typing, which provided a
restricted version of type derivation over the \(\lambda_\cap^{BCD}\)
system, we modified strict types to include intersections on both the
left and right sub-terms of a strict type:

\begin{Definition}[Semi-strict intersection types]

\begin{center}
$\begin{aligned}
\mathcal{T}_s &::= \phi\ |\ (\mathcal{T}_s \cap\hdots\cap \mathcal{T}_s) \leadsto (\mathcal{T}_s \cap\hdots\cap \mathcal{T}_s) \\ 
\end{aligned}$
\end{center}

\end{Definition}

\(\ \)

\begin{Remark}

Using strict intersection types and having two intersection typing
relations \(\Vdash\) and \(\Vdash_\ell\) makes proving lemmas about the
system much easier. The clearest example of this is the nice property of
\emph{inversion}, one gets ``for free'' with strict types. Take, for
example, the term \(\Gamma \Vdash uv : \tau\) (where for the puproses of
this example, \(uv\) is a term of the simply-typed \(\lambda\)-calculus
and not a \(\lamy\) term). Since for the \(\Vdash\) relation, \(uv\) can
only be given a strict intersection type, we can easily prove the
following inversion lemma:

\begin{Lemma}[Inversion Lemma for $(app)$]

\label{Lemma:invApp} In the following lemma, \(\tau_i\) is an
intersection, i.e.~a list of strict intersection types
\(\mathcal{T}_s\):

\begin{center}
$\Gamma \Vdash uv : \tau \iff \exists \tau_i.\ \Gamma \Vdash u : \tau_i \leadsto \tau \land \Gamma \Vdash_\ell v : \tau_i$
\end{center}

\end{Lemma}

Such a lemma is in fact not even needed in Agda, since the shape of the
term \(uv\) and the type \(\tau\) uniquely determine that the derivation
tree must have had an application of the \((app)\) rule at its base. In
an Agda proof, such as:

\begin{minted}[]{agda}
sample-lemma Îâ©uvâ¶Ï = ?
\end{minted}

One can perform a case analysis on the variable \texttt{Îâ©uvâ¶Ï} (the
type of which is \texttt{Î â© app u v â¶ Ï}) and obtain:

\begin{minted}[escapeinside=||]{agda}
sample-lemma (app Îâ©uâ¶Ï|$_\texttt{i}$|~>Ï Îâ©vâ¶Ï|$_\texttt{i}$|) = ?
\end{minted}

In the \(\lambda_\cap^{BCD}\) system (or similar), such an inversion
lemma would be a lot more complicated and might look something like:

\begin{center}
$\begin{aligned}
\Gamma \Vdash uv : \tau \iff &\exists k \geq 1.\ \exists \tau_1,\hdots,\tau_k,\psi_1,\hdots,\psi_k.\\
&\tau \subseteq \psi_1 \cap \hdots \cap \psi_k \land\\
&\forall i \in \{1,\hdots,k\}.\ \Gamma \Vdash u : \tau_i \leadsto \psi_i \land \Gamma \Vdash v : \tau_i
\end{aligned}$
\end{center}

The semi-strict typing loses some of the advantages of the strict types,
as we will later modify the typing relation, losing the ``free''
\emph{inversion} properties that we currently have, i.e.~for a given
term \(uv\), \cref{Lemma:invApp} won't be trivial any more. However, the
complexity of the inversion lemmas for semi-strict typing is still lower
than that of the unrestricted intersection-typing systems.

\end{Remark}

The final version of the \((Y)\) rule, along with the other modified
rules of the typing relation are presented below:

\begin{Definition}[Intersection-type assignment]

This definition assumes that the typing context \(\Gamma\), which is a
list of triples \((x, \tau_i, A)\), is well formed. For each triple,
written as \(x : \tau_i ::_\ell A\), this means that the free variable
\(x\) does not appear elsewhere in the domain of \(\Gamma\). Each
intersection type \(\tau_i\), associated with a variable \(x\), also
refines a simple type \(A\). In the definition below, we also assume the
following convention \(\bigcap \tau \equiv [\tau]\):
\label{Definition:itypAssignment}

\begin{center}
  \AxiomC{$\exists (x : \tau_i ::_\ell A) \in \Gamma.\ \bigcap \tau \subseteq^A_\ell \tau_i$}
  \LeftLabel{$(var)$}
  \UnaryInfC{$\Gamma \Vdash x_{\{A\}} : \tau$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{$\Gamma \Vdash u_{\{A \to B\}} : \tau_i \leadsto \tau_j$}
  \AxiomC{$\Gamma \Vdash_\ell v_{\{A\}} : \tau_i$}
  \LeftLabel{$(app)$}
  \RightLabel{$(\bigcap \tau \subseteq^B_\ell \tau_j)$}
  \BinaryInfC{$\Gamma \Vdash uv : \tau$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{$\forall x \not\in L.\ (x : \tau_i ::_\ell A),\Gamma \Vdash_\ell m^x : \tau_j$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\Gamma \Vdash \lambda_A.m : \tau_i \leadsto \tau_j$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$\exists \tau_x.\ \bigcap (\tau_x \leadsto \tau_x) \subseteq^{A \to A}_\ell \tau_i \land \tau_j \subseteq^A_\ell \tau_x$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$\Gamma \Vdash_s Y_{A} : \tau_i \leadsto \tau_j$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\Gamma \Vdash_\ell m : \omega$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$\Gamma \Vdash m : \tau$}
  \AxiomC{$\Gamma \Vdash_\ell m : \tau_i$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\Gamma \Vdash_\ell m : \tau , \tau_i$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

\section{Proof of subject expansion}\label{proof-of-subject-expansion}

An interesting property of the intersection types is the fact that they
admit both subject expansion and subject reduction, namely \(\Vdash\) is
closed under \(\beta\)-equality. In this section, we will focus on the
subject expansion lemma:

\begin{Theorem}[Subject expansion for $\Vdash/\Vdash_\ell$]

\begin{enumerate}
\def\labelenumi{\roman{enumi})}
\tightlist
\item
  \(\Gamma \Vdash m : \tau \implies m \red m' \implies \Gamma \Vdash m' : \tau\)
\item
  \(\Gamma \Vdash_\ell m : \tau_i \implies m \red m' \implies \Gamma \Vdash_\ell m' : \tau_i\)
\end{enumerate}

\end{Theorem}

The proof of this theorem follows by induction on the
\(\beta\)-reduction \(m \red m'\). We will focus on the \((Y)\)
reduction rule and show that given a well typed term
\(\Gamma \Vdash m(Y_\sigma m) : \tau\), s.t.
\(Y_\sigma m \red m(Y_\sigma m)\), we can also type \(Y_\sigma m\) with
the same intersection type \(\tau\).\\
We will start with a very high-level overview of the proof. Having
assumed, \(\Gamma \Vdash m(Y_\sigma m) : \tau\), we must necessarily
have the following derivation tree for the intersection typing relation
\(\Vdash\):

\begin{figure}[h]
\begin{center}
  \vskip 1em
  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash_s m : \tau_i \leadsto \tau_j$}
  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash_\ell Y_A m : \tau_i$}
  \LeftLabel{$(app)$}
  \RightLabel{$([\tau] \subseteq^A_\ell \tau_j)$}
  \BinaryInfC{$\Gamma \Vdash m(Y_{A} m) : \tau$}
  \DisplayProof
  \vskip 1em
\end{center}
\caption{Analysis of the shape of the derivation tree for $\Gamma \Vdash m(Y_{A} m) : \tau$}
\label{figure:shapemYm}
\end{figure}

We have two cases, where \(\tau_i\) is an empty intersection
\(\tau_i \equiv \omega\) or a non-empty list of strict intersection
types \(\tau_i \equiv [\tau_1,\hdots,\tau_n]\).

\subsection{\texorpdfstring{\(\tau_i \equiv \omega\)}{\textbackslash{}tau\_i \textbackslash{}equiv \textbackslash{}omega}}\label{tau_i-equiv-omega}

From \cref{figure:shapemYm} above, we have
\((1): \Gamma \Vdash_s m : \omega \leadsto \tau_j\). Then, we can
construct the following proof tree:

\begin{center}
  \AxiomC{$[[\tau] \leadsto [\tau]] \subseteq^{A \to A}_\ell [\omega \leadsto [\tau]] \land [\tau] \subseteq^A_\ell [\tau]$}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$\Gamma \Vdash Y : [\omega \leadsto [\tau]] \leadsto [\tau]$}

  \AxiomC{$\Gamma \Vdash m : \omega \leadsto [\tau]$}
  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\Gamma \Vdash_\ell m : \omega$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\Gamma \Vdash_\ell m : [\omega \leadsto [\tau]]$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$\Gamma \Vdash Y_{A} m : \tau$}
  \DisplayProof
  \vskip 1em
\end{center}

The only (non-trivial) open branch in the above tree is to show that
\(\Gamma \Vdash m : \omega \leadsto [\tau]\). In order to do so, we need
to use the sub-typing lemma for intersection types:

\begin{Lemma}[Sub-typing for $\Vdash/\Vdash_\ell$]

In the definition below, the binary relation \(\subseteq_\Gamma\) is
defined for any well-formed contexts \(\Gamma\) and \(\Gamma'\), where
for each triple \((x : \tau_i ::_\ell A) \in \Gamma\), there is a
corresponding triple \((x : \tau_j ::_\ell A) \in \Gamma'\) s.t.
\(\tau_i \subseteq^A_\ell \tau_j\ \):

\begin{center}
  \vskip 1em
  \AxiomC{$\Gamma \Vdash m_{\{A\}} : \tau$}
  \LeftLabel{$(\subseteq)$}
  \RightLabel{$(\Gamma \subseteq_\Gamma \Gamma', \tau' \subseteq^A \tau)$}
  \UnaryInfC{$\Gamma' \Vdash m_{\{A\}} : \tau'$}
  \DisplayProof
  \hskip 1.5em
  \AxiomC{$\Gamma \Vdash_\ell m_{\{A\}} : \tau_i$}
  \LeftLabel{$(\subseteq_\ell)$}
  \RightLabel{$(\Gamma \subseteq_\Gamma \Gamma', \tau_j \subseteq^A_\ell \tau_i)$}
  \UnaryInfC{$\Gamma' \Vdash_\ell m_{\{A\}} : \tau_j$}
  \DisplayProof
  \vskip 1em
\end{center}

\begin{proof}

Ommited.

\end{proof}

\end{Lemma}

Thus, we have:

\begin{center}
  \AxiomC{}
  \LeftLabel{$(1)$}
  \UnaryInfC{$\Gamma \Vdash m : \omega \leadsto \tau_j$}
  \LeftLabel{$(\subseteq)$}
  \RightLabel{$(\Gamma \subseteq_\Gamma \Gamma, \omega \leadsto [\tau] \subseteq^{A \to A} \omega \leadsto \tau_j)$}
  \UnaryInfC{$\Gamma \Vdash m : \omega \leadsto [\tau]$}
  \DisplayProof
  \vskip 1em
\end{center}

\subsection{\texorpdfstring{\(\tau_i \equiv [\tau_1,\hdots,\tau_n]\)}{\textbackslash{}tau\_i \textbackslash{}equiv {[}\textbackslash{}tau\_1,\textbackslash{}hdots,\textbackslash{}tau\_n{]}}}\label{tau_i-equiv-tau_1hdotstau_n}

This case of the proof is a lot more involved and required several
additional rules and lemmas. We will outline the main ideas of the proof
in this section.\\
\(\ \)

\begin{Remark}

The first thing to note is that since \(\tau_i\) is a non-empty list of
semi-strict intersection types, it will have the shape:

\begin{center}
  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash Y_A m : \tau_1$}

  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash Y_A m : \tau_2$}

  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash Y_A m : \tau_n$}

  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\Gamma \Vdash_\ell Y_A m : \omega$}
  \BinaryInfC{$\Gamma \Vdash_\ell Y_A m : [\tau_n]$}
  \UnaryInfC{$\vdots$} 
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\Gamma \Vdash_\ell Y_A m : [\tau_2,\hdots,\tau_n]$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\Gamma \Vdash_\ell Y_A m : \tau_i$}
  \DisplayProof
  \vskip 1em
\end{center}

We will simplify this notation slightly and just write:

\begin{center}
  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash Y_A m : \tau_1$}

  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash Y_A m : \tau_2$}

  \AxiomC{$\hdots$}

  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash Y_A m : \tau_n$}

  \QuaternaryInfC{$\Gamma \Vdash_\ell Y_A m : \tau_i$}
  \DisplayProof
  \vskip 1em
\end{center}

\end{Remark}

In order to type \(Y_A m\) with \(\tau\), we first have to show that for
every tree above, we can find a type \(\tau'_k\) s.t.
\(\Gamma \Vdash m : \tau'_k \leadsto \tau'_k\) and
\([\tau_k] \subseteq^A_\ell \tau'_k\):

\begin{Lemma}

\(\Gamma \Vdash Y_A m : \tau \implies \exists \tau'.\ \Gamma \Vdash_\ell m : [\tau' \leadsto \tau'] \land [\tau] \subseteq^A_\ell \tau'\)

\begin{proof}

\label{Lemma:Ymax} Unfolding the typing tree of
\(\Gamma \Vdash Y_A m : \tau\), we have:

\begin{center}
  \AxiomC{$[\tau_x \leadsto \tau_x] \subseteq^{A \to A} \tau_i \land \tau_j \subseteq^A \tau_x$}
  \UnaryInfC{$\Gamma \Vdash_s Y_A : \tau_i \leadsto \tau_j$}
  \AxiomC{$\Gamma \Vdash_\ell m : \tau_i$}
  \LeftLabel{$(app)$}
  \RightLabel{$([\tau] \subseteq^A_\ell \tau_j)$}
  \BinaryInfC{$\Gamma \Vdash Y_A m : \tau$}
  \DisplayProof
  \vskip 1em
\end{center}

Then it follows by transitivity, that
\([\tau] \subseteq^A_\ell \tau_x\), and
\(\Gamma \Vdash_\ell m : [\tau_x \leadsto \tau_x]\) by sub-typing:

\begin{center}
  \vskip 1em
  \AxiomC{$\Gamma \Vdash_\ell m : \tau_i$}
  \LeftLabel{$(\subseteq_\ell)$}
  \RightLabel{$(\Gamma \subseteq_\Gamma \Gamma, [\tau_x \leadsto \tau_x] \subseteq^{A \to A}_\ell \tau_i)$}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau_x \leadsto \tau_x]$}
  \DisplayProof
  \hskip 1em
  \AxiomC{$[\tau] \subseteq^A_\ell \tau_j$}
  \AxiomC{$\tau_j \subseteq^A_\ell \tau_x$}
  \LeftLabel{$(trans)$}
  \BinaryInfC{$[\tau] \subseteq^A_\ell \tau_x$}
  \DisplayProof
\end{center}

\end{proof}

\end{Lemma}

Since for every type \(\tau_k \in \tau_i\) we now have
\(\Gamma \Vdash_\ell m : [\tau'_k \leadsto \tau'_k]\) as well as
\([\tau_k] \subseteq^A_\ell \tau'_k\), we want to ``mege'' all these
types given to \(m\):

\begin{center}
  \vskip 1em
  \AxiomC{$\Gamma \Vdash Y_A m : \tau_1$}
  \LeftLabel{(\cref{Lemma:Ymax})}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_1 \leadsto \tau'_1]$}

  \AxiomC{$\hdots$}

  \AxiomC{$\Gamma \Vdash Y_A m : \tau_n$}
  \LeftLabel{(\cref{Lemma:Ymax})}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_n \leadsto \tau'_n]$}

  \LeftLabel{(???)}
  \TrinaryInfC{$\Gamma \Vdash m : \tau'_1\concat\hdots\concat\tau'_n \leadsto \tau'_1\concat\hdots\concat\tau'_n$}
  \DisplayProof
  \vskip 1em
\end{center}

such that we have
\(\tau'_i \equiv \tau'_1 \concat \hdots \concat \tau'_n\) where
\(\tau_i \subseteq^A_\ell \tau'_i\).

To illustrate how to prove the last step in the tree above (i.e.~how to
derive the (???) rule), we will look at a simpler example, where
\(\tau_i \equiv [\tau_1, \tau_2]\). Thus, we want to show:

\begin{center}
  \vskip 1em
  \AxiomC{$\Gamma \Vdash Y_A m : \tau_1$}
  \LeftLabel{(\cref{Lemma:Ymax})}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_1 \leadsto \tau'_1]$}

  \AxiomC{$\Gamma \Vdash Y_A m : \tau_2$}
  \LeftLabel{(\cref{Lemma:Ymax})}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_2 \leadsto \tau'_2]$}

  \BinaryInfC{$\Gamma \Vdash m : \tau'_1\concat\tau'_2 \leadsto \tau'_1\concat\tau'_2$}
  \DisplayProof
  \vskip 1em
\end{center}

Using the sub-typing lemma we can show:

\begin{center}
  \vskip 1em
  \AxiomC{$\Gamma \Vdash_\ell m : [\tau'_1 \leadsto \tau'_1]$}
  \LeftLabel{$(\subseteq_\ell)$}
  \RightLabel{$([\tau'_1\concat\tau'_2 \leadsto \tau'_1] \subseteq^{A \to A}_\ell [\tau'_1 \leadsto \tau'_1])$}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_1\concat\tau'_2 \leadsto \tau'_1]$}
  \DisplayProof
  \vskip 1em
\end{center}

since we have:

\begin{center}
  \AxiomC{}
  \LeftLabel{$(\subseteq^*)$\footnotemark}
  \UnaryInfC{$\tau'_1 \subseteq^A \tau'_1\concat\tau'_2$}
  \AxiomC{}
  \LeftLabel{$(refl)$}
  \UnaryInfC{$\tau'_1 \subseteq^A \tau'_1$}
  \BinaryInfC{$\tau'_1\concat\tau'_2 \leadsto \tau'_1 \subseteq^{A \to A} \tau'_1 \leadsto \tau'_1$}
  
  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\omega \subseteq^{A \to A}_\ell [\tau'_1 \leadsto \tau'_1]$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$[\tau'_1\concat\tau'_2 \leadsto \tau'_1] \subseteq^{A \to A}_\ell [\tau'_1 \leadsto \tau'_1]$}
  \DisplayProof
  \vskip 1em
\end{center}

\footnotetext{This is a derived rule defined for the subset relation on lists, i.e. if the list $\tau_k$ is a subset of $\tau_n$, then it is also a subtype of $\tau_n$. Th derivation of this rule trivially follows from the $(refl)$, $(nil)$ and $(cons)$ rules.}

Similarly, we can also prove
\(\Gamma \Vdash_\ell m : [\tau'_1\concat\tau'_2 \leadsto \tau'_2]\), but
at this point there is no way we can merge these two types, to produce
\(\Gamma \Vdash_\ell m : [\tau'_1\concat\tau'_2 \leadsto \tau'_1\concat\tau'_2]\).\\
In order to proceed, we had to introduce a new rule to the typing
relation \(\Vdash\), to allow us to derive the type above:

\begin{center}
  \AxiomC{$\Gamma \Vdash m_{\{A \to B\}} : \tau_i \leadsto \tau_j$}
  \AxiomC{$\Gamma \Vdash m_{\{A \to B\}} : \tau_i \leadsto \tau_k$}
  \LeftLabel{$(\tocap)$}
  \RightLabel{$(\tau_{jk} \subseteq^B \tau_j \concat \tau_k)$}
  \BinaryInfC{$\Gamma \Vdash m_{\{A \to B\}} : \tau_i \leadsto \tau_{jk}$}
  \DisplayProof
  \vskip 1em
\end{center}

Introducing this rule created a host of complications, the chief of
which was the fact that we lost our ``free'' inversion lemmas, as it is
now no longer obvious from the shape of the term, which rule was used
last in the type derivation tree.

\begin{Example}

Consider a term \(\lambda_A. m\) s.t.
\(\Gamma \Vdash \lambda_A. m : \tau\). Since \(\tau\) must necessarily
be of the shape \(\psi_i \leadsto \psi_j\), either of these two
derivation trees could be valid:

\begin{center}
  \vskip 1em
  \AxiomC{$\vdots$}
  \UnaryInfC{$\forall x \not\in L.\ (x : \psi_i ::_\ell A),\Gamma \Vdash_\ell (m^x)_{\{B\}} : \psi_j$}
  \LeftLabel{$(abs)$}
  \UnaryInfC{$\Gamma \Vdash (\lambda_A. m)_{\{A \to B\}} : \psi_i \leadsto \psi_j$}
  \DisplayProof
  \vskip 1.5em
  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash_s (\lambda_A. m)_{\{A \to B\}} : \psi_i \leadsto \tau_j$}
  \AxiomC{$\vdots$}
  \UnaryInfC{$\Gamma \Vdash_s (\lambda_A. m)_{\{A \to B\}} : \psi_i \leadsto \tau_k$}
  \LeftLabel{$(\tocap)$}
  \RightLabel{$(\psi_j \subseteq^B \tau_j \concat \tau_k)$}
  \BinaryInfC{$\Gamma \Vdash_s (\lambda_A. m)_{\{A \to B\}} : \psi_i \leadsto \psi_j$}
  \DisplayProof
  \vskip 1em
\end{center}

However, it's easy to see that since these derivation trees must be
finite, if we apply \((\tocap)\) multiple times, eventually, all of the
branches will have to have an application of the \((abs)\) rule.

\end{Example}

Besides having to derive inversion lemmas for the \((abs)\) and \((Y)\)
rules, another derived rule, namely the sub-typing rule, breaks. In
order to fix this rule, we also had to add an axiom scheme corresponding
to the \((\tocap)\) rule, to the definition of the type subset relation:

\begin{center}
  \vskip 1em
  \AxiomC{$[\tau_i \leadsto (\tau_j \concat \tau_k)] \concat \tau_m ::_\ell A \to B$}
  \LeftLabel{$(\tocap_\ell)$}
  \UnaryInfC{$[\tau_i \leadsto (\tau_j \concat \tau_k)] \concat \tau_m \subseteq^{A \to B}_\ell [\tau_i \leadsto \tau_j ,\ \tau_i \leadsto \tau_k] \concat \tau_m$}
  \DisplayProof
  \vskip 1em
\end{center}

\(\ \)

\begin{Remark}

Initially, we tried to simply add \((\tocap_\ell)\) to the type subset
relation and add the sub-typing rule to \(\Vdash\), instead of having
the rule \((\tocap)\). However, this made the inversion lemmas, as well
as some other lemmas too difficult to prove. Finding the right balance
in the formalization of the \((\tocap)/(\tocap_\ell)\) and the
sub-typing rules proved to be perhaps the most challenging part of the
formalization of intersection types.

\end{Remark}

We can now finish or proof for the case where
\(\tau_i \equiv [\tau_1, \tau_2]\):

\begin{center}
  \AxiomC{$\Gamma \Vdash Y_A m : \tau_1$}
  \LeftLabel{(\cref{Lemma:Ymax})}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_1 \leadsto \tau'_1]$}
  \LeftLabel{$(\subseteq_\ell)$}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_1\concat\tau'_2 \leadsto \tau'_1]$}
  \LeftLabel{$(\in_\ell)$\footnotemark}
  \UnaryInfC{$\Gamma \Vdash m : \tau'_1\concat\tau'_2 \leadsto \tau'_1$}

  \AxiomC{$\Gamma \Vdash Y_A m : \tau_2$}
  \LeftLabel{(\cref{Lemma:Ymax})}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_2 \leadsto \tau'_2]$}
  \LeftLabel{$(\subseteq_\ell)$}
  \UnaryInfC{$\Gamma \Vdash_\ell m : [\tau'_1\concat\tau'_2 \leadsto \tau'_2]$}
  \LeftLabel{$(\in_\ell)$}
  \UnaryInfC{$\Gamma \Vdash m : \tau'_1\concat\tau'_2 \leadsto \tau'_2$}
  
  \LeftLabel{$(\tocap)$}
  \BinaryInfC{$\Gamma \Vdash m : \tau'_1\concat\tau'_2 \leadsto \tau'_1\concat\tau'_2$}
  \DisplayProof
  \vskip 1em
\end{center}

\footnotetext{The derived rule $(\in_\ell)$ is used to convert between the strict typing relation $\Vdash$ and intersection typing $\Vdash_\ell$ and is stated as: $\Gamma \Vdash_\ell m : \tau_i \land \tau \in \tau_i \implies \Gamma \Vdash m : \tau$}

Having demonstrated the case when \(\tau_i \equiv [\tau_1, \tau_2]\),
the proof when \(\tau_i\) is arbitrarily long proceeds in much the same
way:

\begin{Lemma}

\label{Lemma:Ymaxl} Assuming \(\tau_i\) is a non-empty intersection, we
have:\\
\(\Gamma \Vdash_\ell Y_A m : \tau_i \implies \exists \tau'_i.\ \Gamma \Vdash m : \tau'_i \leadsto \tau'_i \land \tau_i \subseteq^A_\ell \tau'_i\)

\end{Lemma}

Using \cref{Lemma:Ymaxl}, we can finally show that
\(\Gamma \Vdash Y_A m : \tau\).

\setlength\parindent{-.75mm}
\(\begin{aligned}\text{First, from \cref{figure:shapemYm}, we have: } &(1): \Gamma \Vdash m : \tau_i \leadsto \tau_j \text{ , }\\ &(2): [\tau] \subseteq^A_\ell \tau_j \text{ and }\\ &(3): \Gamma \Vdash_\ell Y_A m : \tau_i. \end{aligned}\)

\(\begin{aligned} \text{Since $\tau_i$ is not empty, from $(3)$ and \cref{Lemma:Ymaxl}, we have some $\tau'_i$ s.t. } &(4): \Gamma \Vdash m : \tau'_i \leadsto \tau'_i\text{ and }\\ &(5): \tau_i \subseteq^A_\ell \tau'_i. \end{aligned}\)

\setlength\parindent{0mm}

We can therefore derive
\(\Gamma \Vdash_\ell m : [\tau'_i \leadsto ([\tau] \concat \tau'_i)]\):

\begin{center}
  \AxiomC{}
  \LeftLabel{$(1)$}
  \UnaryInfC{$\Gamma \Vdash m : \tau_i \leadsto \tau_j$}
  \LeftLabel{$(\subseteq)$}
  \RightLabel{$(\tau'_i \leadsto [\tau] \subseteq^{A \to A} \tau_i \leadsto \tau_j)$}
  \UnaryInfC{$\Gamma \Vdash m : \tau'_i \leadsto [\tau]$}
  
  \AxiomC{}
  \LeftLabel{$(4)$}
  \UnaryInfC{$\Gamma \Vdash m : \tau'_i \leadsto \tau'_i$}
  \LeftLabel{$(\tocap)$}
  \BinaryInfC{$\Gamma \Vdash m : \tau'_i \leadsto ([\tau] \concat \tau'_i)$}

  \AxiomC{}
  \LeftLabel{$(nil)$}
  \UnaryInfC{$\Gamma \Vdash_\ell m : \omega$}
  \LeftLabel{$(cons)$}
  \BinaryInfC{$\Gamma \Vdash_\ell m : [\tau'_i \leadsto ([\tau] \concat \tau'_i)]$}
  \DisplayProof
  \vskip 1em
\end{center}

\(\ \)

\begin{Remark}

In the sub-typing rule, used in the tree above,
\(\tau'_i \leadsto [\tau] \subseteq^{A \to A} \tau_i \leadsto \tau_j\)
follows by:

\begin{center}
  \vskip 1em
  \AxiomC{}
  \LeftLabel{$(5)$}
  \UnaryInfC{$\tau_i \subseteq^A_\ell \tau'_i$}
    
  \AxiomC{}
  \LeftLabel{$(2)$}
  \UnaryInfC{$[\tau] \subseteq^A_\ell \tau_j$}

  \LeftLabel{$(arr)$}
  \BinaryInfC{$(\tau'_i \leadsto [\tau] \subseteq^{A \to A} \tau_i \leadsto \tau_j)$}
  \DisplayProof
  \vskip 1em
\end{center}

\end{Remark}

Finally, putting all the pieces together, we get
\(\Gamma \Vdash Y_{A} m : \tau\):

\begin{center}
  \vskip 1em
  \AxiomC{
    \stackanchor{$[([\tau] \concat \tau'_i) \leadsto ([\tau] \concat \tau'_i)] \subseteq^{A \to A}_\ell [\tau'_i \leadsto ([\tau] \concat \tau'_i)]\ \land$}
    {$[\tau] \subseteq^A_\ell [\tau] \concat \tau'_i$}}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$\Gamma \Vdash Y : [\tau'_i \leadsto ([\tau] \concat \tau'_i)] \leadsto [\tau]$}

  \AxiomC{$\Gamma \Vdash_\ell m : [\tau'_i \leadsto ([\tau] \concat \tau'_i)]$}
  \LeftLabel{$(app)$}
  \BinaryInfC{$\Gamma \Vdash Y_{A} m : \tau$}
  \DisplayProof
  \vskip 1em
\end{center}

\section{Proofs of termination for the LN
representation}\label{proofs-of-termination-for-the-ln-representation}

This final section of the chapter briefly describes an interesting
implementation quirk/overhead, encountered when proving the substitution
lemma, which was required for the proofs of both subject expansion and
reduction:

\begin{Lemma}[Substitution lemma]

Given that \(m\) and \(n\) are both well formed terms and
\(x \not\in \dom\ \Gamma\), we have:

\begin{center}
$\Gamma \Vdash m_{\{A\}}[n_{\{B\}}/x] : \tau \iff \exists \tau_i.\ (x : \tau_i ::_\ell B), \Gamma \Vdash m_{\{A\}} : \tau \land \Gamma \Vdash_\ell n_{\{B\}} : \tau_i$
\end{center}

\end{Lemma}

In the backwards direction (\(\Leftarrow\)), this proof is fairly
straight forward and follows much like the homonymous lemma for simple
types.\\
The other direction (\(\Rightarrow\)), used in the proof of subject
expansion, turned out to be more complicated in Agda. This part of the
proof proceeds by induction on the well formed term \(m\), and whilst
trying to prove the goal, when \(m\) is a \(\lambda\)-term, Agda's
termination checker would fail. To show why this was the case, we first
examine the definition for the \(\lambda\)-case:

\begin{minted}[escapeinside=||]{agda}
subst-â©-2 : â {A B Î Ï x} ->
  {m : Î A} {n : Î B} -> 
  ÎTerm m -> ÎTerm n ->
  x â dom Î -> Î â© (m Î[ x ::= n ]) â¶ Ï ->
  â(Î» Ï|$_\texttt{i}$| -> ( ((x , Ï|$_\texttt{i}$| , B) â· Î) â© m â¶ Ï ) Ã ( Î â©|$_\ell$| n â¶ Ï|$_\texttt{i}$| ))
|$\texttt{\vdots}$|
subst-â©-2 {A â¶ B} {C} {Î} {Ï ~> Ï'} {x} 
  {lam .A p} {n} 
  (lam L {.p} cf) trm-n 
  xâÎ (abs L' cf') = ?
\end{minted}

Informally, the (pieces of) definition above can be read as:

\begin{itemize}
\tightlist
\item
  \texttt{{lam .A p}} : \(m \equiv \lambda_A.p\)
\item
  \texttt{(lam L {.p} cf)} : \(p\) is a well formed \(\lambda\)-term,
  s.t. we have \(\forall x' \not\in L.\ \trm(p^{x'})\) for some finite
  \(L\).\\
  (This is captured by the type of \texttt{cf}, which is
  \texttt{xâ â L â ÎTerm (Î[ 0 >> fv xâ ] p)}.)
\item
  \texttt{trm-n} : \(n\) is a well-formed \(\lamy\) term
\item
  \texttt{(abs L' cf')} : the last rule in the derivation tree of
  \(\Gamma \Vdash \lambda_A.p_{\{B\}}[n_{\{C\}}/x] : \tau \leadsto \tau'\)
  was the \((abs)\) rule and therefore we have \texttt{cf'}, which
  encodes the premise, that there is some finite \(L'\) s.t.
  \(\forall x' \not\in L'.\ (x' : \tau ::_\ell A),\Gamma \Vdash_\ell (p[n/x])^{x'} : \tau\).
\end{itemize}

The proof proceeds, by first showing that we can obtain a fresh \(x'\)
s.t. \(\trm(p^{x'})\). By picking a sufficiently fresh \(x'\), we can
also derive
\((x' : \tau ::_\ell A),\Gamma \Vdash_\ell (p^{x'})[n/x] : \tau\),
essentially swapping the substitution and opening from the assumption
above.\\
However, when we then try to apply the induction hypothesis, which
corresponds to a recursive call in Agda, we get an error, claiming that
termination checking failed. In order to see why this happens, we will
ignore the explicit arguments passed to \texttt{subst-â©$_\ell$-2} and
instead focus on the implicit arguments:

\begin{minted}[escapeinside=||]{agda}
ih = subst-â©|$_\ell$|-2 {A} {C} {(x' , Ï , A) â· Î} {Ï'} {x} 
  {Î[ 0 >> fv x' ] p} {n} ...
\end{minted}

Agda's termination checking relies on the fact that the data-types,
being pattern matched on, get structurally smaller in recursive
calls\footnote{The details on how the termination checking algorithm in
  Agda works are sparse, so we are not actually sure about the specifics
  of how the termination check fails.}. Thus, the parameter
\texttt{Î[ 0 >> fv x' ] m} in this definition is obviously problematic,
as Agda doesn't know that \(p^{x'}\) is structurally smaller than \(m\)
(i.e. \(\lambda_A. p\)), even though we know that is the case, as the
open operation simply replaces a bound variable with a free one.
However, whilst \(p^{x'}\) is not ``bigger'' than \(p\), it is not,
strictly speaking, structurally smaller than \(\lambda_A.p\) and
therefore, structural induction/recursion principles cannot be used in
this definition.

Whilst one can suppress termination checking for a specific
definition/lemma in Agda, by adding the \texttt{\{-\# TERMINATING \#-\}}
pragma in front of the definition, it is generally not a good idea to do
this, even though we know that the definition is actually terminating.
We initially contemplated using well-founded recursion to prove that the
proof terminates, but having little experience in Agda, this looked
quite complicated.\\
Instead, we devised a simple ``hack'' to allow Agda to prove termination
for this (slightly modified) lemma, by first defining ``skeleton'' terms
\(T\), which capture the structure of \(\lamy\) terms:

\begin{Definition}

\(T ::= *\ |\ \circ T\ |\ T\ \amp\ T\)

\end{Definition}

\(\ \)

\begin{Example}

As an illustration, take the LN \(\lamy\) term \(\lambda.0(Y_A x)\). We
can represented this term as a tree (left). Then, we simply replace any
\(\lambda\) and \(Y_\sigma\) with \(\circ\), application becomes \& and
any free or bound variables are represented as * in the skeleton tree
(on the right):

\begin{minipage}{.5\textwidth}
\begin{center}
\begin{tikzpicture}[distance=1em,
  every node/.style = {align=center}]]
  \node {$\lambda$}
    child { node {app}
      child { node {0} }
      child { node {app} 
        child { node {$Y_A$} }
        child { node {x} } } };
\end{tikzpicture}
\end{center}
\end{minipage}\begin{minipage}{.5\textwidth}
\begin{center}
\begin{tikzpicture}[distance=3em,
  every node/.style = {align=center}]]
  \node {$\circ$}
    child { node {$\amp$}
      child { node {*} }
      child { node {$\amp$} 
        child { node {*} }
        child { node {*} } } };
\end{tikzpicture}
\end{center}
\end{minipage}

Thus, the skeleton term of \(\lambda.0(Y_A x)\) is
\(\circ(*\ \amp\ (*\ \amp\ *))\).

\end{Example}

Next, we defined the congruence relation \(\sim_T\) between locally
nameless \(\lamy\) terms and skeleton terms:

\begin{Definition}[$\sim_T$ relation]

In the following definition, \(m,p,q\) range over simply-typed locally
nameless \(\lamy\) terms and \(s,t\) range over skeleton terms \(T\):

\begin{center}
  \AxiomC{}
  \LeftLabel{$(bvar)$}
  \UnaryInfC{$n \sim_T *$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(fvar)$}
  \UnaryInfC{$x \sim_T *$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{}
  \LeftLabel{$(Y)$}
  \UnaryInfC{$Y_A \sim_T *$}
  \DisplayProof
  %------------------------------------
  \vskip 1.5em
  \AxiomC{$m \sim_T t$}
  \LeftLabel{$(un)$}
  \UnaryInfC{$\lambda_A.m \sim_T \circ t$}
  \DisplayProof
  %------------------------------------
  \hskip 1.5em
  \AxiomC{$p \sim_T s$}
  \AxiomC{$q \sim_T t$}
  \LeftLabel{$(bin)$}
  \BinaryInfC{$pq \sim_T s\ \amp\ t$}
  \DisplayProof
  \vskip 1.5em
\end{center}

\end{Definition}

Having defined the \(\sim_T\) relation, we could now augment our
substitution lemma proof with a skeleton tree corresponding to the LN
term \(m\), performing (simultaneous) induction on the congruence
relation \(m \sim_T t\). For the \(\lambda\)-case, we have a skeleton
tree of the form \(\circ t\) (for \(m \equiv \lambda_A.p\))\$. The
inductive hypothesis call is now:

\begin{minted}[escapeinside=||]{agda}
ih = subst-â©|$_\ell$|-2 {A} {C} {(x' , Ï , A) â· Î} {Ï'} {x} 
  {Î[ 0 >> fv x' ] m} {n} {t} (opn-~T-inv m~t) ...
\end{minted}

where \(t\) is the skeleton tree corresponding to \(p\), and by
\cref{Lemma:openCongInv} (\texttt{opn-\textasciitilde T-inv}), also to
\(p^{x'}\):

\begin{Lemma}

\label{Lemma:openCongInv} \(m \sim_T t \implies \{k \to x\}m \sim_T t\)

\begin{proof}

By induction on the relation \(m \sim_T t\). The only interesting case
is \((bvar)\). We have two cases, when \(n = k\) or \(n \neq k\). In
both cases, we have \(n \sim_T *\), thus in case \(n \neq k\), the
result follows by assumption, otherwise we have
\(n\{k \to x\} \equiv x\) and thus \(x \sim_T *\) by \((fvar)\).

\end{proof}

\end{Lemma}

After this modification, Agda (grudgingly) accepted the definition as
terminating, even though this rather complicated inductive/recursive
definition of the proof now takes a rather long time to compile
(slowdown by up to a factor of 6 compared to other theories of similar
length).

\footnotesize

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\hypertarget{refs}{}
\hypertarget{ref-aydemir05}{}
Aydemir, Brian E., Aaron Bohannon, Matthew Fairbairn, J. Nathan Foster,
Benjamin C. Pierce, Peter Sewell, Dimitrios Vytiniotis, Geoffrey
Washburn, Stephanie Weirich, and Steve Zdancewic. 2005. ``Mechanized
Metatheory for the Masses: The Poplmark Challenge.'' In \emph{Theorem
Proving in Higher Order Logics: 18th International Conference, Tphols
2005, Oxford, Uk, August 22-25, 2005. Proceedings}, edited by Joe Hurd
and Tom Melham, 50--65. Berlin, Heidelberg: Springer Berlin Heidelberg.
doi:\href{https://doi.org/10.1007/11541868_4}{10.1007/11541868\_4}.

\hypertarget{ref-aydemir08}{}
Aydemir, Brian, Arthur CharguÃ©raud, Benjamin C. Pierce, Randy Pollack,
and Stephanie Weirich. 2008. ``Engineering Formal Metatheory.'' In
\emph{Proceedings of the 35th Annual Acm Sigplan-Sigact Symposium on
Principles of Programming Languages}, 3--15. POPL '08. New York, NY,
USA: ACM.
doi:\href{https://doi.org/10.1145/1328438.1328443}{10.1145/1328438.1328443}.

\hypertarget{ref-bakel}{}
Bakel, Steffen van. 2003. ``Semantics with Intersection Types.''
\url{http://www.doc.ic.ac.uk/~svb/SemIntTypes/Notes.pdf}.

\hypertarget{ref-barendregt13}{}
Barendregt, Henk, Wil Dekkers, and Richard Statman. 2013. \emph{Lambda
Calculus with Types}. New York, NY, USA: Cambridge University Press.

\hypertarget{ref-berghofer06}{}
Berghofer, Stefan, and Christian Urban. 2006. ``A Head-to-Head
Comparison of de Bruijn Indices and Names.'' In \emph{IN Proc. Int.
Workshop on Logical Frameworks and Metalanguages: THEORY and Practice},
46--59.

\hypertarget{ref-clairambault13}{}
Clairambault, Pierre, and Andrzej S. Murawski. 2013. ``BÃ¶hm Trees as
Higher-Order Recursive Schemes.'' In \emph{IARCS Annual Conference on
Foundations of Software Technology and Theoretical Computer Science,
FSTTCS 2013, December 12-14, 2013, Guwahati, India}, 91--102.
doi:\href{https://doi.org/10.4230/LIPIcs.FSTTCS.2013.91}{10.4230/LIPIcs.FSTTCS.2013.91}.

\hypertarget{ref-harper93}{}
Harper, Robert, Furio Honsell, and Gordon Plotkin. 1993. ``A Framework
for Defining Logics.'' \emph{J. ACM} 40 (1). New York, NY, USA: ACM:
143--84.
doi:\href{https://doi.org/10.1145/138027.138060}{10.1145/138027.138060}.

\hypertarget{ref-kobayashi09}{}
Kobayashi, Naoki. 2009. ``Types and Higher-Order Recursion Schemes for
Verification of Higher-Order Programs.'' In \emph{Proceedings of the
36th Annual Acm Sigplan-Sigact Symposium on Principles of Programming
Languages}, 416--28. POPL '09. New York, NY, USA: ACM.
doi:\href{https://doi.org/10.1145/1480881.1480933}{10.1145/1480881.1480933}.

\hypertarget{ref-kobayashi13}{}
---------. 2013. ``Model Checking Higher-Order Programs.'' \emph{J. ACM}
60 (3). New York, NY, USA: ACM: 20:1--20:62.
doi:\href{https://doi.org/10.1145/2487241.2487246}{10.1145/2487241.2487246}.

\hypertarget{ref-ong06}{}
Ong, C.-H. L. 2006. ``On Model-Checking Trees Generated by Higher-Order
Recursion Schemes.'' In \emph{Proceedings of the 21st Annual Ieee
Symposium on Logic in Computer Science}, 81--90. LICS '06. Washington,
DC, USA: IEEE Computer Society.
doi:\href{https://doi.org/10.1109/LICS.2006.38}{10.1109/LICS.2006.38}.

\hypertarget{ref-pfenning88}{}
Pfenning, F., and C. Elliott. 1988. ``Higher-Order Abstract Syntax.'' In
\emph{Proceedings of the Acm Sigplan 1988 Conference on Programming
Language Design and Implementation}, 199--208. PLDI '88. New York, NY,
USA: ACM.
doi:\href{https://doi.org/10.1145/53990.54010}{10.1145/53990.54010}.

\hypertarget{ref-pollack95}{}
Pollack, Robert. 1995. ``Polishing up the Tait-Martin-LÃ¶f Proof of the
Church-Rosser Theorem.''

\hypertarget{ref-ramsay14}{}
Ramsay, Steven J., Robin P. Neatherway, and C.-H. Luke Ong. 2014. ``A
Type-Directed Abstraction Refinement Approach to Higher-Order Model
Checking.'' \emph{SIGPLAN Not.} 49 (1). New York, NY, USA: ACM: 61--72.
doi:\href{https://doi.org/10.1145/2578855.2535873}{10.1145/2578855.2535873}.

\hypertarget{ref-takahashi95}{}
Takahashi, M. 1995. ``Parallel Reductions in \(\lambda\)-Calculus.''
\emph{Information and Computation} 118 (1): 120--27.
\url{http://www.sciencedirect.com/science/article/pii/S0890540185710577}.

\hypertarget{ref-tsukada14}{}
Tsukada, Takeshi, and C.-H. Luke Ong. 2014. ``Compositional Higher-Order
Model Checking via \$\$-Regular Games over BÃ¶hm Trees.'' In
\emph{Proceedings of the Joint Meeting of the Twenty-Third Eacsl Annual
Conference on Computer Science Logic (Csl) and the Twenty-Ninth Annual
Acm/Ieee Symposium on Logic in Computer Science (Lics)}, 78:1--78:10.
CSL-Lics '14. New York, NY, USA: ACM.
doi:\href{https://doi.org/10.1145/2603088.2603133}{10.1145/2603088.2603133}.

% - Back matter ----------------------------------------------------------------



\setcounter{secnumdepth}{0}
\chapter*{Appendix}\label{appendix}
\addcontentsline{toc}{chapter}{Appendix}
\addtocontents{toc}{\protect\setcounter{tocdepth}{1}}
\input{appendix/LamYNom.tex}
\newpage
\input{appendix/LamYNmless.tex}
\newpage


\end{document}
